{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a tiny version of YOLO with DIOR dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement YOLO architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TinyYOLOv1                               [1, 441]                  --\n",
       "├─Conv2d: 1-1                            [1, 8, 128, 128]          224\n",
       "├─SquareActivation: 1-2                  [1, 8, 128, 128]          --\n",
       "├─BatchNorm2d: 1-3                       [1, 8, 128, 128]          16\n",
       "├─AvgPool2d: 1-4                         [1, 8, 64, 64]            --\n",
       "├─Conv2d: 1-5                            [1, 16, 32, 32]           1,168\n",
       "├─SquareActivation: 1-6                  [1, 16, 32, 32]           --\n",
       "├─BatchNorm2d: 1-7                       [1, 16, 32, 32]           32\n",
       "├─AvgPool2d: 1-8                         [1, 16, 16, 16]           --\n",
       "├─Flatten: 1-9                           [1, 4096]                 --\n",
       "├─Linear: 1-10                           [1, 2048]                 8,390,656\n",
       "├─SquareActivation: 1-11                 [1, 2048]                 --\n",
       "├─Linear: 1-12                           [1, 441]                  903,609\n",
       "==========================================================================================\n",
       "Total params: 9,295,705\n",
       "Trainable params: 9,295,705\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 14.16\n",
       "==========================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 2.38\n",
       "Params size (MB): 37.18\n",
       "Estimated Total Size (MB): 40.35\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TinyYOLOv1(nn.Module):\n",
    "    def __init__(self, B=2, num_classes=3):\n",
    "        super(TinyYOLOv1, self).__init__()\n",
    "        S = 7  # grid size\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, stride=2, padding=1)\n",
    "        self.square_activation1 = SquareActivation()\n",
    "        self.batch_norm1 = nn.BatchNorm2d(8)\n",
    "        self.avgpool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.square_activation2 = SquareActivation()\n",
    "        self.batch_norm2 = nn.BatchNorm2d(16)\n",
    "        self.avgpool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(4096, 2048)\n",
    "        self.square_activation_ft = SquareActivation()\n",
    "        self.fc2 = nn.Linear(2048, S * S * (B * 3 + num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.square_activation1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.avgpool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.square_activation2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.avgpool2(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.square_activation_ft(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Assuming the SquareActivation is defined somewhere else\n",
    "class SquareActivation(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * x\n",
    "\n",
    "model = TinyYOLOv1()\n",
    "summary(model, input_size=(1, 3, 256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance between centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def euclidean_distance(point1, point2):\n",
    "#     x1 = point1[...,0]\n",
    "#     y1 = point1[...,1]\n",
    "#     x2 = point2[...,0]\n",
    "#     y2 = point2[...,1]\n",
    "\n",
    "#     distance = torch.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "#     return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(center_preds, center_labels):\n",
    "    \"\"\"\n",
    "    Calculate euclidean distance\n",
    "    Parameters:\n",
    "        center_preds: predictions of centers (BATCH_SIZE, 2)\n",
    "        center_labels: target of centers of shape (BATCH_SIZE, 2)\n",
    "    Returns:\n",
    "        distance: euclidean distance for all examples\n",
    "    \"\"\"\n",
    "\n",
    "    x1 = center_preds[..., 0:1]\n",
    "    y1 = center_preds[..., 1:2]\n",
    "    x2 = center_labels[..., 0:1]\n",
    "    y2 = center_labels[..., 1:2]\n",
    "\n",
    "    distance = torch.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non Max Suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(centers,distance_threshold, conf_threshold):\n",
    "    \"\"\"\n",
    "    Does Non Max Suppression given bboxes\n",
    "    Parameters:\n",
    "        bboxes (list): list of lists containing all centers with each centers\n",
    "        specified as [class_pred, prob_score, x_center, y_center]\n",
    "        distance_threshold (float): threshold where predicted centers is correct\n",
    "        conf_threshold (float): confidence threshold to remove predicted centers (independent of IoU) \n",
    "    Returns:\n",
    "        list: bboxes after performing NMS given a threshold\n",
    "    \"\"\"\n",
    "\n",
    "    assert type(centers) == list\n",
    "\n",
    "    centers = [center for center in centers if center[1] > conf_threshold]\n",
    "    centers = sorted(centers, key=lambda x: x[1], reverse=True)\n",
    "    centers_after_nms = []\n",
    "\n",
    "    while centers:\n",
    "        chosen_center = centers.pop(0)\n",
    "\n",
    "        centers = [\n",
    "            center\n",
    "            for center in centers\n",
    "            if center[0] != chosen_center[0]\n",
    "            or euclidean_distance(\n",
    "                torch.tensor(chosen_center[2:]),\n",
    "                torch.tensor(center[2:]),\n",
    "            )\n",
    "            < distance_threshold\n",
    "        ]\n",
    "\n",
    "        centers_after_nms.append(chosen_center)\n",
    "    #print(f\"bboxes_after_nms: {bboxes_after_nms}\")\n",
    "\n",
    "    return centers_after_nms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Average Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_distance_between_centers(pred_centers, true_centers):\n",
    "    \"\"\"\n",
    "    Calculate the mean distance between predicted and true object centers.\n",
    "    Parameters:\n",
    "        pred_centers (list): List containing predicted centers with each center specified as [x_center, y_center]\n",
    "        true_centers (list): List containing true centers with each center specified as [x_center, y_center]\n",
    "    Returns:\n",
    "        float: Mean distance between centers for all examples\n",
    "    \"\"\"\n",
    "    total_distance = 0.0\n",
    "    num_examples = len(pred_centers)\n",
    "    for pred_center, true_center in zip(pred_centers, true_centers):\n",
    "        pred_center = torch.tensor(pred_center)\n",
    "        true_center = torch.tensor(true_center)\n",
    "        distance = euclidean_distance(pred_center, true_center)\n",
    "        total_distance += distance.item()\n",
    "\n",
    "    mean_distance = total_distance / num_examples\n",
    "    return mean_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Class Confidence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_class_score(pred_classes, true_classes):\n",
    "    \"\"\"\n",
    "    Calculate the mean class score between predicted and target classes.\n",
    "    Parameters:\n",
    "        pred_classes (list): List containing predicted class scores for each example\n",
    "        true_classes (list): List containing true class scores for each example\n",
    "    Returns:\n",
    "        float: Mean class score between predicted and target classes for all examples\n",
    "    \"\"\"\n",
    "    total_score = 0.0\n",
    "    num_examples = len(pred_classes)\n",
    "    print(f\"num_examples_pred: {num_examples}\")\n",
    "    print(f\"num example true: {len(true_classes)}\")\n",
    "    for pred_class, true_class in zip(pred_classes, true_classes):\n",
    "        pred_class = torch.tensor(pred_class)\n",
    "        true_class = torch.tensor(true_class)\n",
    "        score = torch.mean(torch.abs(pred_class - true_class)).item()\n",
    "        total_score += score\n",
    "\n",
    "    mean_score = total_score / num_examples\n",
    "    return mean_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It describes a trade-off between precision and recall.\n",
    "\n",
    "**Precision**, also referred to as the positive predictive value, describes how well a model predicts the positive class. \n",
    "$$Precision=\\frac{TP}{TP+FP}$$\n",
    ">   Of all bounding box **predictions**, what fraction was actually correct?\n",
    "\n",
    "**Recall**, also called sensitivity tells you if your model made the right predictions when it should have. \n",
    "$$Recall=\\frac{TP}{TP+FN}$$\n",
    ">   Of all **target** bounding boxes, what fraction did we correctly detect?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(\n",
    "        pred_centers, true_centers, distance_threshold=3, num_classes=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates mean average precision\n",
    "    Parameters:\n",
    "        pred_centers (list): list of lists containing all predicted centers with each center\n",
    "        specified as [train_idx, class_pred, prob_score, x_center, y_center]\n",
    "        true_centers (list): list of lists containing all true centers with each center\n",
    "        distance_threshold (float): threshold where predicted centers is correct\n",
    "        num_classes (int): number of classes\n",
    "    Returns:\n",
    "        float: mean average precision\n",
    "    \"\"\"\n",
    "\n",
    "    average_precisions = []\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        for detection in pred_centers:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "\n",
    "        for true_center in true_centers:\n",
    "            if true_center[1] == c:\n",
    "                ground_truths.append(true_center)\n",
    "\n",
    "        # find the amount of centers for each training example\n",
    "        # Counter here finds how many ground truth centers we get\n",
    "        # for each training example, so let's say img 0 has 3,\n",
    "        # img 1 has 5 then we will obtain a dictionary with:\n",
    "        # amount_centers = {0:3, 1:5}\n",
    "        amount_centers = Counter([gt[0] for gt in ground_truths])\n",
    "\n",
    "        # We then go through each key, val in this dictionary\n",
    "        # and convert to the following (w.r.t same example):\n",
    "        # ammount_centers = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "        for key, val in amount_centers.items():\n",
    "            amount_centers[key] = torch.zeros(val)\n",
    "\n",
    "        # sort by box probabilities which is index 2\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "        # If none exists for this class then we can safely skip\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # Only take out the ground_truths that have the same training idx as detection\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            best_dist = 0\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                dist = euclidean_distance(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:]),\n",
    "                )\n",
    "                if dist > best_dist:\n",
    "                    best_dist = dist\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_dist > distance_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if amount_centers[detection[0]][best_gt_idx] == 0:\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_centers[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "            \n",
    "        #[1, 1, 0, 1, 0] -> [1, 2, 2, 3, 3]\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        # torch.trapz for numerical integration\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "    \n",
    "    return sum(average_precisions) / len(average_precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and convert centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO delete non max suppression, make check on pred centers and true centers\n",
    "def get_bboxes(\n",
    "    loader,\n",
    "    model,\n",
    "    distance_threshold,\n",
    "    conf_threshold,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    all_pred_centers = []\n",
    "    all_true_centers = []\n",
    "\n",
    "    # make sure model is in eval before get bboxes\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "\n",
    "    for batch_idx, (x, labels) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        labels = labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        true_centers = cellcenters_to_centers(labels)\n",
    "        centers = cellcenters_to_centers(predictions)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            nms_centers = non_max_suppression(\n",
    "                centers[idx],\n",
    "                distance_threshold=distance_threshold,\n",
    "                conf_threshold=conf_threshold,\n",
    "            )\n",
    "\n",
    "            # # Activate only for test\n",
    "            # if batch_idx == 0 and idx == 0:\n",
    "            #    plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n",
    "\n",
    "            for nms_center in nms_centers:\n",
    "                all_pred_centers.append([train_idx] + nms_center)\n",
    "\n",
    "            for center in true_centers[idx]:\n",
    "                # many will get converted to 0 pred\n",
    "                #if center[1] > distance_threshold:\n",
    "                all_true_centers.append([train_idx] + center)\n",
    "\n",
    "            train_idx += 1\n",
    "\n",
    "    model.train()\n",
    "    return all_pred_centers, all_true_centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cellcenters(predictions, S=7, C=3):\n",
    "    \"\"\"\n",
    "    Converts predictions from the model to centers\n",
    "    \"\"\"\n",
    "    predictions = predictions.to(\"cpu\")\n",
    "    batch_size = predictions.shape[0]\n",
    "    predictions = predictions.reshape(batch_size, 7, 7, C + 6)\n",
    "    centers1 = predictions[..., C + 1:C + 3]\n",
    "    centers2 = predictions[..., C + 4:C + 6]\n",
    "    \n",
    "    scores = torch.cat(\n",
    "        (predictions[..., C].unsqueeze(0), predictions[..., C + 3].unsqueeze(0)), dim=0\n",
    "    )\n",
    "    best_center = scores.argmax(0).unsqueeze(-1)\n",
    "    best_centers = centers1 * (1 - best_center) + best_center * centers2\n",
    "    # This results in a tensor with shape (batch_size, 7, 7, 1) where each element represents the index of a grid cell.\n",
    "    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n",
    "\n",
    "    x = 1 / S * (best_centers[..., :1] + cell_indices)\n",
    "    # Permute because is used here to swap these indices to match the (x, y) convention used in the best_boxes tensor.\n",
    "    # [0,1,2]->[0,0,0]\n",
    "    # [0,1,2]->[1,1,1]\n",
    "    # [0,1,2]->[2,2,2]\n",
    "    y = 1 / S * (best_centers[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
    "\n",
    "    converted_centers = torch.cat((x, y), dim=-1)\n",
    "    predicted_class = predictions[..., :C].argmax(-1).unsqueeze(-1)\n",
    "    best_confidence = torch.max(predictions[..., C], predictions[..., C + 3]).unsqueeze(\n",
    "        -1\n",
    "    )\n",
    "    converted_preds = torch.cat(\n",
    "        (predicted_class, best_confidence, converted_centers), dim=-1\n",
    "    )\n",
    "    #print(f\"converted_preds: {converted_preds}\")\n",
    "\n",
    "    return converted_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cellcenters_to_centers(out, S=7):\n",
    "    converted_pred = convert_cellcenters(out).reshape(out.shape[0], S * S, -1)\n",
    "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
    "    all_centers = []\n",
    "\n",
    "    for ex_idx in range(out.shape[0]):\n",
    "        centers = []\n",
    "\n",
    "        for center_idx in range(S * S):\n",
    "            centers.append([x.item() for x in converted_pred[ex_idx, center_idx, :]])\n",
    "        all_centers.append(centers)\n",
    "        \n",
    "    return all_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Loader of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, centers):\n",
    "        for t in self.transforms:\n",
    "            img, centers = t(img), centers\n",
    "\n",
    "        return img, centers\n",
    "\n",
    "\n",
    "transform = Compose([transforms.Resize((256, 256)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, S=7, B=2, C=3, transform=None, train=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.train = train\n",
    "\n",
    "        # Determine the directory of the images and labels\n",
    "        if self.train:\n",
    "            self.img_dir = os.path.join(self.root_dir, 'images/train')\n",
    "            self.label_dir = os.path.join(self.root_dir, 'labels/train')\n",
    "        else:\n",
    "            self.img_dir = os.path.join(self.root_dir, 'images/test')\n",
    "            self.label_dir = os.path.join(self.root_dir, 'labels/test')\n",
    "\n",
    "        self.img_ids = os.listdir(self.img_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.img_ids[index].split('.')[0]\n",
    "        centers = []\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, img_id + '.jpg')\n",
    "        image = Image.open(img_path)\n",
    "        # Load labels\n",
    "        label_path = os.path.join(self.label_dir, img_id + '.txt')\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                class_label, x, y, width, height = map(float, line.strip().split())\n",
    "                centers.append([class_label, x, y])\n",
    "\n",
    "        centers = torch.tensor(centers)        \n",
    "        if self.transform:\n",
    "            image, centers = self.transform(image, centers)\n",
    "        # Convert To Cells\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.C + 3 * self.B))\n",
    "        for center in centers:\n",
    "            class_label, x, y = center\n",
    "            class_label = int(class_label)\n",
    "            i, j = int(self.S * y), int(self.S * x)\n",
    "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
    "\n",
    "            if label_matrix[i, j, self.C] == 0:\n",
    "                label_matrix[i, j, self.C] = 1\n",
    "\n",
    "                center_coordinates = torch.tensor(\n",
    "                    [x_cell, y_cell]\n",
    "                )\n",
    "\n",
    "                label_matrix[i, j, self.C + 1:self.C + 3] = center_coordinates\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "    \n",
    "        #print(f\"label_matrix shape: {label_matrix.shape}\")\n",
    "\n",
    "        return image, label_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From original paper: \n",
    ">   YOLO predicts multiple bounding boxes per grid cell. At training time we only want one bounding box predictor to be responsible for each object. We assign one predictor to be “responsible” for predicting an object based on which prediction has the highest current IOU with the ground truth. This leads to specialization between the bounding box predictors.\n",
    "Each predictor gets better at predicting certain sizes, aspect ratios, or classes of object, improving overall recall. \n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\lambda_{\\text {coord }} \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{i j}^{\\text {obj }}\\left[\\left(x_i-\\hat{x}_i\\right)^2+\\left(y_i-\\hat{y}_i\\right)^2\\right] \\\\\n",
    "+\\lambda_{\\text {coord }} \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{i j}^{\\text {obj }}\\left[\\left(\\sqrt{w_i}-\\sqrt{\\hat{w}_i}\\right)^2+\\left(\\sqrt{h_i}-\\sqrt{\\hat{h}_i}\\right)^2\\right] \\\\\n",
    "+\\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{i j}^{\\text {obj }}\\left(C_i-\\hat{C}_i\\right)^2 \\\\\n",
    "+\\lambda_{\\text {noobj }} \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{i j}^{\\text {noobj }}\\left(C_i-\\hat{C}_i\\right)^2 \\\\\n",
    "+\\sum_{i=0}^{S^2} \\mathbb{1}_i^{\\text {obj }} \\sum_{c \\in \\text { classes }}\\left(p_i(c)-\\hat{p}_i(c)\\right)^2\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "During training we optimize the following, multi-part where $ 1_{obj}^i $ denotes if object appears in cell **i** and $1_{obj}^{ij}$ denotes that the **j**  bounding box predictor in cell i is “responsible” for that prediction.\n",
    "\n",
    "In every image many grid cells do not contain any object. This pushes the “confidence” scores of those cells towards zero, often overpowering the gradient from cells that do contain objects. This can lead to model instability, as the model may prioritize learning to predict empty cells rather than focusing on correctly detecting objects in cells containing them, causing training to diverge early on. To remedy this, we increase the loss from bounding box coordinate predictions and decrease the loss from confidence predictions for boxes that don’t contain objects. We use two parameters, $\\lambda_{coord}$ and $\\lambda_{noobj}$  to accomplish this.\n",
    "\n",
    "Note that the loss function only penalizes classification error if an object is present in that grid cell (hence the conditional class probability discussed earlier). It also only penalizes bounding box coordinate error if that predictor is “responsible” for the ground truth box (i.e. has the highest\n",
    "IOU of any predictor in that grid cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the loss for yolo (v1) model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S=7, B=2, C=3):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        \"\"\"\n",
    "        S is split size of image (in paper 7),\n",
    "        B is number of boxes (in paper 2),\n",
    "        C is number of classes (in paper 20, in dataset 3),\n",
    "        \"\"\"\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        # These are from Yolo paper, signifying how much we should\n",
    "        # pay loss for no object (noobj) and the box coordinates (coord)\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        # predictions are shaped (BATCH_SIZE, S*S(C+B*3) when inputted\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 3)\n",
    "        # Calculate IoU for the two predicted bounding boxes with target bbox\n",
    "        iou_c1 = euclidean_distance(predictions[..., self.C + 1:self.C + 3], target[..., self.C + 1:self.C + 3])\n",
    "        iou_c2 = euclidean_distance(predictions[..., self.C + 4:self.C + 6], target[..., self.C + 1:self.C + 3])\n",
    "        ious = torch.cat([iou_c1.unsqueeze(0), iou_c2.unsqueeze(0)], dim=0)\n",
    "        # Take the box with highest IoU out of the two prediction\n",
    "        # Note that bestbox will be indices of 0, 1 for which bbox was best\n",
    "        iou_maxes, bestcenter = torch.max(ious, dim=0)\n",
    "        exists_center = target[..., self.C].unsqueeze(3)  # in paper this is Iobj_i\n",
    "        # ======================== #\n",
    "        #   FOR CENTER COORDINATES #\n",
    "        # ======================== #\n",
    "\n",
    "        # Set boxes with no object in them to 0. We only take out one of the two \n",
    "        # predictions, which is the one with highest Iou calculated previously.\n",
    "\n",
    "        center_predictions = exists_center * (\n",
    "            (\n",
    "                bestcenter * predictions[..., self.C + 4:self.C + 6]\n",
    "                + (1 - bestcenter) * predictions[..., self.C + 1:self.C + 3]\n",
    "            )\n",
    "        )\n",
    "        center_targets = exists_center * target[..., self.C + 1:self.C + 3]\n",
    "\n",
    "        center_loss = self.mse(\n",
    "            torch.flatten(center_predictions, end_dim=-2),\n",
    "            torch.flatten(center_targets, end_dim=-2),\n",
    "        )\n",
    "\n",
    "        # ==================== #\n",
    "        #   FOR OBJECT LOSS    #\n",
    "        # ==================== #\n",
    "\n",
    "        # pred_box is the confidence score for the bbox with highest IoU\n",
    "        pred_center = (\n",
    "            bestcenter * predictions[..., self.C + 3:self.C + 4] + (1 - bestcenter) * predictions[..., self.C:self.C + 1]\n",
    "        )\n",
    "\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_center * pred_center),\n",
    "            torch.flatten(exists_center * target[..., self.C:self.C + 1]),\n",
    "        )\n",
    "\n",
    "        # ======================= #\n",
    "        #   FOR NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "\n",
    "        #max_no_obj = torch.max(predictions[..., 20:21], predictions[..., 25:26])\n",
    "        #no_object_loss = self.mse(\n",
    "        #    torch.flatten((1 - exists_box) * max_no_obj, start_dim=1),\n",
    "        #    torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
    "        #)\n",
    "\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_center) * predictions[..., self.C:self.C + 1], start_dim=1),\n",
    "            torch.flatten((1 - exists_center) * target[..., self.C:self.C + 1], start_dim=1),\n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_center) * predictions[..., self.C + 3:self.C + 4], start_dim=1),\n",
    "            torch.flatten((1 - exists_center) * target[..., self.C:self.C + 1], start_dim=1)\n",
    "        )\n",
    "\n",
    "        # ================== #\n",
    "        #   FOR CLASS LOSS   #\n",
    "        # ================== #\n",
    "\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_center * predictions[..., :self.C], end_dim=-2,),\n",
    "            torch.flatten(exists_center * target[..., :self.C], end_dim=-2,),\n",
    "        )\n",
    "\n",
    "        loss = (\n",
    "            self.lambda_coord * center_loss  # first two rows in paper\n",
    "            + object_loss  # third row in paper\n",
    "            + self.lambda_noobj * no_object_loss  # forth row\n",
    "            + class_loss  # fifth row\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32 # 64 in original paper but resource exhausted error otherwise.\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 50\n",
    "LOAD_MODEL = False\n",
    "LOAD_MODEL_FILE = \"model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, optimizer, loss_fn):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    mean_loss = []\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "        mean_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_postfix(loss = loss.item())\n",
    "        \n",
    "    print(f\"Mean loss was {sum(mean_loss) / len(mean_loss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dir = 'three_class_dataset'\n",
    "model = TinyYOLOv1().to(DEVICE)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "loss_fn = YoloLoss()\n",
    "\n",
    "\n",
    "train_dataset = DiorDataset(\n",
    "    root_dir=files_dir,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:21<00:00,  2.67it/s, loss=234]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 480.78000814872877\n",
      "Train Mean Distance: 1001.8012756347656\n",
      "num_examples_pred: 5\n",
      "num example true: 88837\n",
      "Train Mean Class Score: 200.85681762695313\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:20<00:00,  2.77it/s, loss=185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 352.16268251653304\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m train_fn(train_loader, model, optimizer, loss_fn)\n\u001b[0;32m----> 4\u001b[0m pred_boxes, target_boxes \u001b[38;5;241m=\u001b[39m \u001b[43mget_bboxes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistance_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# print(f\"pred_boxes: {len(pred_boxes)}, target_boxes: {len(target_boxes)}\")\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# mean_avg_prec = mean_average_precision(\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#     pred_boxes, target_boxes, distance_threshold=3\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# print(f\"Train mAP: {mean_avg_prec}\")\u001b[39;00m\n\u001b[1;32m     13\u001b[0m mean_dist \u001b[38;5;241m=\u001b[39m mean_distance_between_centers(pred_boxes, target_boxes)\n",
      "Cell \u001b[0;32mIn[99], line 15\u001b[0m, in \u001b[0;36mget_bboxes\u001b[0;34m(loader, model, distance_threshold, conf_threshold, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     13\u001b[0m train_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 15\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ObjDct_Repo/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/ObjDct_Repo/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/ObjDct_Repo/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/ObjDct_Repo/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[103], line 50\u001b[0m, in \u001b[0;36mDiorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_matrix[i, j, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     48\u001b[0m     label_matrix[i, j, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 50\u001b[0m     center_coordinates \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_cell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_cell\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     label_matrix[i, j, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m center_coordinates\n\u001b[1;32m     55\u001b[0m     label_matrix[i, j, class_label] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    train_fn(train_loader, model, optimizer, loss_fn)\n",
    "    pred_boxes, target_boxes = get_bboxes(\n",
    "        train_loader, model, distance_threshold=3, conf_threshold=0.5\n",
    "    )\n",
    "    \n",
    "    # print(f\"pred_boxes: {len(pred_boxes)}, target_boxes: {len(target_boxes)}\")\n",
    "    # mean_avg_prec = mean_average_precision(\n",
    "    #     pred_boxes, target_boxes, distance_threshold=3\n",
    "    # )\n",
    "    # print(f\"Train mAP: {mean_avg_prec}\")\n",
    "    mean_dist = mean_distance_between_centers(pred_boxes, target_boxes)\n",
    "    print(f\"Train Mean Distance: {mean_dist}\")\n",
    "    mean_class = mean_class_score(pred_boxes, target_boxes)\n",
    "    print(f\"Train Mean Class Score: {mean_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"tiny_yolo.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "test_transforms = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(max_size=int(IMAGE_SIZE)),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=int(IMAGE_SIZE),\n",
    "            min_width=int(IMAGE_SIZE),\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "        ),\n",
    "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[],),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dir='one_class_dataset'\n",
    "    \n",
    "test_dataset = DiorDataset( \n",
    "    root_dir=files_dir,\n",
    "    transform=test_transforms,\n",
    "    train=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = torch.load(\"tiny_yolo.pth\")\n",
    "# Load the state dictionary from the .pth file\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_boxes, target_boxes = get_bboxes(\n",
    "        test_loader, model, iou_threshold=0.5, threshold=0.4\n",
    "    )\n",
    "\n",
    "    mean_avg_prec = mean_average_precision(\n",
    "        pred_boxes, target_boxes, iou_threshold=0.5\n",
    "    )\n",
    "    print(f\"Test mAP: {mean_avg_prec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = torch.rand(1, 3, 256, 256)\n",
    "# model.eval()\n",
    "# output = model(sample)\n",
    "# print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Set the path to save the ONNX model\n",
    "# onnx_model_path = \"model.onnx\"\n",
    "\n",
    "# # Export the model to ONNX format\n",
    "# torch.onnx.export(model, sample, onnx_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PYHELAYERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a subset of plain samples from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_img_list=[]\n",
    "# test_label_list=[]\n",
    "# for image,label in test_dataset:\n",
    "#     test_img_list.append(image)\n",
    "#     test_label_list.append(label)\n",
    "#     if len(test_img_list)==1:\n",
    "#         break\n",
    "\n",
    "# test_img_array = np.array(test_img_list)\n",
    "# test_label_array = np.array(test_label_list)\n",
    "# # test_img_array = test_img_array[:11728]\n",
    "# # test_label_array = test_label_array[:11728]\n",
    "# # test_img_array = test_img_array.reshape(733,16,3,64,64)\n",
    "# # test_label_array = test_label_array.reshape(733,16,7,7,30)\n",
    "# print(test_img_array.shape)\n",
    "# print(test_label_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize he scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyhelayers\n",
    "# import utils\n",
    "\n",
    "# utils.verify_memory()\n",
    "\n",
    "# print('Misc. initalizations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = pyhelayers.DefaultContext()\n",
    "# print('HE context ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# nnp = pyhelayers.NeuralNetPlain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper_params = pyhelayers.PlainModelHyperParams()\n",
    "# nnp.init_from_files(hyper_params, [\"model.onnx\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# he_run_req = pyhelayers.HeRunRequirements()\n",
    "# he_run_req.set_he_context_options([pyhelayers.DefaultContext()])\n",
    "# he_run_req.optimize_for_batch_size(1)\n",
    "\n",
    "# profile = pyhelayers.HeModel.compile(nnp, he_run_req)\n",
    "# batch_size = profile.get_optimal_batch_size()\n",
    "# print('Profile ready. Batch size=',batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile.get_he_config_requirement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = pyhelayers.HeModel.create_context(profile)\n",
    "# print('HE context initalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context.get_scheme_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context.get_default_scale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context.get_library_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context.has_secret_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context.save_secret_key_to_file('secret_key.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn = pyhelayers.NeuralNet(context)\n",
    "# nn.encode_encrypt(nnp, profile)\n",
    "# print('Encrypted network ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plain_samples, labels = utils.extract_batch(test_img_array, test_label_array, batch_size, 0)\n",
    "\n",
    "# print('Batch of size',batch_size,'loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(plain_samples.shape)\n",
    "# print(plain_samples.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iop = nn.create_io_processor()\n",
    "# samples = pyhelayers.EncryptedData(context)\n",
    "# iop.encode_encrypt_inputs_for_predict(samples, [plain_samples])\n",
    "# print('Test data encrypted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make prediction on encrypted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.start_timer()\n",
    "\n",
    "# predictions = pyhelayers.EncryptedData(context)\n",
    "# nn.predict(predictions, samples)\n",
    "\n",
    "# duration=utils.end_timer('predict')\n",
    "# utils.report_duration('predict per sample',duration/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plain_predictions_aHE = iop.decrypt_decode_output(predictions)\n",
    "# print(f\"plain prediction shape after HE: {plain_predictions_aHE.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(plain_predictions_aHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## In case I have test samples > batch size\n",
    "# plain_predictions_aHE = plain_predictions_aHE.reshape(,1470)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate prediction with HE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plain_predictions_bHE=[]\n",
    "# for sample in test_img_array:\n",
    "#     tensor_sample = torch.tensor(sample).unsqueeze(0)\n",
    "#     output = model(tensor_sample)\n",
    "#     output = output.detach().numpy()\n",
    "#     plain_predictions_bHE.append(output)\n",
    "# plain_predictions_bHE = np.array(plain_predictions_bHE)\n",
    "# plain_predictions_bHE = plain_predictions_bHE.reshape(16, 1470)\n",
    "# print(f\"plain prediction shape before HE: {plain_predictions_bHE.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Compute the absolute differences between plain prediction before and after HE\n",
    "# differences = np.abs(plain_predictions_bHE - plain_predictions_aHE)\n",
    "\n",
    "# # Compute relevant statistics\n",
    "# mean_difference = np.mean(differences)\n",
    "# max_difference = np.max(differences)\n",
    "# min_difference = np.min(differences)\n",
    "# std_difference = np.std(differences)\n",
    "\n",
    "# print(f\"Mean difference: {mean_difference}\")\n",
    "# print(f\"Max difference: {max_difference}\")\n",
    "# print(f\"Min difference: {min_difference}\")\n",
    "# print(f\"Std difference: {std_difference}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert prediction in bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_bboxes_from_prediction(\n",
    "#     predictions,\n",
    "#     test_image_array,\n",
    "#     iou_threshold,\n",
    "#     threshold,\n",
    "# ):\n",
    "#     all_pred_boxes = []\n",
    "\n",
    "    \n",
    "#     train_idx = 0\n",
    "\n",
    "#     bboxes = cellboxes_to_boxes(predictions)\n",
    "\n",
    "#     for idx in range(len(test_image_array)):\n",
    "#         image = test_image_array[idx]\n",
    "\n",
    "#         nms_boxes = non_max_suppression(\n",
    "#             bboxes[idx],\n",
    "#             iou_threshold=iou_threshold,\n",
    "#             threshold=threshold,\n",
    "#         )\n",
    "\n",
    "#         # Activate only for test\n",
    "#         if  idx == 0:\n",
    "#             plot_image(image.permute(1,2,0).to(\"cpu\"), nms_boxes)\n",
    "\n",
    "#         for nms_box in nms_boxes:\n",
    "#             all_pred_boxes.append([train_idx] + nms_box)\n",
    "\n",
    "        \n",
    "\n",
    "#         train_idx += 1\n",
    "\n",
    "#     return all_pred_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_predaHE = torch.tensor(plain_predictions_aHE)\n",
    "# tensor_predbHE = torch.tensor(plain_predictions_bHE)\n",
    "# tensor_imgs = torch.tensor(test_img_array)\n",
    "\n",
    "# print(\"Prediction before HE\")\n",
    "# bboxes_aHE = get_bboxes_from_prediction(tensor_predbHE, tensor_imgs, iou_threshold=0.5, threshold=0.4)\n",
    "\n",
    "# print(\"Prediction after HE\")\n",
    "# bboxes_bHE = get_bboxes_from_prediction(tensor_predaHE, tensor_imgs, iou_threshold=0.5, threshold=0.4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
