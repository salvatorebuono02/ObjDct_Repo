{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "DATA_PATH = 'data'\n",
    "CLASSES_PATH = os.path.join(DATA_PATH, 'classes.json')\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 135\n",
    "WARMUP_EPOCHS = 0\n",
    "LEARNING_RATE = 1E-4\n",
    "\n",
    "EPSILON = 1E-6\n",
    "IMAGE_SIZE = (88, 88)\n",
    "\n",
    "S = 4       # Divide each image into a SxS grid\n",
    "B = 2       # Number of bounding boxes to predict\n",
    "C = 1      # Number of classes in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import matplotlib.patches as patches\n",
    "import torchvision.transforms as T\n",
    "from PIL import ImageDraw, ImageFont\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def get_iou(p, a):\n",
    "    p_tl, p_br = bbox_to_coords(p)          # (batch, S, S, B, 2)\n",
    "    a_tl, a_br = bbox_to_coords(a)\n",
    "\n",
    "    # Largest top-left corner and smallest bottom-right corner give the intersection\n",
    "    coords_join_size = (-1, -1, -1, B, B, 2)\n",
    "    tl = torch.max(\n",
    "        p_tl.unsqueeze(4).expand(coords_join_size),         # (batch, S, S, B, 1, 2) -> (batch, S, S, B, B, 2)\n",
    "        a_tl.unsqueeze(3).expand(coords_join_size)          # (batch, S, S, 1, B, 2) -> (batch, S, S, B, B, 2)\n",
    "    )\n",
    "    br = torch.min(\n",
    "        p_br.unsqueeze(4).expand(coords_join_size),\n",
    "        a_br.unsqueeze(3).expand(coords_join_size)\n",
    "    )\n",
    "\n",
    "    intersection_sides = torch.clamp(br - tl, min=0.0)\n",
    "    intersection = intersection_sides[..., 0] \\\n",
    "                   * intersection_sides[..., 1]       # (batch, S, S, B, B)\n",
    "\n",
    "    p_area = bbox_attr(p, 2) * bbox_attr(p, 3)                  # (batch, S, S, B)\n",
    "    p_area = p_area.unsqueeze(4).expand_as(intersection)        # (batch, S, S, B, 1) -> (batch, S, S, B, B)\n",
    "\n",
    "    a_area = bbox_attr(a, 2) * bbox_attr(a, 3)                  # (batch, S, S, B)\n",
    "    a_area = a_area.unsqueeze(3).expand_as(intersection)        # (batch, S, S, 1, B) -> (batch, S, S, B, B)\n",
    "\n",
    "    union = p_area + a_area - intersection\n",
    "\n",
    "    # Catch division-by-zero\n",
    "    zero_unions = (union == 0.0)\n",
    "    union[zero_unions] = EPSILON\n",
    "    intersection[zero_unions] = 0.0\n",
    "\n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "def bbox_to_coords(t):\n",
    "    \"\"\"Changes format of bounding boxes from [x, y, width, height] to ([x1, y1], [x2, y2]).\"\"\"\n",
    "\n",
    "    width = bbox_attr(t, 2)\n",
    "    x = bbox_attr(t, 0)\n",
    "    x1 = x - width / 2.0\n",
    "    x2 = x + width / 2.0\n",
    "\n",
    "    height = bbox_attr(t, 3)\n",
    "    y = bbox_attr(t, 1)\n",
    "    y1 = y - height / 2.0\n",
    "    y2 = y + height / 2.0\n",
    "\n",
    "    return torch.stack((x1, y1), dim=4), torch.stack((x2, y2), dim=4)\n",
    "\n",
    "\n",
    "def scheduler_lambda(epoch):\n",
    "    if epoch < WARMUP_EPOCHS + 75:\n",
    "        return 1\n",
    "    elif epoch < WARMUP_EPOCHS + 105:\n",
    "        return 0.1\n",
    "    else:\n",
    "        return 0.01\n",
    "\n",
    "\n",
    "def load_class_dict():\n",
    "    if os.path.exists(CLASSES_PATH):\n",
    "        with open(CLASSES_PATH, 'r') as file:\n",
    "            return json.load(file)\n",
    "    new_dict = {}\n",
    "    save_class_dict(new_dict)\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "def load_class_array():\n",
    "    classes = load_class_dict()\n",
    "    result = [None for _ in range(len(classes))]\n",
    "    for c, i in classes.items():\n",
    "        result[i] = c\n",
    "    return result\n",
    "\n",
    "\n",
    "def save_class_dict(obj):\n",
    "    folder = os.path.dirname(CLASSES_PATH)\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    with open(CLASSES_PATH, 'w') as file:\n",
    "        json.dump(obj, file, indent=2)\n",
    "\n",
    "\n",
    "def get_dimensions(label):\n",
    "    size = label['annotation']['size']\n",
    "    return int(size['width']), int(size['height'])\n",
    "\n",
    "\n",
    "def get_bounding_boxes(label):\n",
    "    width, height = get_dimensions(label)\n",
    "    x_scale = IMAGE_SIZE[0] / width\n",
    "    y_scale = IMAGE_SIZE[1] / height\n",
    "    boxes = []\n",
    "    objects = label['annotation']['object']\n",
    "    for obj in objects:\n",
    "        box = obj['bndbox']\n",
    "        coords = (\n",
    "            int(int(box['xmin']) * x_scale),\n",
    "            int(int(box['xmax']) * x_scale),\n",
    "            int(int(box['ymin']) * y_scale),\n",
    "            int(int(box['ymax']) * y_scale)\n",
    "        )\n",
    "        name = obj['name']\n",
    "        boxes.append((name, coords))\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def bbox_attr(data, i):\n",
    "    \"\"\"Returns the Ith attribute of each bounding box in data.\"\"\"\n",
    "\n",
    "    attr_start = C + i\n",
    "    return data[..., attr_start::5]\n",
    "\n",
    "\n",
    "def scale_bbox_coord(coord, center, scale):\n",
    "    return ((coord - center) * scale) + center\n",
    "\n",
    "\n",
    "def get_overlap(a, b):\n",
    "    \"\"\"Returns proportion overlap between two boxes in the form (tl, width, height, confidence, class).\"\"\"\n",
    "\n",
    "    a_tl, a_width, a_height, _, _ = a\n",
    "    b_tl, b_width, b_height, _, _ = b\n",
    "\n",
    "    i_tl = (\n",
    "        max(a_tl[0], b_tl[0]),\n",
    "        max(a_tl[1], b_tl[1])\n",
    "    )\n",
    "    i_br = (\n",
    "        min(a_tl[0] + a_width, b_tl[0] + b_width),\n",
    "        min(a_tl[1] + a_height, b_tl[1] + b_height),\n",
    "    )\n",
    "\n",
    "    intersection = max(0, i_br[0] - i_tl[0]) \\\n",
    "                   * max(0, i_br[1] - i_tl[1])\n",
    "\n",
    "    a_area = a_width * a_height\n",
    "    b_area = b_width * b_height\n",
    "\n",
    "    a_intersection = b_intersection = intersection\n",
    "    if a_area == 0:\n",
    "        a_intersection = 0\n",
    "        a_area = EPSILON\n",
    "    if b_area == 0:\n",
    "        b_intersection = 0\n",
    "        b_area = EPSILON\n",
    "\n",
    "    return torch.max(\n",
    "        a_intersection / a_area,\n",
    "        b_intersection / b_area\n",
    "    ).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxes(data, labels, classes, color='orange', min_confidence=0.2, max_overlap=0.5, file=None):\n",
    "    \"\"\"Plots bounding boxes on the given image.\"\"\"\n",
    "    grid_size_x = data.size(dim=2) / S\n",
    "    grid_size_y = data.size(dim=1) / S\n",
    "    m = labels.size(dim=0)\n",
    "    n = labels.size(dim=1)\n",
    "\n",
    "    bboxes = []\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            for k in range((labels.size(dim=2) - C) // 5):\n",
    "                bbox_start = 5 * k + C\n",
    "                bbox_end = 5 * (k + 1) + C\n",
    "                bbox = labels[i, j, bbox_start:bbox_end]\n",
    "                class_index = 0\n",
    "                confidence = labels[i, j, class_index].item() * bbox[4].item()          # pr(c) * IOU\n",
    "                if confidence > min_confidence:\n",
    "                    width = bbox[2] * IMAGE_SIZE[0]\n",
    "                    height = bbox[3] * IMAGE_SIZE[1]\n",
    "                    tl = (\n",
    "                        bbox[0] * IMAGE_SIZE[0] + j * grid_size_x - width / 2,\n",
    "                        bbox[1] * IMAGE_SIZE[1] + i * grid_size_y - height / 2\n",
    "                    )\n",
    "                    bboxes.append([tl, width, height, confidence, class_index])\n",
    "\n",
    "    # Sort by highest to lowest confidence\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[3], reverse=True)\n",
    "\n",
    "    # Calculate IOUs between each pair of boxes\n",
    "    num_boxes = len(bboxes)\n",
    "    iou = [[0 for _ in range(num_boxes)] for _ in range(num_boxes)]\n",
    "    for i in range(num_boxes):\n",
    "        for j in range(num_boxes):\n",
    "            iou[i][j] = get_overlap(bboxes[i], bboxes[j])\n",
    "\n",
    "    # Non-maximum suppression and render image\n",
    "    image = T.ToPILImage()(data)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    discarded = set()\n",
    "    for i in range(num_boxes):\n",
    "        if i not in discarded:\n",
    "            tl, width, height, confidence, class_index = bboxes[i]\n",
    "\n",
    "            # Decrease confidence of other conflicting bboxes\n",
    "            for j in range(num_boxes):\n",
    "                other_class = bboxes[j][4]\n",
    "                if j != i and other_class == class_index and iou[i][j] > max_overlap:\n",
    "                    discarded.add(j)\n",
    "\n",
    "            # Annotate image\n",
    "            draw.rectangle((tl, (tl[0] + width, tl[1] + height)), outline='orange')\n",
    "            text_pos = (max(0, tl[0]), max(0, tl[1] - 11))\n",
    "            text = f'car {round(confidence * 100, 1)}%'\n",
    "            text_bbox = draw.textbbox(text_pos, text)\n",
    "            draw.rectangle(text_bbox, fill='orange')\n",
    "            draw.text(text_pos, text)\n",
    "    if file is None:\n",
    "        image.show()\n",
    "    else:\n",
    "        output_dir = os.path.dirname(file)\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        if not file.endswith('.png'):\n",
    "            file += '.png'\n",
    "        image.save(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/VOCtrainval_11-May-2012.tar\n",
      "Extracting data/VOCtrainval_11-May-2012.tar to data\n",
      "tensor([[[ 1.1490, -0.6957, -0.7360,  ..., -0.5899, -1.0907, -1.5877],\n",
      "         [ 1.5868, -0.5431, -0.6931,  ..., -0.9154, -1.3376, -1.5065],\n",
      "         [ 1.8262,  0.2371,  0.4580,  ..., -1.3887, -1.6523, -1.8551],\n",
      "         ...,\n",
      "         [ 0.5374,  0.5428,  0.5538,  ...,  0.2864,  0.6001,  0.5623],\n",
      "         [ 0.4863,  0.5045,  0.4972,  ..., -0.1126, -0.0948,  0.0390],\n",
      "         [ 0.4158,  0.4906,  0.5705,  ..., -0.0849, -0.1190, -0.1542]],\n",
      "\n",
      "        [[ 1.5046, -0.4341, -0.5148,  ..., -0.3460, -0.9647, -1.5150],\n",
      "         [ 1.9263, -0.2801, -0.5617,  ..., -0.8313, -1.3558, -1.5984],\n",
      "         [ 2.1506,  0.6540,  0.6640,  ..., -1.5073, -1.7435, -1.8723],\n",
      "         ...,\n",
      "         [ 0.7836,  0.8049,  0.8075,  ...,  0.4754,  0.8146,  0.8095],\n",
      "         [ 0.7426,  0.7635,  0.7472,  ...,  0.0874,  0.0620,  0.1910],\n",
      "         [ 0.6749,  0.7299,  0.8382,  ...,  0.1041,  0.0446,  0.0553]],\n",
      "\n",
      "        [[ 1.8191, -0.1673, -0.2464,  ...,  0.0554, -0.6866, -1.2955],\n",
      "         [ 2.2582,  0.0769, -0.1704,  ..., -0.6315, -1.2041, -1.4592],\n",
      "         [ 2.4841,  1.0375,  1.0789,  ..., -1.4127, -1.6225, -1.6930],\n",
      "         ...,\n",
      "         [ 0.8739,  0.8863,  0.8971,  ...,  0.7340,  1.0413,  1.0330],\n",
      "         [ 0.8149,  0.8348,  0.8329,  ...,  0.3511,  0.3392,  0.4816],\n",
      "         [ 0.7299,  0.8136,  0.9077,  ...,  0.3606,  0.3105,  0.3062]]])\n",
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[1.0000, 0.1364, 0.0795, 0.0909, 0.0909, 1.0000, 0.1364, 0.0795,\n",
      "          0.0909, 0.0909, 1.0000],\n",
      "         [1.0000, 0.1307, 0.0795, 0.1477, 0.1136, 1.0000, 0.1307, 0.0795,\n",
      "          0.1477, 0.1136, 1.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets.voc import VOCDetection\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class YoloPascalVocDataset(Dataset):\n",
    "    def __init__(self, set_type, normalize=False, augment=False):\n",
    "        assert set_type in {'train', 'test'}\n",
    "        self.dataset = VOCDetection(\n",
    "            root=DATA_PATH,\n",
    "            year='2012',\n",
    "            image_set=('train' if set_type == 'train' else 'val'),\n",
    "            download=True,\n",
    "            transform=T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Resize(IMAGE_SIZE)\n",
    "            ])\n",
    "        )\n",
    "        self.normalize = normalize\n",
    "        self.augment = augment\n",
    "        self.classes = {\"car\" : 0}\n",
    "\n",
    "        # Preprocess dataset to only include images with cars\n",
    "        self.indices = []\n",
    "        for i in range(len(self.dataset)):\n",
    "            _, label = self.dataset[i]\n",
    "            for _, bbox_pair in enumerate(get_bounding_boxes(label)):\n",
    "                name, _ = bbox_pair\n",
    "                if name == 'car':\n",
    "                    self.indices.append(i)\n",
    "                    break\n",
    "        \n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        index = self.indices[i]\n",
    "        data, label = self.dataset[index]\n",
    "        original_data = data\n",
    "        if self.normalize:\n",
    "            data = TF.normalize(data, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        grid_size_x = data.size(dim=2) / S  # Images in PyTorch have size (channels, height, width)\n",
    "        grid_size_y = data.size(dim=1) / S\n",
    "\n",
    "        # Process bounding boxes into the SxSx(5*B+C) ground truth tensor\n",
    "        boxes = {}\n",
    "        class_names = {}                    # Track what class each grid cell has been assigned to\n",
    "        depth = 5 * B + C                   # 5 numbers per bbox, then one-hot encoding of label\n",
    "        ground_truth = torch.zeros((S, S, depth))\n",
    "        for j, bbox_pair in enumerate(get_bounding_boxes(label)):\n",
    "            name, coords = bbox_pair\n",
    "            if name != 'car':\n",
    "                continue\n",
    "            assert name in self.classes, f\"Unrecognized class '{name}'\"\n",
    "            class_index = self.classes[name]\n",
    "            x_min, x_max, y_min, y_max = coords\n",
    "\n",
    "            # Calculate the position of center of bounding box\n",
    "            mid_x = (x_max + x_min) / 2\n",
    "            mid_y = (y_max + y_min) / 2\n",
    "            col = int(mid_x // grid_size_x)\n",
    "            row = int(mid_y // grid_size_y)\n",
    "\n",
    "            if 0 <= col < S and 0 <= row < S:\n",
    "                cell = (row, col)\n",
    "                if cell not in class_names or name == class_names[cell]:\n",
    "                    # Insert class one-hot encoding into ground truth\n",
    "                    one_hot = torch.zeros(C)\n",
    "                    one_hot[class_index] = 1.0\n",
    "                    ground_truth[row, col, :C] = one_hot\n",
    "                    class_names[cell] = name\n",
    "\n",
    "                    # Insert bounding box into ground truth tensor\n",
    "                    bbox_index = boxes.get(cell, 0)\n",
    "                    if bbox_index < B:\n",
    "                        bbox_truth = (\n",
    "                            (mid_x - col * grid_size_x) / IMAGE_SIZE[0],     # X coord relative to grid square\n",
    "                            (mid_y - row * grid_size_y) / IMAGE_SIZE[1],     # Y coord relative to grid square\n",
    "                            (x_max - x_min) / IMAGE_SIZE[0],                 # Width\n",
    "                            (y_max - y_min) / IMAGE_SIZE[1],                 # Height\n",
    "                            1.0                                              # Confidence\n",
    "                        )\n",
    "\n",
    "                        # Fill all bbox slots with current bbox (starting from current bbox slot, avoid overriding prev)\n",
    "                        # This prevents having \"dead\" boxes (zeros) at the end, which messes up IOU loss calculations\n",
    "                        bbox_start = 5 * bbox_index + C\n",
    "                        ground_truth[row, col, bbox_start:] = torch.tensor(bbox_truth).repeat(B - bbox_index)\n",
    "                        boxes[cell] = bbox_index + 1\n",
    "\n",
    "        return data, ground_truth, original_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "\n",
    "# Display data\n",
    "obj_classes = load_class_array()\n",
    "train_set = YoloPascalVocDataset('train', normalize=True, augment=False)\n",
    "\n",
    "negative_labels = 0\n",
    "smallest = 0\n",
    "largest = 0\n",
    "for data, label, _ in train_set:\n",
    "    negative_labels += torch.sum(label < 0).item()\n",
    "    smallest = min(smallest, torch.min(data).item())\n",
    "    largest = max(largest, torch.max(data).item())\n",
    "    print(data)\n",
    "    print(label)\n",
    "    break\n",
    "    # plot_boxes(data, label, obj_classes, max_overlap=float('inf'))\n",
    "# print('num_negatives', negative_labels)\n",
    "# print('dist', smallest, largest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TinyissimoYOLO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.depth = B * 5 + C\n",
    "\n",
    "        layers = [\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=3),                   # Conv 1\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),                           # Conv 2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),                           # Conv 3\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),                          # Conv 4\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        ]\n",
    "\n",
    "        layers += [\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*5*5, 256),                            # Linear 1\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, S * S * self.depth),                      # Linear 2\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.reshape(\n",
    "            self.model.forward(x),\n",
    "            (x.size(dim=0), S, S, self.depth)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 4, 11])\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy input of the same size as your images\n",
    "dummy_input = torch.randn(1, 3, 88, 88)\n",
    "\n",
    "# Create an instance of your model and run the dummy input through it\n",
    "model = TinyissimoYOLO()\n",
    "output = model(dummy_input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TinyissimoYOLO                           [1, 4, 4, 11]             --\n",
       "├─Sequential: 1-1                        --                        --\n",
       "│    └─Conv2d: 2-1                       [1, 16, 92, 92]           448\n",
       "│    └─ReLU: 2-2                         [1, 16, 92, 92]           --\n",
       "│    └─MaxPool2d: 2-3                    [1, 16, 46, 46]           --\n",
       "│    └─Conv2d: 2-4                       [1, 32, 46, 46]           4,640\n",
       "│    └─ReLU: 2-5                         [1, 32, 46, 46]           --\n",
       "│    └─MaxPool2d: 2-6                    [1, 32, 23, 23]           --\n",
       "│    └─Conv2d: 2-7                       [1, 64, 23, 23]           18,496\n",
       "│    └─ReLU: 2-8                         [1, 64, 23, 23]           --\n",
       "│    └─MaxPool2d: 2-9                    [1, 64, 11, 11]           --\n",
       "│    └─Conv2d: 2-10                      [1, 128, 11, 11]          73,856\n",
       "│    └─ReLU: 2-11                        [1, 128, 11, 11]          --\n",
       "│    └─MaxPool2d: 2-12                   [1, 128, 5, 5]            --\n",
       "│    └─Flatten: 2-13                     [1, 3200]                 --\n",
       "│    └─Linear: 2-14                      [1, 256]                  819,456\n",
       "│    └─ReLU: 2-15                        [1, 256]                  --\n",
       "│    └─Linear: 2-16                      [1, 176]                  45,232\n",
       "==========================================================================================\n",
       "Total params: 962,128\n",
       "Trainable params: 962,128\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 33.20\n",
       "==========================================================================================\n",
       "Input size (MB): 0.09\n",
       "Forward/backward pass size (MB): 2.02\n",
       "Params size (MB): 3.85\n",
       "Estimated Total Size (MB): 5.96\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "model = TinyissimoYOLO()\n",
    "summary(model, (1, 3, 88, 88))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class SumSquaredErrorLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l_coord = 5\n",
    "        self.l_noobj = 0.5\n",
    "\n",
    "    def forward(self, p, a):\n",
    "        # Calculate IOU of each predicted bbox against the ground truth bbox\n",
    "        iou = get_iou(p, a)                     # (batch, S, S, B, B)\n",
    "        max_iou = torch.max(iou, dim=-1)[0]     # (batch, S, S, B)\n",
    "\n",
    "        # Get masks\n",
    "        bbox_mask = bbox_attr(a, 4) > 0.0\n",
    "        p_template = bbox_attr(p, 4) > 0.0\n",
    "        obj_i = bbox_mask[..., 0:1]         # 1 if grid I has any object at all\n",
    "        responsible = torch.zeros_like(p_template).scatter_(       # (batch, S, S, B)\n",
    "            -1,\n",
    "            torch.argmax(max_iou, dim=-1, keepdim=True),                # (batch, S, S, B)\n",
    "            value=1                         # 1 if bounding box is \"responsible\" for predicting the object\n",
    "        )\n",
    "        obj_ij = obj_i * responsible        # 1 if object exists AND bbox is responsible\n",
    "        noobj_ij = ~obj_ij                  # Otherwise, confidence should be 0\n",
    "\n",
    "        # XY position losses\n",
    "        x_losses = mse_loss(\n",
    "            obj_ij * bbox_attr(p, 0),\n",
    "            obj_ij * bbox_attr(a, 0)\n",
    "        )\n",
    "        y_losses = mse_loss(\n",
    "            obj_ij * bbox_attr(p, 1),\n",
    "            obj_ij * bbox_attr(a, 1)\n",
    "        )\n",
    "        pos_losses = x_losses + y_losses\n",
    "        # print('pos_losses', pos_losses.item())\n",
    "\n",
    "        # Bbox dimension losses\n",
    "        p_width = bbox_attr(p, 2)\n",
    "        a_width = bbox_attr(a, 2)\n",
    "        width_losses = mse_loss(\n",
    "            obj_ij * torch.sign(p_width) * torch.sqrt(torch.abs(p_width) + EPSILON),\n",
    "            obj_ij * torch.sqrt(a_width)\n",
    "        )\n",
    "        p_height = bbox_attr(p, 3)\n",
    "        a_height = bbox_attr(a, 3)\n",
    "        height_losses = mse_loss(\n",
    "            obj_ij * torch.sign(p_height) * torch.sqrt(torch.abs(p_height) + EPSILON),\n",
    "            obj_ij * torch.sqrt(a_height)\n",
    "        )\n",
    "        dim_losses = width_losses + height_losses\n",
    "        # print('dim_losses', dim_losses.item())\n",
    "\n",
    "        # Confidence losses (target confidence is IOU)\n",
    "        obj_confidence_losses = mse_loss(\n",
    "            obj_ij * bbox_attr(p, 4),\n",
    "            obj_ij * torch.ones_like(max_iou)\n",
    "        )\n",
    "        # print('obj_confidence_losses', obj_confidence_losses.item())\n",
    "        noobj_confidence_losses = mse_loss(\n",
    "            noobj_ij * bbox_attr(p, 4),\n",
    "            torch.zeros_like(max_iou)\n",
    "        )\n",
    "        # print('noobj_confidence_losses', noobj_confidence_losses.item())\n",
    "\n",
    "        # Classification losses\n",
    "        class_losses = mse_loss(\n",
    "            obj_i * p[..., :C],\n",
    "            obj_i * a[..., :C]\n",
    "        )\n",
    "        # print('class_losses', class_losses.item())\n",
    "\n",
    "        total = self.l_coord * (pos_losses + dim_losses) \\\n",
    "                + obj_confidence_losses \\\n",
    "                + self.l_noobj * noobj_confidence_losses \\\n",
    "                + class_losses\n",
    "        return total / BATCH_SIZE\n",
    "\n",
    "\n",
    "def mse_loss(a, b):\n",
    "    flattened_a = torch.flatten(a, end_dim=-2)\n",
    "    flattened_b = torch.flatten(b, end_dim=-2).expand_as(flattened_a)\n",
    "    return F.mse_loss(\n",
    "        flattened_a,\n",
    "        flattened_b,\n",
    "        reduction='sum'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/VOCtrainval_11-May-2012.tar\n",
      "Extracting data/VOCtrainval_11-May-2012.tar to data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arch/ObjDct_Repo/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.autograd.set_detect_anomaly(True)         # Check for nan loss\n",
    "writer = SummaryWriter()\n",
    "now = datetime.now()\n",
    "\n",
    "model = TinyissimoYOLO().to(device)\n",
    "loss_function = SumSquaredErrorLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE\n",
    "    )\n",
    "\n",
    "\n",
    "train_set = YoloPascalVocDataset('train', normalize=True, augment=False)\n",
    "train_loader = DataLoader(\n",
    "        train_set,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=8,\n",
    "        persistent_workers=True,\n",
    "        drop_last=True,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([32, 3, 88, 88])\n",
      "Label shape: torch.Size([32, 4, 4, 11])\n"
     ]
    }
   ],
   "source": [
    "data, label, _ = next(iter(train_loader))\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Label shape: {label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders\n",
    "root = os.path.join(\n",
    "    'models',\n",
    "    'yolo_v1',\n",
    "    now.strftime('%m_%d_%Y'),\n",
    "    now.strftime('%H_%M_%S')\n",
    ")\n",
    "weight_dir = os.path.join(root, 'weights')\n",
    "if not os.path.isdir(weight_dir):\n",
    "    os.makedirs(weight_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "train_losses = np.empty((2, 0))\n",
    "train_errors = np.empty((2, 0))\n",
    "\n",
    "def save_metrics():\n",
    "        np.save(os.path.join(root, 'train_losses'), train_losses)\n",
    "        np.save(os.path.join(root, 'train_errors'), train_errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 135/135 [03:57<00:00,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(WARMUP_EPOCHS + EPOCHS), desc='Epoch'):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, labels, _ in tqdm(train_loader, desc='Train', leave=False):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model.forward(data)\n",
    "        loss = loss_function(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() / len(train_loader)\n",
    "        \n",
    "        del data, labels\n",
    "\n",
    "    # Step and graph scheduler once an epoch\n",
    "    # writer.add_scalar('Learning Rate', scheduler.get_last_lr()[0], epoch)\n",
    "    # scheduler.step()\n",
    "\n",
    "    train_losses = np.append(train_losses, [[epoch], [train_loss]], axis=1)\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "\n",
    "save_metrics()\n",
    "torch.save(model.state_dict(), os.path.join(weight_dir, 'final'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
