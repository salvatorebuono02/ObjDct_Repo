{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import gdown\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "from xml.etree import ElementTree\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "raw_data_path = \"raw_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists(raw_data_path):\n",
    "    print('[INFO] Raw data directory exists, skiping download.')\n",
    "else:\n",
    "    os.makedirs(raw_data_path)\n",
    "    print('[INFO] Raw data directory is been created.')\n",
    "    print('[INFO] Downloading data...\\n')\n",
    "    gdown.download('https://drive.google.com/uc?id=1KoQzqR20qvIXDf1qsXCHGxD003IPmXMw', output=os.path.join(raw_data_path, 'Annotations.zip'))\n",
    "    gdown.download('https://drive.google.com/file/d/1wq0FQCBsbrnf-sJfloi7IR9tBH87GGGS/view?usp=sharing', output=os.path.join(raw_data_path, 'JPEGImages-test.zip'),fuzzy=True)\n",
    "    gdown.download('https://drive.google.com/file/d/1NVRSBm3RfpGGtZvgLJG5e_XD9uP4DZmI/view?usp=sharing', output=os.path.join(raw_data_path, 'JPEGImages-trainval.zip'),fuzzy=True)\n",
    "    print('[INFO] Data is been downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dior_data_path = \"dior_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Extracting the zip data files\n",
    "if os.path.exists(dior_data_path):\n",
    "    print('[INFO] DIOR data directory exists, skiping extraction.')\n",
    "else:\n",
    "    os.makedirs(dior_data_path)\n",
    "    for i in os.listdir(raw_data_path):\n",
    "        filename = os.path.join(raw_data_path, i) \n",
    "        shutil.unpack_archive(filename=filename, extract_dir=dior_data_path)\n",
    "        print(f'[INFO] File \"{filename}\" is been extracted to \"{dior_data_path}\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Path for all the data\n",
    "annot_data_path = 'dior_data/Annotations/Horizontal Bounding Boxes'\n",
    "trainval_data_path = 'dior_data/JPEGImages-trainval'\n",
    "test_data_path = 'dior_data/JPEGImages-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a list of annotation files\n",
    "annot_file_list = sorted([os.path.join(annot_data_path, i) for i in os.listdir(annot_data_path) if '.xml' in i])\n",
    "annot_file_list[:5], annot_file_list[-5:], len(annot_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a list of training and validation images\n",
    "trainval_file_list = sorted([os.path.join(trainval_data_path, i) for i in os.listdir(trainval_data_path) if '.jpg' in i])\n",
    "trainval_file_list[:5], trainval_file_list[-5:], len(trainval_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a list of testing images\n",
    "test_file_list = sorted([os.path.join(test_data_path, i) for i in os.listdir(test_data_path) if '.jpg' in i])\n",
    "test_file_list[:5], test_file_list[-5:], len(test_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Combining all the images path in one list\n",
    "image_data_file_list = np.concatenate((trainval_file_list, test_file_list))\n",
    "image_data_file_list[:5], image_data_file_list[-5:], len(image_data_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Extracting data from annotation files\n",
    "meta_list = [] # To store general info for every image\n",
    "object_list_train = [] # To store object classes info of train dataset\n",
    "object_list_test = [] # To store object classes info of test dataset\n",
    "\n",
    "for file in tqdm(annot_file_list):\n",
    "    meta_dict = {}\n",
    "    root = ElementTree.parse(file).getroot()\n",
    "    \n",
    "    # Filename - extracted\n",
    "    for path in image_data_file_list:\n",
    "        if root.find('filename').text in path:\n",
    "            meta_dict['filename'] = path\n",
    "            meta_dict['split_type'] = path.split('/')[1]\n",
    "    \n",
    "    # Width - extracted\n",
    "    meta_dict['width'] = int(root.find('size').find('width').text)\n",
    "    \n",
    "    # Height - extracted\n",
    "    meta_dict['height'] = int(root.find('size').find('height').text)\n",
    "    \n",
    "    # Objects - extracted and combined into a single string\n",
    "    meta_dict['objects'] = ', '.join(np.unique([obj.find('name').text for obj in root.findall('object')]))\n",
    "    meta_list.append(meta_dict)\n",
    "    \n",
    "    # Collecting all the object classes instance and counting total appearance\n",
    "    for obj in root.findall('object'):\n",
    "        if meta_dict['split_type'] == 'JPEGImages-trainval':\n",
    "            object_list_train.append(obj.find('name').text)\n",
    "        elif meta_dict['split_type'] == 'JPEGImages-test':\n",
    "            object_list_test.append(obj.find('name').text)\n",
    "    \n",
    "# Counting the instance for every object class\n",
    "object_instance_list_train = Counter(sorted(object_list_train))\n",
    "object_instance_list_test = Counter(sorted(object_list_test))\n",
    "    \n",
    "# Collecting Class list and indexing it also in a sequence\n",
    "class_dict = {k: v for v, k in enumerate(sorted(np.unique(object_list_train)))}\n",
    "\n",
    "meta_list[:5], object_instance_list_train, object_instance_list_test, class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe from the extracted data\n",
    "meta_df = pd.DataFrame(meta_list)\n",
    "meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Size of the images width: {meta_df.width.unique()[0]} and height: {meta_df.height.unique()[0]}')\n",
    "print(f'Total number of classes with all possible combination: {len(meta_df.objects.unique())}')\n",
    "print(f'Total length of the training/validation dataset: {len(meta_df[meta_df[\"split_type\"] == \"JPEGImages-trainval\"])} and testing dataset: {len(meta_df[meta_df[\"split_type\"] == \"JPEGImages-test\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Top 20 classes in the dataset\n",
    "meta_df.objects.value_counts()[:20].plot(kind='barh').invert_yaxis()\n",
    "plt.xlabel('Images (Count)')\n",
    "plt.title('Top 20 Objects Classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "meta_df[meta_df[\"split_type\"] == \"JPEGImages-trainval\"].objects.value_counts()[:20].plot(kind='barh').invert_yaxis()\n",
    "plt.xlabel('Images (Count)')\n",
    "plt.title('Train/Val Dataset', fontsize=16)\n",
    "plt.subplot(1, 2, 2)\n",
    "meta_df[meta_df[\"split_type\"] == \"JPEGImages-test\"].objects.value_counts()[:20].plot(kind='barh').invert_yaxis()\n",
    "plt.xlabel('Images (Count)')\n",
    "plt.title('Test Dataset', fontsize=16)\n",
    "plt.suptitle('Top 20 Objects Classes in the Dataset', fontsize=20, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(28, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(list(object_instance_list_train.keys()), list(object_instance_list_train.values()))\n",
    "plt.xlabel('Objects (Count)')\n",
    "plt.title('Train/Val Dataset', fontsize=16)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(list(object_instance_list_test.keys()), list(object_instance_list_test.values()))\n",
    "plt.xlabel('Objects (Count)')\n",
    "plt.title('Test Dataset', fontsize=16)\n",
    "plt.suptitle('Total Count of Object Instances Per Class in the Dataset', fontsize=20, fontweight='bold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualizing the classes - rerun the code for different classes.\n",
    "viz_class = random.sample(meta_df.objects.tolist(), 1)[0]\n",
    "viz_list = meta_df[meta_df['objects'] == viz_class].filename.tolist()\n",
    "plt.figure(figsize=(20, 5))\n",
    "rand = random.sample(viz_list, 4)\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.imshow(plt.imread(rand[i]))\n",
    "    plt.suptitle(f'Objects in the Image: {viz_class}', fontsize=20, fontweight='bold')\n",
    "    plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a function for extracting data\n",
    "def extract_data_from_xml(xml_file: str):\n",
    "    \"\"\"\n",
    "    A function to extract data like filename, size, classes and bboxes from xml file.\n",
    "    \n",
    "    Parameters: xml_file: str, A string containing the path to the file.\n",
    "    \n",
    "    Returns: data_dict: dict, A dict containing all the extracted data.\n",
    "    \"\"\"\n",
    "    root = ElementTree.parse(xml_file).getroot()\n",
    "    \n",
    "    # Creating dict and list to store data\n",
    "    data_dict = {}\n",
    "    data_dict['bboxes'] = []\n",
    "    \n",
    "    # Reading the xml file\n",
    "    for element in root:\n",
    "        # Getting the filename\n",
    "        if element.tag == 'filename':\n",
    "            data_dict['filename'] = element.text\n",
    "        \n",
    "        # Getting the image size\n",
    "        elif element.tag == 'size':\n",
    "            image_size = []\n",
    "            for size_element in element:\n",
    "                image_size.append(int(size_element.text))\n",
    "            data_dict['image_size'] = image_size\n",
    "        \n",
    "        # Getting the bounding box\n",
    "        elif element.tag == 'object':\n",
    "            bbox = {}\n",
    "            for obj_element in element:\n",
    "                # Object or Class name\n",
    "                if obj_element.tag == 'name':\n",
    "                    bbox['class'] = obj_element.text\n",
    "                # Object bounding box \n",
    "                elif obj_element.tag == 'bndbox':\n",
    "                    for bbox_element in obj_element:\n",
    "                        bbox[bbox_element.tag] = int(bbox_element.text)\n",
    "            data_dict['bboxes'].append(bbox)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "example = extract_data_from_xml(annot_file_list[1])\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a function to create a YOLO format annotation\n",
    "def convert_dict_to_yolo(data_dict: dict):\n",
    "    \"\"\"\n",
    "    A function to convert the extracted data dict into a text file as per the YOLO format.\n",
    "    The final text file is saved in the directory \"dior_data/yolo_annotations/data_dict['filename'].txt\".\n",
    "    \n",
    "    Parameters: data_dict: dict, A dict containing the data.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    # Reading the bounding box data\n",
    "    for bbox in data_dict['bboxes']:\n",
    "        try:\n",
    "            class_id = class_dict[bbox['class']]\n",
    "        except KeyError:\n",
    "            print(f'Invalid Class. Object class: \"{bbox[\"class\"]}\" not present in the class list.')\n",
    "            \n",
    "        # Transforming the bbox in Yolo format [X, Y, W, H]\n",
    "        img_w, img_h, _ = data_dict['image_size'] # Normalizing the bbox using image size\n",
    "        \n",
    "        x_center = ((bbox['xmin'] + bbox['xmax']) / 2) / img_w\n",
    "        y_center = ((bbox['ymin'] + bbox['ymax']) / 2) / img_h\n",
    "        width = (bbox['xmax'] - bbox['xmin']) / img_w \n",
    "        height = (bbox['ymax'] - bbox['ymin']) / img_h\n",
    "        \n",
    "        # Writing the new data to the data list in Yolo format\n",
    "        data.append(f'{class_id} {x_center:.3f} {y_center:.3f} {width:.3f} {height:.3f}')\n",
    "        \n",
    "    # File name for saving the text file(same as xml and jpg file name)\n",
    "    yolo_annot_dir = os.path.join('dior_data', 'yolo_annotations')\n",
    "    if not os.path.exists(yolo_annot_dir):\n",
    "        os.makedirs(yolo_annot_dir)\n",
    "    save_file_name = os.path.join(yolo_annot_dir, data_dict['filename'].replace('jpg', 'txt'))\n",
    "    \n",
    "    # Saving the yolo annotation in a text file\n",
    "    f = open(save_file_name, 'w+')\n",
    "    f.write('\\n'.join(data))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Converting all the xml files into Yolo format text files\n",
    "print('[INFO] Annotation extraction and creation into Yolo has started.')\n",
    "for annot_file in tqdm(annot_file_list):\n",
    "    data_dict = extract_data_from_xml(annot_file)\n",
    "    convert_dict_to_yolo(data_dict)\n",
    "print('[INFO] All the annotation are converted into Yolo format.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "yolo_annot_path = 'dior_data/yolo_annotations'\n",
    "yolo_annot_file_list = sorted([os.path.join(yolo_annot_path, i) for i in os.listdir(yolo_annot_path) if '.txt' in i])\n",
    "yolo_annot_file_list[:5], yolo_annot_file_list[-5:], len(yolo_annot_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class_dict_idx = dict(zip(class_dict.values(), class_dict.keys()))\n",
    "class_dict_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_bboxes(img_file: str, annot_file: str, class_dict: dict):\n",
    "    \"\"\"\n",
    "    A function to plot the bounding boxes amd their object classes onto the image.\n",
    "    \n",
    "    Parameters:\n",
    "        img_file: str, A string containing the path to the image file.\n",
    "        annot_file: str, A string containing the path to the annotation file in yolo format.\n",
    "        class_dict: dict, A dict containing the classes in the similar sequence as per the annot_file.\n",
    "    \"\"\"\n",
    "    # Reading the image and annot file\n",
    "    image = cv2.imread(img_file)\n",
    "    img_h, img_w, _ = image.shape\n",
    "    \n",
    "    with open(annot_file, 'r') as f:\n",
    "        data = f.read().split('\\n')\n",
    "        data = [i.split(' ') for i in data]\n",
    "        data = [[float(j) for j in i] for i in data]\n",
    "    \n",
    "    # Calculating the bbox in Pascal VOC format\n",
    "    for bbox in data:\n",
    "        class_idx, x_center, y_center, width, height = bbox\n",
    "        xmin = int((x_center - width / 2) * img_w)\n",
    "        ymin = int((y_center - height / 2) * img_h)\n",
    "        xmax = int((x_center + width / 2) * img_w)\n",
    "        ymax = int((y_center + height / 2) * img_h)\n",
    "        \n",
    "        # Correcting bbox if out of image size\n",
    "        if xmin < 0:\n",
    "            xmin = 0\n",
    "        if ymin < 0:\n",
    "            ymin = 0\n",
    "        if xmax > img_w - 1:\n",
    "            xmax = img_w - 1\n",
    "        if ymax > img_h - 1:\n",
    "            ymax = img_h - 1\n",
    "        \n",
    "        # Creating the box and label for the image\n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (255, 255, 0), 2)\n",
    "        cv2.putText(image, class_dict[class_idx], (xmin, 0 if ymin-10 < 0 else ymin-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 0), 2)\n",
    "    \n",
    "    # Displaying the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 8))\n",
    "rand_int = random.sample(range(len(yolo_annot_file_list)), 3)\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plot_bboxes(image_data_file_list[rand_int[i]], yolo_annot_file_list[rand_int[i]], class_dict_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Creating images and labels directory into the data directory\n",
    "root_dir = 'dataset'\n",
    "image_dir = 'dataset/images'\n",
    "label_dir = 'dataset/labels'\n",
    "img_train_dir = 'dataset/images/train'\n",
    "label_train_dir = 'dataset/labels/train'\n",
    "img_test_dir = 'dataset/images/test'\n",
    "label_test_dir = 'dataset/labels/test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Moving the training labels\n",
    "if not os.path.exists(label_train_dir):\n",
    "    os.makedirs(label_train_dir)\n",
    "\n",
    "for filepath in tqdm(trainval_file_list):\n",
    "    file_path = os.path.join('dior_data/yolo_annotations', filepath.replace('jpg', 'txt').split('/')[-1])\n",
    "    if os.path.isfile(file_path):\n",
    "        shutil.copy(file_path, label_train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Moving the validation labels[20% of test dataset]\n",
    "if not os.path.exists(label_test_dir):\n",
    "    os.makedirs(label_test_dir)\n",
    "\n",
    "for filepath in tqdm(test_file_list):\n",
    "    file_path = os.path.join('dior_data/yolo_annotations', filepath.replace('jpg', 'txt').split('/')[-1])\n",
    "    if os.path.isfile(file_path):\n",
    "        shutil.move(file_path, label_test_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
