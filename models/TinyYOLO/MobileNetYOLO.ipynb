{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a tiny version of YOLO with DIOR dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement YOLO architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, t):\n",
    "        return torch.pow(t, 1) + torch.pow(t, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyissimoYOLO(\n",
      "  (backbone): MobileNet(\n",
      "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Sequential(\n",
      "      (0): DepthwiseSeparableConv(\n",
      "        (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (pointwise): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (lin_relu): LinearActivation()\n",
      "      )\n",
      "      (1): DepthwiseSeparableConv(\n",
      "        (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "        (pointwise): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (lin_relu): LinearActivation()\n",
      "      )\n",
      "      (2): DepthwiseSeparableConv(\n",
      "        (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "        (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (lin_relu): LinearActivation()\n",
      "      )\n",
      "      (3): DepthwiseSeparableConv(\n",
      "        (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "        (pointwise): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (lin_relu): LinearActivation()\n",
      "      )\n",
      "      (4): DepthwiseSeparableConv(\n",
      "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (lin_relu): LinearActivation()\n",
      "      )\n",
      "      (5): DepthwiseSeparableConv(\n",
      "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "        (pointwise): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (lin_relu): LinearActivation()\n",
      "      )\n",
      "      (6): DepthwiseSeparableConv(\n",
      "        (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "        (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (lin_relu): LinearActivation()\n",
      "      )\n",
      "      (7): DepthwiseSeparableConv(\n",
      "        (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "        (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (lin_relu): LinearActivation()\n",
      "      )\n",
      "      (8): DepthwiseSeparableConv(\n",
      "        (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "        (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (lin_relu): LinearActivation()\n",
      "      )\n",
      "      (9): DepthwiseSeparableConv(\n",
      "        (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "        (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (lin_relu): LinearActivation()\n",
      "      )\n",
      "      (10): DepthwiseSeparableConv(\n",
      "        (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "        (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (lin_relu): LinearActivation()\n",
      "      )\n",
      "      (11): DepthwiseSeparableConv(\n",
      "        (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "        (pointwise): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (lin_relu): LinearActivation()\n",
      "      )\n",
      "      (12): DepthwiseSeparableConv(\n",
      "        (depthwise): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "        (pointwise): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (lin_relu): LinearActivation()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fclayers): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=36864, out_features=256, bias=True)\n",
      "    (2): LinearActivation()\n",
      "    (3): Linear(in_features=256, out_features=176, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Definizione della MobileNet\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(DepthwiseSeparableConv, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.lin_relu = LinearActivation()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        x = self.bn(x)\n",
    "        return self.lin_relu(x)\n",
    "\n",
    "class MobileNet(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super(MobileNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1, bias=False)  # Cambiato stride da 2 a 1\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = self._make_layers(in_channels=32)\n",
    "    \n",
    "    def _make_layers(self, in_channels):\n",
    "        layers = []\n",
    "        config = [\n",
    "            # out_channels, stride\n",
    "            (64, 1),\n",
    "            (128, 2),\n",
    "            (128, 1),\n",
    "            (256, 2),\n",
    "            (256, 1),\n",
    "            (512, 2),\n",
    "            (512, 1),\n",
    "            (512, 1),\n",
    "            (512, 1),\n",
    "            (512, 1),\n",
    "            (512, 1),\n",
    "            (1024, 2),\n",
    "            (1024, 1)\n",
    "        ]\n",
    "        \n",
    "        for out_channels, stride in config:\n",
    "            layers.append(DepthwiseSeparableConv(in_channels, out_channels, stride))\n",
    "            in_channels = out_channels\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# TinyissimoYOLO con MobileNet come backbone\n",
    "class TinyissimoYOLO(nn.Module):\n",
    "    def __init__(self, B=2, num_classes=1, S=4):\n",
    "        super(TinyissimoYOLO, self).__init__()\n",
    "        \n",
    "        # Sostituzione del backbone convoluzionale con MobileNet\n",
    "        self.backbone = MobileNet(in_channels=1)\n",
    "\n",
    "        # Calcolo delle dimensioni in output (con input 80x80)\n",
    "        self.fclayers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024*6*6, 256),  # 5x5 è il risultato di 80x80 attraverso MobileNet\n",
    "            LinearActivation(),\n",
    "            nn.Linear(256, S*S*(num_classes + 5*B)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.fclayers(x)\n",
    "        return x\n",
    "\n",
    "# Esempio di creazione del modello\n",
    "model = TinyissimoYOLO()\n",
    "\n",
    "# Stampa del modello\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "TinyissimoYOLO                                [1, 176]                  --\n",
       "├─MobileNet: 1-1                              [1, 1024, 6, 6]           --\n",
       "│    └─Conv2d: 2-1                            [1, 32, 88, 88]           288\n",
       "│    └─BatchNorm2d: 2-2                       [1, 32, 88, 88]           64\n",
       "│    └─ReLU: 2-3                              [1, 32, 88, 88]           --\n",
       "│    └─Sequential: 2-4                        [1, 1024, 6, 6]           --\n",
       "│    │    └─DepthwiseSeparableConv: 3-1       [1, 64, 88, 88]           2,464\n",
       "│    │    └─DepthwiseSeparableConv: 3-2       [1, 128, 44, 44]          9,024\n",
       "│    │    └─DepthwiseSeparableConv: 3-3       [1, 128, 44, 44]          17,792\n",
       "│    │    └─DepthwiseSeparableConv: 3-4       [1, 256, 22, 22]          34,432\n",
       "│    │    └─DepthwiseSeparableConv: 3-5       [1, 256, 22, 22]          68,352\n",
       "│    │    └─DepthwiseSeparableConv: 3-6       [1, 512, 11, 11]          134,400\n",
       "│    │    └─DepthwiseSeparableConv: 3-7       [1, 512, 11, 11]          267,776\n",
       "│    │    └─DepthwiseSeparableConv: 3-8       [1, 512, 11, 11]          267,776\n",
       "│    │    └─DepthwiseSeparableConv: 3-9       [1, 512, 11, 11]          267,776\n",
       "│    │    └─DepthwiseSeparableConv: 3-10      [1, 512, 11, 11]          267,776\n",
       "│    │    └─DepthwiseSeparableConv: 3-11      [1, 512, 11, 11]          267,776\n",
       "│    │    └─DepthwiseSeparableConv: 3-12      [1, 1024, 6, 6]           530,944\n",
       "│    │    └─DepthwiseSeparableConv: 3-13      [1, 1024, 6, 6]           1,059,840\n",
       "├─Sequential: 1-2                             [1, 176]                  --\n",
       "│    └─Flatten: 2-5                           [1, 36864]                --\n",
       "│    └─Linear: 2-6                            [1, 256]                  9,437,440\n",
       "│    └─LinearActivation: 2-7                  [1, 256]                  --\n",
       "│    └─Linear: 2-8                            [1, 176]                  45,232\n",
       "===============================================================================================\n",
       "Total params: 12,679,152\n",
       "Trainable params: 12,679,152\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 364.64\n",
       "===============================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 40.53\n",
       "Params size (MB): 50.72\n",
       "Estimated Total Size (MB): 91.28\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(1, 1, 88, 88))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection over Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(boxes_preds, boxes_labels):\n",
    "    \"\"\"\n",
    "    Calculates intersection over union\n",
    "    \n",
    "    Parameters:\n",
    "        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)    \n",
    "    Returns:\n",
    "        tensor: Intersection over union for all examples\n",
    "    \"\"\"\n",
    "    # boxes_preds shape is (N, 4) where N is the number of bboxes\n",
    "    #boxes_labels shape is (n, 4)\n",
    "    \n",
    "    box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "    box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "    box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "    box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "    box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "    box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "    box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "    box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "    \n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "    #print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
    "    \n",
    "    #.clamp(0) is for the case when they don't intersect. Since when they don't intersect, one of these will be negative so that should become 0\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "    #print(f\"intersection: {intersection}\")\n",
    "\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "    #print(f\"box1_area: {box1_area}, box2_area: {box2_area}\")\n",
    "    \n",
    "    return intersection / (box1_area + box2_area - intersection + 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non Max Suppression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input**: A list of Proposal boxes B, corresponding confidence scores S and overlap threshold N.\n",
    "\n",
    "**Output**: A list of filtered proposals D.\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "1.  Select the proposal with highest confidence score, remove it from B and add it to the final proposal list D. (Initially D is empty).\n",
    "2.  Now compare this proposal with all the proposals — calculate the IOU (Intersection over Union) of this proposal with every other proposal. If the IOU is greater than the threshold N, remove that proposal from B.\n",
    "3.  Again take the proposal with the highest confidence from the remaining proposals in B and remove it from B and add it to D.\n",
    "4.  Once again calculate the IOU of this proposal with all the proposals in B and eliminate the boxes which have high IOU than threshold.\n",
    "5.  This process is repeated until there are no more proposals left in B.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(bboxes, iou_threshold, threshold):\n",
    "    \"\"\"\n",
    "    Does Non Max Suppression given bboxes\n",
    "    Parameters:\n",
    "        bboxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [class_pred, prob_score, x_center, y_center, width, height]\n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        threshold (float): threshold to remove predicted bboxes (independent of IoU) \n",
    "    Returns:\n",
    "        list: bboxes after performing NMS given a specific IoU threshold\n",
    "    \"\"\"\n",
    "\n",
    "    assert type(bboxes) == list\n",
    "\n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        chosen_box = bboxes.pop(0)\n",
    "\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            if box[0] != chosen_box[0]\n",
    "            or intersection_over_union(\n",
    "                torch.tensor(chosen_box[2:]),\n",
    "                torch.tensor(box[2:]),\n",
    "            )\n",
    "            < iou_threshold\n",
    "        ]\n",
    "\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "    #print(f\"bboxes_after_nms: {bboxes_after_nms}\")\n",
    "\n",
    "    return bboxes_after_nms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It describes a trade-off between precision and recall.\n",
    "\n",
    "**Precision**, also referred to as the positive predictive value, describes how well a model predicts the positive class. \n",
    "$$Precision=\\frac{TP}{TP+FP}$$\n",
    ">   Of all bounding box **predictions**, what fraction was actually correct?\n",
    "\n",
    "**Recall**, also called sensitivity tells you if your model made the right predictions when it should have. \n",
    "$$Recall=\\frac{TP}{TP+FN}$$\n",
    ">   Of all **target** bounding boxes, what fraction did we correctly detect?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(\n",
    "    pred_boxes, true_boxes, iou_threshold=0.5, num_classes=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates mean average precision \n",
    "    Parameters:\n",
    "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [train_idx, class_prediction, prob_score, x_center, y_center, width, height]\n",
    "        true_boxes (list): Similar as pred_boxes except all the correct ones \n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        num_classes (int): number of classes\n",
    "    Returns:\n",
    "        float: mAP value across all classes given a specific IoU threshold \n",
    "    \"\"\"\n",
    "\n",
    "    # list storing all AP for respective classes\n",
    "    average_precisions = []\n",
    "\n",
    "    # used for numerical stability later on\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # Go through all predictions and targets,\n",
    "        # and only add the ones that belong to the\n",
    "        # current class c\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "        #print(f\"{c} class has {len(detections)} detections\")\n",
    "\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "        #print(f\"{c} class has {len(ground_truths)} ground truths\")\n",
    "\n",
    "        # find the amount of bboxes for each training example\n",
    "        # Counter here finds how many ground truth bboxes we get\n",
    "        # for each training example, so let's say img 0 has 3,\n",
    "        # img 1 has 5 then we will obtain a dictionary with:\n",
    "        # amount_bboxes = {0:3, 1:5}\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "        #print(f\"{c} class has {len(amount_bboxes)} amount bboxes\")\n",
    "\n",
    "        # We then go through each key, val in this dictionary\n",
    "        # and convert to the following (w.r.t same example):\n",
    "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # sort by box probabilities which is index 2\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "        #print(f\"{c} class has {total_true_bboxes} total true bboxes\")\n",
    "        # If none exists for this class then we can safely skip\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # Only take out the ground_truths that have the same\n",
    "            # training idx as detection\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            #print(f\"{c} class has {num_gts} ground truths for detection {detection_idx}\")\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:])\n",
    "                )\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    # true positive and add this bounding box to seen\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    #These additional detections are considered false positives because they do not correspond to a new, unique object\n",
    "                    #they're essentially \"over-detecting\" an object that has already been correctly identified.\n",
    "                    FP[detection_idx] = 1\n",
    "\n",
    "            # if IOU is lower then the detection is a false positive\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        #[1, 1, 0, 1, 0] -> [1, 2, 2, 3, 3]\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        # torch.trapz for numerical integration\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image, boxes):\n",
    "    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n",
    "    im = np.array(image)\n",
    "    height, width, _ = im.shape\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(1)\n",
    "    # Display the image\n",
    "    ax.imshow(im)\n",
    "\n",
    "    # box[0] is x midpoint, box[2] is width\n",
    "    # box[1] is y midpoint, box[3] is height\n",
    "\n",
    "    # Create a Rectangle patch\n",
    "    for box in boxes:\n",
    "        class_label = int(box[0])\n",
    "        box = box[2:]\n",
    "        assert len(box) == 4, \"Got more values than in x, y, w, h, in a box!\"\n",
    "        upper_left_x = box[0] - box[2] / 2\n",
    "        upper_left_y = box[1] - box[3] / 2\n",
    "        rect = patches.Rectangle(\n",
    "            (upper_left_x * width, upper_left_y * height),\n",
    "            box[2] * width,\n",
    "            box[3] * height,\n",
    "            linewidth=1,\n",
    "            edgecolor=\"r\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add class label text\n",
    "        ax.text(upper_left_x * width, upper_left_y * height, str(class_label), color='r', fontsize=10, verticalalignment='bottom')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and convert boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes(\n",
    "    loader,\n",
    "    model,\n",
    "    iou_threshold,\n",
    "    threshold,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    # make sure model is in eval before get bboxes\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "\n",
    "    for batch_idx, (x, labels) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        true_bboxes = cellboxes_to_boxes(labels)\n",
    "        bboxes = cellboxes_to_boxes(predictions)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            nms_boxes = non_max_suppression(\n",
    "                bboxes[idx],\n",
    "                iou_threshold=iou_threshold,\n",
    "                threshold=threshold,\n",
    "            )\n",
    "\n",
    "            # # Activate only for test\n",
    "            # if batch_idx == 0 and idx == 0:\n",
    "            #    plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n",
    "\n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx] + nms_box)\n",
    "\n",
    "            for box in true_bboxes[idx]:\n",
    "                # many will get converted to 0 pred\n",
    "                if box[1] > threshold:\n",
    "                    all_true_boxes.append([train_idx] + box)\n",
    "\n",
    "            train_idx += 1\n",
    "\n",
    "    model.train()\n",
    "    return all_pred_boxes, all_true_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cellboxes(predictions, S=4, C=1):\n",
    "    \"\"\"\n",
    "    Converts bounding boxes output from Yolo with\n",
    "    an image split size of S into entire image ratios\n",
    "    rather than relative to cell ratios.\n",
    "    \"\"\"\n",
    "    predictions = predictions.to(\"cpu\")\n",
    "    batch_size = predictions.shape[0]\n",
    "    predictions = predictions.reshape(batch_size, S, S, C + 10)\n",
    "    bboxes1 = predictions[..., C + 1:C + 5]\n",
    "    bboxes2 = predictions[..., C + 6:C + 10]\n",
    "    \n",
    "    scores = torch.cat(\n",
    "        (predictions[..., C].unsqueeze(0), predictions[..., C + 5].unsqueeze(0)), dim=0\n",
    "    )\n",
    "    best_box = scores.argmax(0).unsqueeze(-1)\n",
    "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
    "    # This results in a tensor with shape (batch_size, 7, 7, 1) where each element represents the index of a grid cell.\n",
    "    cell_indices = torch.arange(S).repeat(batch_size, S, 1).unsqueeze(-1)\n",
    "\n",
    "    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n",
    "    # Permute because is used here to swap these indices to match the (x, y) convention used in the best_boxes tensor.\n",
    "    # [0,1,2]->[0,0,0]\n",
    "    # [0,1,2]->[1,1,1]\n",
    "    # [0,1,2]->[2,2,2]\n",
    "    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
    "    w_y = 1 / S * best_boxes[..., 2:4]\n",
    "\n",
    "    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n",
    "    predicted_class = predictions[..., :C].argmax(-1).unsqueeze(-1)\n",
    "    best_confidence = torch.max(predictions[..., C], predictions[..., C + 5]).unsqueeze(\n",
    "        -1\n",
    "    )\n",
    "    converted_preds = torch.cat(\n",
    "        (predicted_class, best_confidence, converted_bboxes), dim=-1\n",
    "    )\n",
    "    #print(f\"converted_preds: {converted_preds}\")\n",
    "\n",
    "    return converted_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cellboxes_to_boxes(out, S=4):\n",
    "    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n",
    "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
    "    all_bboxes = []\n",
    "\n",
    "    #iterate over each batch sample\n",
    "    for ex_idx in range(out.shape[0]):\n",
    "        bboxes = []\n",
    "        #iterate over each grid in the grid cell\n",
    "        for bbox_idx in range(S * S):\n",
    "            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n",
    "        all_bboxes.append(bboxes)\n",
    "    #print(f\"all_bboxes: {all_bboxes}\")\n",
    "    return all_bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">   It returns a list containing bounding boxes for each example in the batch. Each bounding box is represented as a list of values '[x, y, width, height, confidence, class_probabilities]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Loader of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ShipDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, S=4, B=2, C=1, transform=None, train=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.train = train\n",
    "\n",
    "       # Determine the directory of the images and labels\n",
    "        if self.train:\n",
    "            self.img_dir = os.path.join(self.root_dir, 'images/train')\n",
    "            self.label_dir = os.path.join(self.root_dir, 'labels/train')\n",
    "        else:\n",
    "            self.img_dir = os.path.join(self.root_dir, 'images/test')\n",
    "            self.label_dir = os.path.join(self.root_dir, 'labels/test')\n",
    "\n",
    "        self.img_ids = os.listdir(self.img_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.img_ids[index].split('.')[0]\n",
    "        boxes = []\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, img_id + '.jpg')\n",
    "        image = Image.open(img_path)\n",
    "        image = image.convert(\"L\")\n",
    "\n",
    "        # Load labels\n",
    "        label_path = os.path.join(self.label_dir, img_id + '.txt')\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                class_label, x, y, width, height = map(float, line.strip().split())\n",
    "                boxes.append([class_label, x, y, width, height])\n",
    "        \n",
    "        \n",
    "        boxes = torch.tensor(boxes)\n",
    "        #print(f\"boxes: {boxes}\")\n",
    "        if self.transform:\n",
    "            image, boxes = self.transform(image, boxes)\n",
    "\n",
    "        # Convert To Cells\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
    "        for box in boxes:\n",
    "            class_label, x, y, width, height = box.tolist()\n",
    "            class_label = int(class_label)\n",
    "\n",
    "            i, j = int(self.S * y), int(self.S * x)\n",
    "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
    "\n",
    "            width_cell, height_cell = (\n",
    "                width * self.S,\n",
    "                height * self.S,\n",
    "            )\n",
    "\n",
    "            if label_matrix[i, j, self.C] == 0:\n",
    "                label_matrix[i, j, self.C] = 1\n",
    "\n",
    "                box_coordinates = torch.tensor(\n",
    "                    [x_cell, y_cell, width_cell, height_cell]\n",
    "                )\n",
    "\n",
    "                label_matrix[i, j, self.C+1:self.C+5] = box_coordinates\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "    \n",
    "        #print(f\"label_matrix shape: {label_matrix.shape}\")\n",
    "\n",
    "        return image, label_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From original paper: \n",
    ">   YOLO predicts multiple bounding boxes per grid cell. At training time we only want one bounding box predictor to be responsible for each object. We assign one predictor to be “responsible” for predicting an object based on which prediction has the highest current IOU with the ground truth. This leads to specialization between the bounding box predictors.\n",
    "Each predictor gets better at predicting certain sizes, aspect ratios, or classes of object, improving overall recall. \n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\lambda_{\\text {coord }} \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{i j}^{\\text {obj }}\\left[\\left(x_i-\\hat{x}_i\\right)^2+\\left(y_i-\\hat{y}_i\\right)^2\\right] \\\\\n",
    "+\\lambda_{\\text {coord }} \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{i j}^{\\text {obj }}\\left[\\left(\\sqrt{w_i}-\\sqrt{\\hat{w}_i}\\right)^2+\\left(\\sqrt{h_i}-\\sqrt{\\hat{h}_i}\\right)^2\\right] \\\\\n",
    "+\\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{i j}^{\\text {obj }}\\left(C_i-\\hat{C}_i\\right)^2 \\\\\n",
    "+\\lambda_{\\text {noobj }} \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{i j}^{\\text {noobj }}\\left(C_i-\\hat{C}_i\\right)^2 \\\\\n",
    "+\\sum_{i=0}^{S^2} \\mathbb{1}_i^{\\text {obj }} \\sum_{c \\in \\text { classes }}\\left(p_i(c)-\\hat{p}_i(c)\\right)^2\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "During training we optimize the following, multi-part where $ 1_{obj}^i $ denotes if object appears in cell **i** and $1_{obj}^{ij}$ denotes that the **j**  bounding box predictor in cell i is “responsible” for that prediction.\n",
    "\n",
    "In every image many grid cells do not contain any object. This pushes the “confidence” scores of those cells towards zero, often overpowering the gradient from cells that do contain objects. This can lead to model instability, as the model may prioritize learning to predict empty cells rather than focusing on correctly detecting objects in cells containing them, causing training to diverge early on. To remedy this, we increase the loss from bounding box coordinate predictions and decrease the loss from confidence predictions for boxes that don’t contain objects. We use two parameters, $\\lambda_{coord}$ and $\\lambda_{noobj}$  to accomplish this.\n",
    "\n",
    "Note that the loss function only penalizes classification error if an object is present in that grid cell (hence the conditional class probability discussed earlier). It also only penalizes bounding box coordinate error if that predictor is “responsible” for the ground truth box (i.e. has the highest\n",
    "IOU of any predictor in that grid cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the loss for yolo (v1) model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S=4, B=2, C=1):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        \"\"\"\n",
    "        S is split size of image (in paper 7),\n",
    "        B is number of boxes (in paper 2),\n",
    "        C is number of classes (in paper 20, in dataset 3),\n",
    "        \"\"\"\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        # These are from Yolo paper, signifying how much we should\n",
    "        # pay loss for no object (noobj) and the box coordinates (coord)\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        # predictions are shaped (BATCH_SIZE, S*S(C+B*5) when inputted\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
    "\n",
    "        # Calculate IoU for the two predicted bounding boxes with target bbox\n",
    "        iou_b1 = intersection_over_union(predictions[..., self.C + 1:self.C + 5], target[..., self.C + 1:self.C + 5])\n",
    "        iou_b2 = intersection_over_union(predictions[..., self.C + 6:self.C + 10], target[..., self.C + 1:self.C + 5])\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "\n",
    "        # Take the box with highest IoU out of the two prediction\n",
    "        # Note that bestbox will be indices of 0, 1 for which bbox was best\n",
    "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
    "        exists_box = target[..., self.C].unsqueeze(3)  # in paper this is Iobj_i\n",
    "\n",
    "        # ======================== #\n",
    "        #   FOR BOX COORDINATES    #\n",
    "        # ======================== #\n",
    "\n",
    "        # Set boxes with no object in them to 0. We only take out one of the two \n",
    "        # predictions, which is the one with highest Iou calculated previously.\n",
    "        box_predictions = exists_box * (\n",
    "            (\n",
    "                bestbox * predictions[..., self.C + 6:self.C + 10]\n",
    "                + (1 - bestbox) * predictions[..., self.C + 1:self.C + 5]\n",
    "            )\n",
    "        )\n",
    "        #print(f\"box_predictions: {box_predictions.shape}\")\n",
    "        box_targets = exists_box * target[..., self.C + 1:self.C + 5]\n",
    "\n",
    "        # Take sqrt of width, height of boxes\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
    "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
    "        )\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions, end_dim=-2),\n",
    "            torch.flatten(box_targets, end_dim=-2),\n",
    "        )\n",
    "\n",
    "        # ==================== #\n",
    "        #   FOR OBJECT LOSS    #\n",
    "        # ==================== #\n",
    "\n",
    "        # pred_box is the confidence score for the bbox with highest IoU\n",
    "        pred_box = (\n",
    "            bestbox * predictions[..., self.C + 5:self.C + 6] + (1 - bestbox) * predictions[..., self.C:self.C + 1]\n",
    "        )\n",
    "\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_box * pred_box),\n",
    "            torch.flatten(exists_box * target[..., self.C:self.C + 1]),\n",
    "        )\n",
    "\n",
    "        # ======================= #\n",
    "        #   FOR NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "\n",
    "        #max_no_obj = torch.max(predictions[..., 20:21], predictions[..., 25:26])\n",
    "        #no_object_loss = self.mse(\n",
    "        #    torch.flatten((1 - exists_box) * max_no_obj, start_dim=1),\n",
    "        #    torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
    "        #)\n",
    "\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., self.C:self.C + 1], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1),\n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., self.C + 5:self.C + 6], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1)\n",
    "        )\n",
    "\n",
    "        # ================== #\n",
    "        #   FOR CLASS LOSS   #\n",
    "        # ================== #\n",
    "\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[..., :self.C], end_dim=-2,),\n",
    "            torch.flatten(exists_box * target[..., :self.C], end_dim=-2,),\n",
    "        )\n",
    "\n",
    "        loss = (\n",
    "            self.lambda_coord * box_loss  # first two rows in paper\n",
    "            + object_loss  # third row in paper\n",
    "            + self.lambda_noobj * no_object_loss  # forth row\n",
    "            + class_loss  # fifth row\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = \"cpu\"\n",
    "BATCH_SIZE = 64 # 64 in original paper but resource exhausted error otherwise.\n",
    "EPOCHS = 25\n",
    "WEIGHT_DECAY = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, optimizer, loss_fn):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    mean_loss = []\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "        mean_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_postfix(loss = loss.item())\n",
    "        \n",
    "    print(f\"Mean loss was {sum(mean_loss) / len(mean_loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, bboxes):\n",
    "        for t in self.transforms:\n",
    "            img, bboxes = t(img), bboxes\n",
    "\n",
    "        return img, bboxes\n",
    "\n",
    "\n",
    "transform = Compose([transforms.Resize((88, 88)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dir = '/home/buono/ObjDct_Repo/data/ShipDataset'\n",
    "model = TinyissimoYOLO().to(DEVICE)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.1, patience=3, mode='max', verbose=True)\n",
    "loss_fn = YoloLoss()\n",
    "\n",
    "\n",
    "train_dataset = ShipDataset(\n",
    "    root_dir=files_dir,\n",
    "    transform=transform,\n",
    "    train=True\n",
    ")\n",
    "\n",
    "test_dataset = ShipDataset(\n",
    "    root_dir=files_dir,\n",
    "    transform=transform,\n",
    "    train=False\n",
    ")\n",
    "\n",
    "# Now you can create your DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eddf53686e704364b397a123ee2058a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/497 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     pred_boxes, target_boxes \u001b[38;5;241m=\u001b[39m get_bboxes(\n\u001b[1;32m      6\u001b[0m         train_loader, model, iou_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m\n\u001b[1;32m      7\u001b[0m     )\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#print(f\"pred_boxes: {len(pred_boxes)}, target_boxes: {len(target_boxes)}\")\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# mean_avg_prec = mean_average_precision(\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#     pred_boxes, target_boxes, iou_threshold=0.5\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# scheduler.step(mean_avg_prec)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[95], line 11\u001b[0m, in \u001b[0;36mtrain_fn\u001b[0;34m(train_loader, model, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m      9\u001b[0m mean_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 11\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     14\u001b[0m loop\u001b[38;5;241m.\u001b[39mset_postfix(loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/DATA/buono/ObjDct_Repo/venv/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/DATA/buono/ObjDct_Repo/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/DATA/buono/ObjDct_Repo/venv/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    train_fn(train_loader, model, optimizer, loss_fn)\n",
    "    \n",
    "    pred_boxes, target_boxes = get_bboxes(\n",
    "        train_loader, model, iou_threshold=0.5, threshold=0.4\n",
    "    )\n",
    "    print(f\"pred_boxes: {len(pred_boxes)}, target_boxes: {len(target_boxes)}\")\n",
    "    mean_avg_prec = mean_average_precision(\n",
    "        pred_boxes, target_boxes, iou_threshold=0.5\n",
    "    )\n",
    "    print(f\"Train mAP: {mean_avg_prec}\")\n",
    "    \n",
    "    \n",
    "    # scheduler.step(mean_avg_prec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/home/buono/ObjDct_Repo/models/trained_models/mobilenetyolo.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyissimoYOLO(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LinearActivation()\n",
       "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LinearActivation()\n",
       "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LinearActivation()\n",
       "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LinearActivation()\n",
       "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (fclayers): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=3200, out_features=256, bias=True)\n",
       "    (2): LinearActivation()\n",
       "    (3): Linear(in_features=256, out_features=176, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "checkpoint = torch.load(\"/home/buono/ObjDct_Repo/models/trained_models/mobilenetyolo.pth\")\n",
    "# Load the state dictionary from the .pth file\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_boxes, target_boxes = get_bboxes(\n",
    "        test_loader, model, iou_threshold=0.5, threshold=0.4\n",
    "    )\n",
    "    mean_avg_prec = mean_average_precision(\n",
    "        pred_boxes, target_boxes, iou_threshold=0.5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15494"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6,\n",
       " 0.0,\n",
       " 1.1046698093414307,\n",
       " 0.48770204186439514,\n",
       " 0.4557408094406128,\n",
       " 0.12663282454013824,\n",
       " 0.12683242559432983]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_boxes[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_confidences = []\n",
    "for prediction in pred_boxes:\n",
    "    confidence = prediction[2]\n",
    "    pred_confidences.append(confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9533683061599731,\n",
       " 0.5343457460403442,\n",
       " 0.4687846899032593,\n",
       " 0.44922223687171936,\n",
       " 0.42876332998275757,\n",
       " 0.4073317050933838,\n",
       " 1.1046698093414307,\n",
       " 0.8002456426620483,\n",
       " 0.7142586708068848,\n",
       " 0.5417858362197876,\n",
       " 0.5179846286773682,\n",
       " 0.5070096850395203,\n",
       " 0.4856957495212555,\n",
       " 0.454276442527771,\n",
       " 0.4366269111633301,\n",
       " 0.7188742756843567,\n",
       " 0.5436949133872986,\n",
       " 123.70005798339844,\n",
       " 99.3355712890625,\n",
       " 74.20442199707031,\n",
       " 73.70069122314453,\n",
       " 71.66365051269531,\n",
       " 68.93413543701172,\n",
       " 65.1718521118164,\n",
       " 60.847782135009766,\n",
       " 40.65019226074219,\n",
       " 36.921382904052734,\n",
       " 15.005477905273438,\n",
       " 6.816977024078369,\n",
       " 3.9056859016418457,\n",
       " 3.3534226417541504,\n",
       " 0.7208556532859802,\n",
       " 5.902571678161621,\n",
       " 3.7398760318756104,\n",
       " 3.123561143875122,\n",
       " 2.817551374435425,\n",
       " 1.7728065252304077,\n",
       " 1.5312912464141846,\n",
       " 1.470460295677185,\n",
       " 1.4319130182266235,\n",
       " 1.2287240028381348,\n",
       " 0.792327880859375,\n",
       " 0.6585595011711121,\n",
       " 0.4421406388282776,\n",
       " 0.41880515217781067,\n",
       " 466.7049560546875,\n",
       " 456.5641174316406,\n",
       " 419.330810546875,\n",
       " 352.2742614746094,\n",
       " 312.2073974609375,\n",
       " 216.3512420654297,\n",
       " 130.91685485839844,\n",
       " 123.07898712158203,\n",
       " 112.45448303222656,\n",
       " 45.77471923828125,\n",
       " 24.676511764526367,\n",
       " 7.122767925262451,\n",
       " 3.2851734161376953,\n",
       " 0.6758137345314026,\n",
       " 0.8786806464195251,\n",
       " 0.5688645243644714,\n",
       " 1.0034093856811523,\n",
       " 0.49649322032928467,\n",
       " 0.7365703582763672,\n",
       " 0.5684512257575989,\n",
       " 0.494954377412796,\n",
       " 0.4823310673236847,\n",
       " 0.45549970865249634,\n",
       " 2.5536539554595947,\n",
       " 2.345243215560913,\n",
       " 1.3414735794067383,\n",
       " 1.24894118309021,\n",
       " 1.004813313484192,\n",
       " 0.9352248907089233,\n",
       " 0.9166180491447449,\n",
       " 0.7124130129814148,\n",
       " 0.6487091779708862,\n",
       " 0.6078205108642578,\n",
       " 0.41344940662384033,\n",
       " 0.442532479763031,\n",
       " 0.689447283744812,\n",
       " 0.541991651058197,\n",
       " 0.46384739875793457,\n",
       " 0.9727086424827576,\n",
       " 0.5216196179389954,\n",
       " 0.431913286447525,\n",
       " 0.6950631141662598,\n",
       " 0.808347761631012,\n",
       " 0.8451935052871704,\n",
       " 2.3998143672943115,\n",
       " 2.355410575866699,\n",
       " 1.4577053785324097,\n",
       " 1.4132344722747803,\n",
       " 1.3968783617019653,\n",
       " 0.8792493939399719,\n",
       " 0.8128565549850464,\n",
       " 0.757673978805542,\n",
       " 0.7294657230377197,\n",
       " 0.4585394561290741,\n",
       " 0.43009448051452637,\n",
       " 0.545263946056366,\n",
       " 0.4315548241138458,\n",
       " 0.8517436981201172,\n",
       " 0.5039693117141724,\n",
       " 0.46795740723609924,\n",
       " 0.4556308090686798,\n",
       " 0.4198424220085144,\n",
       " 0.40110647678375244,\n",
       " 3.508207321166992,\n",
       " 2.8064658641815186,\n",
       " 2.3516383171081543,\n",
       " 2.1804044246673584,\n",
       " 2.0409436225891113,\n",
       " 1.773095726966858,\n",
       " 1.5391660928726196,\n",
       " 1.448775053024292,\n",
       " 1.1272557973861694,\n",
       " 0.9156959056854248,\n",
       " 0.7838602662086487,\n",
       " 0.6844007968902588,\n",
       " 0.40377259254455566,\n",
       " 0.6454629898071289,\n",
       " 0.5125513076782227,\n",
       " 1.768668532371521,\n",
       " 1.189021348953247,\n",
       " 0.484889954328537,\n",
       " 0.47501546144485474,\n",
       " 0.6079742908477783,\n",
       " 0.9954467415809631,\n",
       " 0.6106500029563904,\n",
       " 0.5694805383682251,\n",
       " 0.43773579597473145,\n",
       " 0.8666926622390747,\n",
       " 0.4047253131866455,\n",
       " 0.861122727394104,\n",
       " 0.693641185760498,\n",
       " 0.6529645919799805,\n",
       " 0.5968999862670898,\n",
       " 0.49086830019950867,\n",
       " 0.4799417555332184,\n",
       " 0.8187596797943115,\n",
       " 0.6562005281448364,\n",
       " 0.5558574199676514,\n",
       " 0.4015812575817108,\n",
       " 0.5886043906211853,\n",
       " 1.3621747493743896,\n",
       " 0.6475670337677002,\n",
       " 0.49855461716651917,\n",
       " 0.4436125159263611,\n",
       " 1.8521020412445068,\n",
       " 1.618466854095459,\n",
       " 1.479062795639038,\n",
       " 1.339938998222351,\n",
       " 1.188979983329773,\n",
       " 1.0851376056671143,\n",
       " 1.0253788232803345,\n",
       " 0.5105229616165161,\n",
       " 0.48480963706970215,\n",
       " 0.44475629925727844,\n",
       " 0.40069758892059326,\n",
       " 0.7681594491004944,\n",
       " 46.99187088012695,\n",
       " 37.8245849609375,\n",
       " 29.36713409423828,\n",
       " 22.085628509521484,\n",
       " 20.317949295043945,\n",
       " 19.722410202026367,\n",
       " 19.17108154296875,\n",
       " 16.29806900024414,\n",
       " 12.988182067871094,\n",
       " 9.210257530212402,\n",
       " 8.335373878479004,\n",
       " 5.76555871963501,\n",
       " 5.494531631469727,\n",
       " 0.430330753326416,\n",
       " 3.6937191486358643,\n",
       " 3.1768603324890137,\n",
       " 1.5078099966049194,\n",
       " 1.307748794555664,\n",
       " 1.2421623468399048,\n",
       " 1.192713975906372,\n",
       " 1.1512740850448608,\n",
       " 1.0705026388168335,\n",
       " 0.8880739808082581,\n",
       " 0.8296900987625122,\n",
       " 0.7304936051368713,\n",
       " 0.5860304236412048,\n",
       " 0.9852371215820312,\n",
       " 0.40546202659606934,\n",
       " 0.43826061487197876,\n",
       " 0.426169753074646,\n",
       " 0.42182663083076477,\n",
       " 2.6905176639556885,\n",
       " 2.1195833683013916,\n",
       " 2.045743942260742,\n",
       " 1.2280256748199463,\n",
       " 1.1205651760101318,\n",
       " 1.0866748094558716,\n",
       " 1.0740762948989868,\n",
       " 0.9516739845275879,\n",
       " 0.8913255929946899,\n",
       " 0.7109353542327881,\n",
       " 0.6562190651893616,\n",
       " 0.7312083840370178,\n",
       " 1.0712101459503174,\n",
       " 0.5577670931816101,\n",
       " 0.4218979775905609,\n",
       " 0.524444043636322,\n",
       " 0.6202730536460876,\n",
       " 0.48449355363845825,\n",
       " 0.426748126745224,\n",
       " 0.4241832494735718,\n",
       " 0.9608420729637146,\n",
       " 0.7374667525291443,\n",
       " 0.6620317101478577,\n",
       " 0.498702734708786,\n",
       " 0.4889890253543854,\n",
       " 0.47774043679237366,\n",
       " 0.4498182237148285,\n",
       " 0.6796045899391174,\n",
       " 1.2988362312316895,\n",
       " 1.0158299207687378,\n",
       " 0.5198413729667664,\n",
       " 0.49137553572654724,\n",
       " 0.611855685710907,\n",
       " 0.5087739825248718,\n",
       " 0.42563843727111816,\n",
       " 0.4001619815826416,\n",
       " 0.43458956480026245,\n",
       " 0.6463645100593567,\n",
       " 0.7620207667350769,\n",
       " 0.4886636734008789,\n",
       " 0.40382254123687744,\n",
       " 0.669921338558197,\n",
       " 0.8603951334953308,\n",
       " 0.5323363542556763,\n",
       " 0.5382776260375977,\n",
       " 0.7912373542785645,\n",
       " 0.5031600594520569,\n",
       " 0.7312707901000977,\n",
       " 0.4197900593280792,\n",
       " 0.5500038862228394,\n",
       " 0.7335622310638428,\n",
       " 0.40838325023651123,\n",
       " 1.0195896625518799,\n",
       " 0.408482164144516,\n",
       " 0.4151453673839569,\n",
       " 0.48257097601890564,\n",
       " 0.8178019523620605,\n",
       " 0.6560130715370178,\n",
       " 0.5442715883255005,\n",
       " 0.8622410297393799,\n",
       " 0.503097653388977,\n",
       " 1.3006157875061035,\n",
       " 0.7272335886955261,\n",
       " 0.5722061991691589,\n",
       " 0.49284350872039795,\n",
       " 0.4641087055206299,\n",
       " 0.41305214166641235,\n",
       " 1.763047695159912,\n",
       " 1.5880450010299683,\n",
       " 1.1520220041275024,\n",
       " 1.1184102296829224,\n",
       " 1.0700912475585938,\n",
       " 0.9683824777603149,\n",
       " 0.9053921699523926,\n",
       " 0.8677552342414856,\n",
       " 0.48992493748664856,\n",
       " 0.7098028063774109,\n",
       " 0.6565393805503845,\n",
       " 0.4600575864315033,\n",
       " 10.702125549316406,\n",
       " 9.06160831451416,\n",
       " 6.546787738800049,\n",
       " 6.410429000854492,\n",
       " 5.910157680511475,\n",
       " 5.431559085845947,\n",
       " 4.107022285461426,\n",
       " 3.893126964569092,\n",
       " 2.642016887664795,\n",
       " 1.7611360549926758,\n",
       " 0.7258080244064331,\n",
       " 0.4130996763706207,\n",
       " 0.5686009526252747,\n",
       " 0.40886661410331726,\n",
       " 0.9925984144210815,\n",
       " 0.5357365608215332,\n",
       " 0.5131756067276001,\n",
       " 0.4781668782234192,\n",
       " 0.4449220299720764,\n",
       " 0.5632601976394653,\n",
       " 1.7309985160827637,\n",
       " 1.5162923336029053,\n",
       " 1.0715187788009644,\n",
       " 0.9212764501571655,\n",
       " 0.7957347631454468,\n",
       " 0.72889643907547,\n",
       " 0.7013169527053833,\n",
       " 0.6618459224700928,\n",
       " 0.6455617547035217,\n",
       " 0.5724878311157227,\n",
       " 0.42952674627304077,\n",
       " 2.023181438446045,\n",
       " 1.2466844320297241,\n",
       " 1.0851060152053833,\n",
       " 1.045764684677124,\n",
       " 0.9138998985290527,\n",
       " 0.8214704394340515,\n",
       " 0.6876846551895142,\n",
       " 0.6772664189338684,\n",
       " 0.6754876375198364,\n",
       " 0.434599369764328,\n",
       " 0.7725305557250977,\n",
       " 0.6035044193267822,\n",
       " 0.5435796976089478,\n",
       " 0.529450535774231,\n",
       " 0.42855241894721985,\n",
       " 6.111935615539551,\n",
       " 4.80021858215332,\n",
       " 4.206595420837402,\n",
       " 3.74092960357666,\n",
       " 2.6249842643737793,\n",
       " 2.5376663208007812,\n",
       " 2.46333909034729,\n",
       " 2.208843231201172,\n",
       " 1.3495659828186035,\n",
       " 186.63328552246094,\n",
       " 165.2952423095703,\n",
       " 144.681884765625,\n",
       " 129.40721130371094,\n",
       " 111.4614028930664,\n",
       " 102.46727752685547,\n",
       " 90.27996063232422,\n",
       " 68.96296691894531,\n",
       " 61.17215347290039,\n",
       " 49.02251052856445,\n",
       " 39.7244987487793,\n",
       " 4.062145233154297,\n",
       " 1.1769424676895142,\n",
       " 0.6906927824020386,\n",
       " 0.5136421918869019,\n",
       " 0.46724727749824524,\n",
       " 0.41274842619895935,\n",
       " 0.4775427281856537,\n",
       " 1.9665749073028564,\n",
       " 1.0996836423873901,\n",
       " 0.9911856055259705,\n",
       " 0.8983291983604431,\n",
       " 0.7248185276985168,\n",
       " 0.6528801321983337,\n",
       " 0.6028470396995544,\n",
       " 0.5542711615562439,\n",
       " 0.46088287234306335,\n",
       " 0.4268670380115509,\n",
       " 0.47469285130500793,\n",
       " 0.4337075650691986,\n",
       " 0.4911893308162689,\n",
       " 1.4394168853759766,\n",
       " 0.9292478561401367,\n",
       " 0.7560669779777527,\n",
       " 0.7032073736190796,\n",
       " 0.6756041049957275,\n",
       " 0.43816542625427246,\n",
       " 92.16960906982422,\n",
       " 72.69843292236328,\n",
       " 65.01561737060547,\n",
       " 52.595375061035156,\n",
       " 47.919002532958984,\n",
       " 47.85281753540039,\n",
       " 41.25617599487305,\n",
       " 39.961280822753906,\n",
       " 31.286388397216797,\n",
       " 20.075359344482422,\n",
       " 15.957039833068848,\n",
       " 8.743409156799316,\n",
       " 1.5564656257629395,\n",
       " 0.40474364161491394,\n",
       " 0.40520015358924866,\n",
       " 6.403809547424316,\n",
       " 5.411509037017822,\n",
       " 4.486789703369141,\n",
       " 3.0119636058807373,\n",
       " 2.4950029850006104,\n",
       " 2.1220006942749023,\n",
       " 1.8597880601882935,\n",
       " 1.2490030527114868,\n",
       " 1.1791808605194092,\n",
       " 0.9631624221801758,\n",
       " 0.7722654938697815,\n",
       " 0.6212083101272583,\n",
       " 0.9018541574478149,\n",
       " 0.47903895378112793,\n",
       " 0.6991027593612671,\n",
       " 0.4820713698863983,\n",
       " 0.4696506857872009,\n",
       " 0.7972196340560913,\n",
       " 0.42573198676109314,\n",
       " 0.41945508122444153,\n",
       " 0.43442392349243164,\n",
       " 0.4430796802043915,\n",
       " 1.177636742591858,\n",
       " 0.5049780607223511,\n",
       " 0.47898462414741516,\n",
       " 0.4576508402824402,\n",
       " 1.3531180620193481,\n",
       " 1.1433387994766235,\n",
       " 1.137180209159851,\n",
       " 1.1121736764907837,\n",
       " 0.8383501172065735,\n",
       " 0.8320000171661377,\n",
       " 0.8289722204208374,\n",
       " 0.7923070788383484,\n",
       " 0.7679161429405212,\n",
       " 0.6232108473777771,\n",
       " 0.6175070405006409,\n",
       " 0.6021218299865723,\n",
       " 0.5193313360214233,\n",
       " 1.66665518283844,\n",
       " 1.4733047485351562,\n",
       " 0.9152123332023621,\n",
       " 0.9107306599617004,\n",
       " 0.4304835796356201,\n",
       " 0.46275803446769714,\n",
       " 0.507326066493988,\n",
       " 0.5145955681800842,\n",
       " 0.6017423868179321,\n",
       " 0.46335235238075256,\n",
       " 0.4125189781188965,\n",
       " 0.5577580332756042,\n",
       " 0.5001718401908875,\n",
       " 0.4644196629524231,\n",
       " 0.4237755239009857,\n",
       " 0.5417364239692688,\n",
       " 0.5174599885940552,\n",
       " 0.7760226130485535,\n",
       " 0.6917787194252014,\n",
       " 0.5143103003501892,\n",
       " 0.43164291977882385,\n",
       " 30.695152282714844,\n",
       " 24.1420841217041,\n",
       " 23.094505310058594,\n",
       " 19.512300491333008,\n",
       " 16.11789894104004,\n",
       " 15.756904602050781,\n",
       " 15.287020683288574,\n",
       " 14.168804168701172,\n",
       " 9.505352020263672,\n",
       " 7.806193828582764,\n",
       " 3.576411247253418,\n",
       " 0.5730369091033936,\n",
       " 0.40168553590774536,\n",
       " 0.798483669757843,\n",
       " 0.41948962211608887,\n",
       " 0.5719354152679443,\n",
       " 3.1869285106658936,\n",
       " 1.575409173965454,\n",
       " 1.566232442855835,\n",
       " 1.407576084136963,\n",
       " 1.2622202634811401,\n",
       " 1.2430187463760376,\n",
       " 1.1528172492980957,\n",
       " 1.0040165185928345,\n",
       " 0.4503019452095032,\n",
       " 1.037243127822876,\n",
       " 0.6125386357307434,\n",
       " 0.597877025604248,\n",
       " 0.42917460203170776,\n",
       " 0.5854282975196838,\n",
       " 0.5549910664558411,\n",
       " 0.6358716487884521,\n",
       " 0.48240259289741516,\n",
       " 0.4047640264034271,\n",
       " 1.0574921369552612,\n",
       " 0.4353642463684082,\n",
       " 0.42165812849998474,\n",
       " 0.7053026556968689,\n",
       " 0.6223291754722595,\n",
       " 0.5242540836334229,\n",
       " 0.9166089296340942,\n",
       " 0.5366341471672058,\n",
       " 0.4004021883010864,\n",
       " 0.49174293875694275,\n",
       " 0.5600544214248657,\n",
       " 1.0352414846420288,\n",
       " 0.5570079684257507,\n",
       " 0.5497081279754639,\n",
       " 0.5037878751754761,\n",
       " 0.9517156481742859,\n",
       " 1.3772438764572144,\n",
       " 0.705923855304718,\n",
       " 0.5989194512367249,\n",
       " 0.45578110218048096,\n",
       " 0.4320428967475891,\n",
       " 0.6297557353973389,\n",
       " 0.5828801989555359,\n",
       " 0.6945048570632935,\n",
       " 0.6279954314231873,\n",
       " 0.5969312191009521,\n",
       " 0.5621684789657593,\n",
       " 0.46196383237838745,\n",
       " 0.4305967390537262,\n",
       " 0.4097443222999573,\n",
       " 0.4251013696193695,\n",
       " 0.41014817357063293,\n",
       " 0.7242937088012695,\n",
       " 0.515718936920166,\n",
       " 0.5795013904571533,\n",
       " 0.4002017676830292,\n",
       " 0.7132540941238403,\n",
       " 0.5037469267845154,\n",
       " 0.6692022681236267,\n",
       " 0.4259057343006134,\n",
       " 0.4123111665248871,\n",
       " 0.6592143177986145,\n",
       " 0.5344172120094299,\n",
       " 0.4360222816467285,\n",
       " 0.5211933851242065,\n",
       " 0.7137488722801208,\n",
       " 0.6882458329200745,\n",
       " 0.7759026885032654,\n",
       " 0.520756721496582,\n",
       " 0.45985424518585205,\n",
       " 0.815737783908844,\n",
       " 0.43130603432655334,\n",
       " 0.6195638179779053,\n",
       " 0.5361133217811584,\n",
       " 0.5126997828483582,\n",
       " 1.500439167022705,\n",
       " 0.5812190771102905,\n",
       " 0.4066708981990814,\n",
       " 0.8308808207511902,\n",
       " 0.8235892057418823,\n",
       " 0.6882836818695068,\n",
       " 0.5633627772331238,\n",
       " 0.5407299399375916,\n",
       " 0.4987887144088745,\n",
       " 0.4683579206466675,\n",
       " 0.4150564670562744,\n",
       " 4.085340976715088,\n",
       " 3.5975944995880127,\n",
       " 3.558607339859009,\n",
       " 2.7103383541107178,\n",
       " 2.3640260696411133,\n",
       " 2.237083911895752,\n",
       " 1.7497502565383911,\n",
       " 1.642399787902832,\n",
       " 1.2388375997543335,\n",
       " 0.7764738202095032,\n",
       " 0.56137615442276,\n",
       " 0.47090840339660645,\n",
       " 1.2364033460617065,\n",
       " 0.5769789218902588,\n",
       " 0.5994890928268433,\n",
       " 1.3621973991394043,\n",
       " 0.7392181158065796,\n",
       " 0.7095232009887695,\n",
       " 0.6492285132408142,\n",
       " 0.5875396132469177,\n",
       " 0.4963681399822235,\n",
       " 2.05240535736084,\n",
       " 1.6611981391906738,\n",
       " 0.7659448385238647,\n",
       " 0.5546885132789612,\n",
       " 0.475654661655426,\n",
       " 0.7768305540084839,\n",
       " 0.5228318572044373,\n",
       " 0.6190131306648254,\n",
       " 0.43058910965919495,\n",
       " 0.4381379187107086,\n",
       " 0.6340272426605225,\n",
       " 0.5276988744735718,\n",
       " 0.5010117292404175,\n",
       " 0.6078521013259888,\n",
       " 0.4424130618572235,\n",
       " 0.5415890216827393,\n",
       " 1.332497477531433,\n",
       " 1.083572506904602,\n",
       " 0.9768686294555664,\n",
       " 0.8189550042152405,\n",
       " 0.7537452578544617,\n",
       " 0.7366287708282471,\n",
       " 0.7098601460456848,\n",
       " 0.6760368943214417,\n",
       " 0.5926384925842285,\n",
       " 0.4914235770702362,\n",
       " 14.719500541687012,\n",
       " 12.468320846557617,\n",
       " 11.421738624572754,\n",
       " 8.185789108276367,\n",
       " 6.660318374633789,\n",
       " 6.082521915435791,\n",
       " 4.553295135498047,\n",
       " 3.323531150817871,\n",
       " 2.906569480895996,\n",
       " 2.087773561477661,\n",
       " 1.4612890481948853,\n",
       " 1.3311870098114014,\n",
       " 0.685806155204773,\n",
       " 0.4684099555015564,\n",
       " 0.5544478297233582,\n",
       " 0.534550130367279,\n",
       " 0.4903047978878021,\n",
       " 0.4199281632900238,\n",
       " 19.600080490112305,\n",
       " 19.220874786376953,\n",
       " 15.561607360839844,\n",
       " 12.752365112304688,\n",
       " 12.395547866821289,\n",
       " 10.404629707336426,\n",
       " 3.7664361000061035,\n",
       " 3.4010303020477295,\n",
       " 3.132795572280884,\n",
       " 3.043776512145996,\n",
       " 2.8535654544830322,\n",
       " 2.2005860805511475,\n",
       " 0.7526399493217468,\n",
       " 0.4890930950641632,\n",
       " 0.6674232482910156,\n",
       " 0.41848403215408325,\n",
       " 0.5532020330429077,\n",
       " 0.638140082359314,\n",
       " 0.41371962428092957,\n",
       " 0.45602717995643616,\n",
       " 0.7830896377563477,\n",
       " 0.5197914242744446,\n",
       " 0.41573983430862427,\n",
       " 0.91749107837677,\n",
       " 0.4434601962566376,\n",
       " 0.9070823192596436,\n",
       " 0.4325442910194397,\n",
       " 1.124607801437378,\n",
       " 0.4153735041618347,\n",
       " 0.8142395615577698,\n",
       " 0.5391782522201538,\n",
       " 0.4212453365325928,\n",
       " 0.4657849967479706,\n",
       " 0.4733903110027313,\n",
       " 3.1698837280273438,\n",
       " 2.4069955348968506,\n",
       " 2.031618356704712,\n",
       " 1.802346110343933,\n",
       " 1.7186193466186523,\n",
       " 1.0873931646347046,\n",
       " 1.0437167882919312,\n",
       " 0.8615553975105286,\n",
       " 0.7885323762893677,\n",
       " 0.43002912402153015,\n",
       " 0.5673897862434387,\n",
       " 0.6688510179519653,\n",
       " 1.126695990562439,\n",
       " 0.46447744965553284,\n",
       " 2.4481778144836426,\n",
       " 1.465611457824707,\n",
       " 1.1701961755752563,\n",
       " 1.0518916845321655,\n",
       " 0.8124610185623169,\n",
       " 0.645720362663269,\n",
       " 0.6199286580085754,\n",
       " 0.5629644989967346,\n",
       " 0.47113242745399475,\n",
       " 0.4578973352909088,\n",
       " 0.40014180541038513,\n",
       " 1.0159833431243896,\n",
       " 0.4940022826194763,\n",
       " 0.4038580656051636,\n",
       " 0.6528284549713135,\n",
       " 2.879657745361328,\n",
       " 1.8898826837539673,\n",
       " 1.0559332370758057,\n",
       " 0.7609107494354248,\n",
       " 0.7183635830879211,\n",
       " 0.44442620873451233,\n",
       " 0.4133785665035248,\n",
       " 0.4061782956123352,\n",
       " 0.6834525465965271,\n",
       " 0.46385958790779114,\n",
       " 0.41616958379745483,\n",
       " 1.0431129932403564,\n",
       " 0.9898460507392883,\n",
       " 0.8395439386367798,\n",
       " 0.787875771522522,\n",
       " 0.628652811050415,\n",
       " 0.5799533128738403,\n",
       " 0.4451104700565338,\n",
       " 0.5351908206939697,\n",
       " 0.5017977356910706,\n",
       " 0.7921324968338013,\n",
       " 14.40679931640625,\n",
       " 12.285439491271973,\n",
       " 12.13752555847168,\n",
       " 11.57776165008545,\n",
       " 11.37793254852295,\n",
       " 9.454292297363281,\n",
       " 8.143009185791016,\n",
       " 7.06961727142334,\n",
       " 4.678893566131592,\n",
       " 3.4676616191864014,\n",
       " 3.2316699028015137,\n",
       " 2.3851184844970703,\n",
       " 1.0861866474151611,\n",
       " 0.6737119555473328,\n",
       " 0.49571117758750916,\n",
       " 0.4751667082309723,\n",
       " 0.4015887677669525,\n",
       " 0.4085056483745575,\n",
       " 0.5831642150878906,\n",
       " 0.47098055481910706,\n",
       " 0.406425803899765,\n",
       " 0.7169398069381714,\n",
       " 0.44241392612457275,\n",
       " 1.5638914108276367,\n",
       " 0.8649590015411377,\n",
       " 0.5068384408950806,\n",
       " 0.687609076499939,\n",
       " 0.4727425277233124,\n",
       " 0.4359990656375885,\n",
       " 1.0655553340911865,\n",
       " 0.6665464043617249,\n",
       " 0.5434188842773438,\n",
       " 0.5106361508369446,\n",
       " 0.41085273027420044,\n",
       " 0.5208210945129395,\n",
       " 0.4168732464313507,\n",
       " 0.837113618850708,\n",
       " 0.4775374233722687,\n",
       " 0.4573390483856201,\n",
       " 0.4233495593070984,\n",
       " 1.625989556312561,\n",
       " 1.0056992769241333,\n",
       " 0.8860170245170593,\n",
       " 0.6232160329818726,\n",
       " 0.5851500630378723,\n",
       " 0.5701605081558228,\n",
       " 0.46482014656066895,\n",
       " 0.4335035979747772,\n",
       " 0.5019893646240234,\n",
       " 1.9194142818450928,\n",
       " 1.910955548286438,\n",
       " 1.750888466835022,\n",
       " 1.1259269714355469,\n",
       " 0.9919528961181641,\n",
       " 0.9884244799613953,\n",
       " 0.9881742596626282,\n",
       " 0.9510153532028198,\n",
       " 0.7524251341819763,\n",
       " 0.4848465323448181,\n",
       " 0.4516439735889435,\n",
       " 0.6204356551170349,\n",
       " 0.48110339045524597,\n",
       " 0.5909520387649536,\n",
       " 0.42948055267333984,\n",
       " 0.40059375762939453,\n",
       " 0.4339161813259125,\n",
       " 0.478120893239975,\n",
       " 0.5105560421943665,\n",
       " 0.4802687466144562,\n",
       " 0.44463911652565,\n",
       " 1.0961843729019165,\n",
       " 0.8603998422622681,\n",
       " 0.5193161964416504,\n",
       " 0.4988854229450226,\n",
       " 0.4972350001335144,\n",
       " 0.4527941644191742,\n",
       " 0.41403067111968994,\n",
       " 0.716127872467041,\n",
       " 0.4590838849544525,\n",
       " 0.615321934223175,\n",
       " 0.5447853207588196,\n",
       " 0.47608980536460876,\n",
       " 0.43208640813827515,\n",
       " 1.1265605688095093,\n",
       " 0.9277845025062561,\n",
       " 0.9075667262077332,\n",
       " 0.7225952744483948,\n",
       " 1.2691596746444702,\n",
       " 0.9646174907684326,\n",
       " 0.960760772228241,\n",
       " 0.7965258955955505,\n",
       " 0.6283165216445923,\n",
       " 0.5758568048477173,\n",
       " 0.5627204179763794,\n",
       " 0.5265393257141113,\n",
       " 0.5176407098770142,\n",
       " 0.6167415380477905,\n",
       " 0.4669839143753052,\n",
       " 0.40152299404144287,\n",
       " 8.928468704223633,\n",
       " 8.695450782775879,\n",
       " 6.78041934967041,\n",
       " 6.152720928192139,\n",
       " 5.576058864593506,\n",
       " 4.356748104095459,\n",
       " 4.2513813972473145,\n",
       " 3.6236538887023926,\n",
       " 3.240421772003174,\n",
       " 3.228825807571411,\n",
       " 2.031921863555908,\n",
       " 1.8203381299972534,\n",
       " 0.7911486625671387,\n",
       " 1.0820962190628052,\n",
       " 0.7428250312805176,\n",
       " 0.409854531288147,\n",
       " 0.5137361288070679,\n",
       " 1.2324104309082031,\n",
       " 0.6435734033584595,\n",
       " 0.5275923013687134,\n",
       " 0.47166749835014343,\n",
       " 0.4030764102935791,\n",
       " 0.6800411343574524,\n",
       " 0.5973677039146423,\n",
       " 0.5781189203262329,\n",
       " 0.5255824327468872,\n",
       " 0.4443081319332123,\n",
       " 0.6789965033531189,\n",
       " 0.5265418887138367,\n",
       " 1.005157709121704,\n",
       " 0.9047350287437439,\n",
       " 0.655760645866394,\n",
       " 0.42127516865730286,\n",
       " 0.8536524176597595,\n",
       " 0.5910513401031494,\n",
       " 0.45070597529411316,\n",
       " 0.4342445135116577,\n",
       " 0.7669695615768433,\n",
       " 0.7408140897750854,\n",
       " 24.72286033630371,\n",
       " 22.784854888916016,\n",
       " 18.015304565429688,\n",
       " 15.344497680664062,\n",
       " 14.71821403503418,\n",
       " 8.981982231140137,\n",
       " 8.804763793945312,\n",
       " 7.689050674438477,\n",
       " 7.112233638763428,\n",
       " 5.0672125816345215,\n",
       " 2.3639347553253174,\n",
       " 0.5675849318504333,\n",
       " 0.40946003794670105,\n",
       " 0.454776793718338,\n",
       " 0.5016569495201111,\n",
       " 0.4645589590072632,\n",
       " 1.0023343563079834,\n",
       " 0.9744186997413635,\n",
       " 0.8640269041061401,\n",
       " 0.8394961357116699,\n",
       " 0.759902834892273,\n",
       " 0.5732100009918213,\n",
       " 0.5613266229629517,\n",
       " 0.5494750738143921,\n",
       " 0.467640221118927,\n",
       " 0.41631048917770386,\n",
       " 0.8627508282661438,\n",
       " 0.5065235495567322,\n",
       " 0.49398717284202576,\n",
       " 0.4049861431121826,\n",
       " 0.40465614199638367,\n",
       " 0.6436208486557007,\n",
       " 0.4802243113517761,\n",
       " 0.44801804423332214,\n",
       " 0.400509774684906,\n",
       " 0.85652095079422,\n",
       " 0.6095807552337646,\n",
       " 0.45668214559555054,\n",
       " 0.432656466960907,\n",
       " 0.5013806223869324,\n",
       " 0.4187026619911194,\n",
       " 0.5330904722213745,\n",
       " 1.4371834993362427,\n",
       " 1.1614253520965576,\n",
       " 0.981473982334137,\n",
       " 0.9804753661155701,\n",
       " 0.8188735246658325,\n",
       " 0.6953209638595581,\n",
       " 0.4283520579338074,\n",
       " 0.4060775935649872,\n",
       " 3.0929605960845947,\n",
       " 2.5656745433807373,\n",
       " 2.353501081466675,\n",
       " 1.9965455532073975,\n",
       " 1.7773360013961792,\n",
       " 1.5095553398132324,\n",
       " 1.3983912467956543,\n",
       " 1.164923071861267,\n",
       " 0.9417932629585266,\n",
       " 0.886673092842102,\n",
       " 0.7438997030258179,\n",
       " 0.6319046020507812,\n",
       " 0.455753356218338,\n",
       " 0.5577958226203918,\n",
       " 0.750400185585022,\n",
       " 0.7036345601081848,\n",
       " 0.5268191695213318,\n",
       " 0.5243211984634399,\n",
       " 0.8717018365859985,\n",
       " 0.4185270667076111,\n",
       " 0.5877800583839417,\n",
       " 0.5152750015258789,\n",
       " 0.49760833382606506,\n",
       " 0.44972753524780273,\n",
       " 0.6244880557060242,\n",
       " 0.5493715405464172,\n",
       " 0.5452942848205566,\n",
       " 0.5383198857307434,\n",
       " 0.46563583612442017,\n",
       " 0.47064951062202454,\n",
       " 0.45765215158462524,\n",
       " 0.919143795967102,\n",
       " 0.40864384174346924,\n",
       " 1.035662293434143,\n",
       " 0.5066092014312744,\n",
       " 14.882024765014648,\n",
       " 14.517657279968262,\n",
       " 11.222811698913574,\n",
       " 9.086857795715332,\n",
       " 8.382857322692871,\n",
       " 7.313555717468262,\n",
       " 3.882199764251709,\n",
       " 3.4839227199554443,\n",
       " 2.0570573806762695,\n",
       " 2.055947780609131,\n",
       " 0.4220736026763916,\n",
       " 0.5968085527420044,\n",
       " 0.5593403577804565,\n",
       " 0.44699063897132874,\n",
       " 0.4016183018684387,\n",
       " 0.565960705280304,\n",
       " 2.2614355087280273,\n",
       " 1.9057883024215698,\n",
       " 1.0059057474136353,\n",
       " 0.8479957580566406,\n",
       " 0.7082255482673645,\n",
       " 0.6983242630958557,\n",
       " 0.6798326969146729,\n",
       " 0.6122472882270813,\n",
       " 0.46535003185272217,\n",
       " 0.45754021406173706,\n",
       " 0.5808274149894714,\n",
       " 0.9864116311073303,\n",
       " 0.4806877076625824,\n",
       " 0.4123147130012512,\n",
       " 0.4419768154621124,\n",
       " 34.397464752197266,\n",
       " 24.849435806274414,\n",
       " 24.169214248657227,\n",
       " 20.240840911865234,\n",
       " 18.892717361450195,\n",
       " 11.504758834838867,\n",
       " 8.391280174255371,\n",
       " 6.3308892250061035,\n",
       " 3.703901767730713,\n",
       " 3.5505850315093994,\n",
       " 0.819912314414978,\n",
       " 0.8809484839439392,\n",
       " 0.5528581738471985,\n",
       " 1.3359423875808716,\n",
       " 1.0618633031845093,\n",
       " 0.9278590679168701,\n",
       " 0.8955134749412537,\n",
       " 0.5309244394302368,\n",
       " 0.4444156587123871,\n",
       " 0.4040873646736145,\n",
       " 0.5074882507324219,\n",
       " 0.5114549398422241,\n",
       " 0.46426767110824585,\n",
       " 0.42573651671409607,\n",
       " 0.9110795259475708,\n",
       " 0.5735399723052979,\n",
       " 0.4637080430984497,\n",
       " 1.0320014953613281,\n",
       " 0.4747444987297058,\n",
       " 32.944175720214844,\n",
       " 22.169002532958984,\n",
       " 21.509004592895508,\n",
       " 18.53713035583496,\n",
       " 16.88431167602539,\n",
       " 14.936444282531738,\n",
       " 11.726417541503906,\n",
       " 11.208215713500977,\n",
       " 8.029500961303711,\n",
       " 5.142421722412109,\n",
       " 5.132308006286621,\n",
       " 4.944214344024658,\n",
       " 1.04659104347229,\n",
       " 0.5117133855819702,\n",
       " 0.42840251326560974,\n",
       " 0.42060014605522156,\n",
       " 0.4910491704940796,\n",
       " 0.7915886640548706,\n",
       " 131.65293884277344,\n",
       " 89.20491790771484,\n",
       " 65.11664581298828,\n",
       " 62.7199592590332,\n",
       " 55.04515075683594,\n",
       " 44.35356140136719,\n",
       " 36.80440902709961,\n",
       " 29.79815673828125,\n",
       " 29.579469680786133,\n",
       " 22.959558486938477,\n",
       " 3.381756544113159,\n",
       " 0.7776911854743958,\n",
       " ...]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95336831, 0.53434575, 0.46878469, ..., 0.80921763, 0.63981688,\n",
       "       0.41891909])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_confidences = np.array(pred_confidences)\n",
    "pred_confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming pred_confidence is a numpy array\n",
    "plt.hist(pred_confidences, bins=50)\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of pred_confidence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of pred_confidences:\n",
      "Min: 0.4000180661678314\n",
      "Max: 955.9779052734375\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "min_value = np.min(pred_confidences)\n",
    "max_value = np.max(pred_confidences)\n",
    "\n",
    "print(\"Range of pred_confidences:\")\n",
    "print(\"Min:\", min_value)\n",
    "print(\"Max:\", max_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
