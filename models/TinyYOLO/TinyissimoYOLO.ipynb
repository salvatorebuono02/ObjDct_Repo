{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a tiny version of YOLO with DIOR dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arch/ObjDct_Repo/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement YOLO architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, t):\n",
    "        return torch.pow(t, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyissimoYOLO(nn.Module):\n",
    "    def __init__(self, B=2, num_classes=1, S=4):\n",
    "        super(TinyissimoYOLO, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            LinearActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            LinearActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            LinearActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2,stride = 2)\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            LinearActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2,stride = 2)\n",
    "        )\n",
    "\n",
    "        self.fclayers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*5*5, 256),\n",
    "            LinearActivation(),\n",
    "            nn.Linear(256, S*S*(num_classes + 5*B)),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.fclayers(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "model = TinyissimoYOLO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TinyissimoYOLO                           [1, 176]                  --\n",
       "├─Sequential: 1-1                        [1, 16, 44, 44]           --\n",
       "│    └─Conv2d: 2-1                       [1, 16, 88, 88]           448\n",
       "│    └─LinearActivation: 2-2             [1, 16, 88, 88]           --\n",
       "│    └─AvgPool2d: 2-3                    [1, 16, 44, 44]           --\n",
       "├─Sequential: 1-2                        [1, 32, 22, 22]           --\n",
       "│    └─Conv2d: 2-4                       [1, 32, 44, 44]           4,640\n",
       "│    └─LinearActivation: 2-5             [1, 32, 44, 44]           --\n",
       "│    └─AvgPool2d: 2-6                    [1, 32, 22, 22]           --\n",
       "├─Sequential: 1-3                        [1, 64, 11, 11]           --\n",
       "│    └─Conv2d: 2-7                       [1, 64, 22, 22]           18,496\n",
       "│    └─LinearActivation: 2-8             [1, 64, 22, 22]           --\n",
       "│    └─AvgPool2d: 2-9                    [1, 64, 11, 11]           --\n",
       "├─Sequential: 1-4                        [1, 128, 5, 5]            --\n",
       "│    └─Conv2d: 2-10                      [1, 128, 11, 11]          73,856\n",
       "│    └─LinearActivation: 2-11            [1, 128, 11, 11]          --\n",
       "│    └─AvgPool2d: 2-12                   [1, 128, 5, 5]            --\n",
       "├─Sequential: 1-5                        [1, 176]                  --\n",
       "│    └─Flatten: 2-13                     [1, 3200]                 --\n",
       "│    └─Linear: 2-14                      [1, 256]                  819,456\n",
       "│    └─LinearActivation: 2-15            [1, 256]                  --\n",
       "│    └─Linear: 2-16                      [1, 176]                  45,232\n",
       "==========================================================================================\n",
       "Total params: 962,128\n",
       "Trainable params: 962,128\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 31.21\n",
       "==========================================================================================\n",
       "Input size (MB): 0.09\n",
       "Forward/backward pass size (MB): 1.86\n",
       "Params size (MB): 3.85\n",
       "Estimated Total Size (MB): 5.80\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(1, 3, 88, 88))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection over Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(boxes_preds, boxes_labels):\n",
    "    \"\"\"\n",
    "    Calculates intersection over union\n",
    "    \n",
    "    Parameters:\n",
    "        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)    \n",
    "    Returns:\n",
    "        tensor: Intersection over union for all examples\n",
    "    \"\"\"\n",
    "    # boxes_preds shape is (N, 4) where N is the number of bboxes\n",
    "    #boxes_labels shape is (n, 4)\n",
    "    \n",
    "    box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "    box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "    box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "    box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "    box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "    box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "    box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "    box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "    \n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "    #print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
    "    \n",
    "    #.clamp(0) is for the case when they don't intersect. Since when they don't intersect, one of these will be negative so that should become 0\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "    #print(f\"intersection: {intersection}\")\n",
    "\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "    #print(f\"box1_area: {box1_area}, box2_area: {box2_area}\")\n",
    "    \n",
    "    return intersection / (box1_area + box2_area - intersection + 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non Max Suppression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input**: A list of Proposal boxes B, corresponding confidence scores S and overlap threshold N.\n",
    "\n",
    "**Output**: A list of filtered proposals D.\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "1.  Select the proposal with highest confidence score, remove it from B and add it to the final proposal list D. (Initially D is empty).\n",
    "2.  Now compare this proposal with all the proposals — calculate the IOU (Intersection over Union) of this proposal with every other proposal. If the IOU is greater than the threshold N, remove that proposal from B.\n",
    "3.  Again take the proposal with the highest confidence from the remaining proposals in B and remove it from B and add it to D.\n",
    "4.  Once again calculate the IOU of this proposal with all the proposals in B and eliminate the boxes which have high IOU than threshold.\n",
    "5.  This process is repeated until there are no more proposals left in B.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(bboxes, iou_threshold, threshold):\n",
    "    \"\"\"\n",
    "    Does Non Max Suppression given bboxes\n",
    "    Parameters:\n",
    "        bboxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [class_pred, prob_score, x_center, y_center, width, height]\n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        threshold (float): threshold to remove predicted bboxes (independent of IoU) \n",
    "    Returns:\n",
    "        list: bboxes after performing NMS given a specific IoU threshold\n",
    "    \"\"\"\n",
    "\n",
    "    assert type(bboxes) == list\n",
    "\n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        chosen_box = bboxes.pop(0)\n",
    "\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            if box[0] != chosen_box[0]\n",
    "            or intersection_over_union(\n",
    "                torch.tensor(chosen_box[2:]),\n",
    "                torch.tensor(box[2:]),\n",
    "            )\n",
    "            < iou_threshold\n",
    "        ]\n",
    "\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "    #print(f\"bboxes_after_nms: {bboxes_after_nms}\")\n",
    "\n",
    "    return bboxes_after_nms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It describes a trade-off between precision and recall.\n",
    "\n",
    "**Precision**, also referred to as the positive predictive value, describes how well a model predicts the positive class. \n",
    "$$Precision=\\frac{TP}{TP+FP}$$\n",
    ">   Of all bounding box **predictions**, what fraction was actually correct?\n",
    "\n",
    "**Recall**, also called sensitivity tells you if your model made the right predictions when it should have. \n",
    "$$Recall=\\frac{TP}{TP+FN}$$\n",
    ">   Of all **target** bounding boxes, what fraction did we correctly detect?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(\n",
    "    pred_boxes, true_boxes, iou_threshold=0.5, num_classes=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates mean average precision \n",
    "    Parameters:\n",
    "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [train_idx, class_prediction, prob_score, x_center, y_center, width, height]\n",
    "        true_boxes (list): Similar as pred_boxes except all the correct ones \n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        num_classes (int): number of classes\n",
    "    Returns:\n",
    "        float: mAP value across all classes given a specific IoU threshold \n",
    "    \"\"\"\n",
    "\n",
    "    # list storing all AP for respective classes\n",
    "    average_precisions = []\n",
    "\n",
    "    # used for numerical stability later on\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # Go through all predictions and targets,\n",
    "        # and only add the ones that belong to the\n",
    "        # current class c\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "        #print(f\"{c} class has {len(detections)} detections\")\n",
    "\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "        #print(f\"{c} class has {len(ground_truths)} ground truths\")\n",
    "\n",
    "        # find the amount of bboxes for each training example\n",
    "        # Counter here finds how many ground truth bboxes we get\n",
    "        # for each training example, so let's say img 0 has 3,\n",
    "        # img 1 has 5 then we will obtain a dictionary with:\n",
    "        # amount_bboxes = {0:3, 1:5}\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "        #print(f\"{c} class has {len(amount_bboxes)} amount bboxes\")\n",
    "\n",
    "        # We then go through each key, val in this dictionary\n",
    "        # and convert to the following (w.r.t same example):\n",
    "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # sort by box probabilities which is index 2\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "        #print(f\"{c} class has {total_true_bboxes} total true bboxes\")\n",
    "        # If none exists for this class then we can safely skip\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # Only take out the ground_truths that have the same\n",
    "            # training idx as detection\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            #print(f\"{c} class has {num_gts} ground truths for detection {detection_idx}\")\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:])\n",
    "                )\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    # true positive and add this bounding box to seen\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    #These additional detections are considered false positives because they do not correspond to a new, unique object\n",
    "                    #they're essentially \"over-detecting\" an object that has already been correctly identified.\n",
    "                    FP[detection_idx] = 1\n",
    "\n",
    "            # if IOU is lower then the detection is a false positive\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        #[1, 1, 0, 1, 0] -> [1, 2, 2, 3, 3]\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        # torch.trapz for numerical integration\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image, boxes):\n",
    "    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n",
    "    im = np.array(image)\n",
    "    height, width, _ = im.shape\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(1)\n",
    "    # Display the image\n",
    "    ax.imshow(im)\n",
    "\n",
    "    # box[0] is x midpoint, box[2] is width\n",
    "    # box[1] is y midpoint, box[3] is height\n",
    "\n",
    "    # Create a Rectangle patch\n",
    "    for box in boxes:\n",
    "        class_label = int(box[0])\n",
    "        box = box[2:]\n",
    "        assert len(box) == 4, \"Got more values than in x, y, w, h, in a box!\"\n",
    "        upper_left_x = box[0] - box[2] / 2\n",
    "        upper_left_y = box[1] - box[3] / 2\n",
    "        rect = patches.Rectangle(\n",
    "            (upper_left_x * width, upper_left_y * height),\n",
    "            box[2] * width,\n",
    "            box[3] * height,\n",
    "            linewidth=1,\n",
    "            edgecolor=\"r\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add class label text\n",
    "        ax.text(upper_left_x * width, upper_left_y * height, str(class_label), color='r', fontsize=10, verticalalignment='bottom')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and convert boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes(\n",
    "    loader,\n",
    "    model,\n",
    "    iou_threshold,\n",
    "    threshold,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    # make sure model is in eval before get bboxes\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "\n",
    "    for batch_idx, (x, labels) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        true_bboxes = cellboxes_to_boxes(labels)\n",
    "        bboxes = cellboxes_to_boxes(predictions)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            nms_boxes = non_max_suppression(\n",
    "                bboxes[idx],\n",
    "                iou_threshold=iou_threshold,\n",
    "                threshold=threshold,\n",
    "            )\n",
    "\n",
    "            # # Activate only for test\n",
    "            # if batch_idx == 0 and idx == 0:\n",
    "            #    plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n",
    "\n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx] + nms_box)\n",
    "\n",
    "            for box in true_bboxes[idx]:\n",
    "                # many will get converted to 0 pred\n",
    "                if box[1] > threshold:\n",
    "                    all_true_boxes.append([train_idx] + box)\n",
    "\n",
    "            train_idx += 1\n",
    "\n",
    "    model.train()\n",
    "    return all_pred_boxes, all_true_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cellboxes(predictions, S=4, C=1):\n",
    "    \"\"\"\n",
    "    Converts bounding boxes output from Yolo with\n",
    "    an image split size of S into entire image ratios\n",
    "    rather than relative to cell ratios.\n",
    "    \"\"\"\n",
    "    predictions = predictions.to(\"cpu\")\n",
    "    batch_size = predictions.shape[0]\n",
    "    predictions = predictions.reshape(batch_size, S, S, C + 10)\n",
    "    bboxes1 = predictions[..., C + 1:C + 5]\n",
    "    bboxes2 = predictions[..., C + 6:C + 10]\n",
    "    \n",
    "    scores = torch.cat(\n",
    "        (predictions[..., C].unsqueeze(0), predictions[..., C + 5].unsqueeze(0)), dim=0\n",
    "    )\n",
    "    best_box = scores.argmax(0).unsqueeze(-1)\n",
    "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
    "    # This results in a tensor with shape (batch_size, 7, 7, 1) where each element represents the index of a grid cell.\n",
    "    cell_indices = torch.arange(S).repeat(batch_size, S, 1).unsqueeze(-1)\n",
    "\n",
    "    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n",
    "    # Permute because is used here to swap these indices to match the (x, y) convention used in the best_boxes tensor.\n",
    "    # [0,1,2]->[0,0,0]\n",
    "    # [0,1,2]->[1,1,1]\n",
    "    # [0,1,2]->[2,2,2]\n",
    "    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
    "    w_y = 1 / S * best_boxes[..., 2:4]\n",
    "\n",
    "    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n",
    "    predicted_class = predictions[..., :C].argmax(-1).unsqueeze(-1)\n",
    "    best_confidence = torch.max(predictions[..., C], predictions[..., C + 5]).unsqueeze(\n",
    "        -1\n",
    "    )\n",
    "    converted_preds = torch.cat(\n",
    "        (predicted_class, best_confidence, converted_bboxes), dim=-1\n",
    "    )\n",
    "    #print(f\"converted_preds: {converted_preds}\")\n",
    "\n",
    "    return converted_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cellboxes_to_boxes(out, S=4):\n",
    "    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n",
    "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
    "    all_bboxes = []\n",
    "\n",
    "    #iterate over each batch sample\n",
    "    for ex_idx in range(out.shape[0]):\n",
    "        bboxes = []\n",
    "        #iterate over each grid in the grid cell\n",
    "        for bbox_idx in range(S * S):\n",
    "            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n",
    "        all_bboxes.append(bboxes)\n",
    "    #print(f\"all_bboxes: {all_bboxes}\")\n",
    "    return all_bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">   It returns a list containing bounding boxes for each example in the batch. Each bounding box is represented as a list of values '[x, y, width, height, confidence, class_probabilities]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Loader of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DiorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, S=4, B=2, C=1, transform=None, train=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.train = train\n",
    "\n",
    "        # Determine the directory of the images and labels\n",
    "        self.img_dir = os.path.join(self.root_dir, 'images')\n",
    "        self.label_dir = os.path.join(self.root_dir, 'label')\n",
    "        \n",
    "        # if self.train:\n",
    "        #     self.img_dir = os.path.join(self.img_dir, 'train')\n",
    "        #     self.label_dir = os.path.join(self.label_dir, 'train')\n",
    "        # else:\n",
    "        #     self.img_dir = os.path.join(self.img_dir, 'test')\n",
    "        #     self.label_dir = os.path.join(self.label_dir, 'test')\n",
    "        \n",
    "        \n",
    "\n",
    "        self.img_ids = os.listdir(self.img_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.img_ids[index].split('.')[0]\n",
    "        boxes = []\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, img_id + '.jpg')\n",
    "        image = Image.open(img_path)\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "        # Load labels\n",
    "        label_path = os.path.join(self.label_dir, img_id + '.txt')\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                class_label, x, y, width, height = map(float, line.strip().split())\n",
    "                boxes.append([class_label, x, y, width, height])\n",
    "        \n",
    "        if len(boxes) > 3:\n",
    "            boxes = boxes[:3]\n",
    "        boxes = torch.tensor(boxes)\n",
    "        #print(f\"boxes: {boxes}\")\n",
    "        if self.transform:\n",
    "            image, boxes = self.transform(image, boxes)\n",
    "\n",
    "        # Convert To Cells\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
    "        for box in boxes:\n",
    "            class_label, x, y, width, height = box.tolist()\n",
    "            class_label = int(class_label)\n",
    "\n",
    "            i, j = int(self.S * y), int(self.S * x)\n",
    "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
    "\n",
    "            width_cell, height_cell = (\n",
    "                width * self.S,\n",
    "                height * self.S,\n",
    "            )\n",
    "\n",
    "            if label_matrix[i, j, self.C] == 0:\n",
    "                label_matrix[i, j, self.C] = 1\n",
    "\n",
    "                box_coordinates = torch.tensor(\n",
    "                    [x_cell, y_cell, width_cell, height_cell]\n",
    "                )\n",
    "\n",
    "                label_matrix[i, j, self.C+1:self.C+5] = box_coordinates\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "    \n",
    "        #print(f\"label_matrix shape: {label_matrix.shape}\")\n",
    "\n",
    "        return image, label_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From original paper: \n",
    ">   YOLO predicts multiple bounding boxes per grid cell. At training time we only want one bounding box predictor to be responsible for each object. We assign one predictor to be “responsible” for predicting an object based on which prediction has the highest current IOU with the ground truth. This leads to specialization between the bounding box predictors.\n",
    "Each predictor gets better at predicting certain sizes, aspect ratios, or classes of object, improving overall recall. \n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\lambda_{\\text {coord }} \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{i j}^{\\text {obj }}\\left[\\left(x_i-\\hat{x}_i\\right)^2+\\left(y_i-\\hat{y}_i\\right)^2\\right] \\\\\n",
    "+\\lambda_{\\text {coord }} \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{i j}^{\\text {obj }}\\left[\\left(\\sqrt{w_i}-\\sqrt{\\hat{w}_i}\\right)^2+\\left(\\sqrt{h_i}-\\sqrt{\\hat{h}_i}\\right)^2\\right] \\\\\n",
    "+\\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{i j}^{\\text {obj }}\\left(C_i-\\hat{C}_i\\right)^2 \\\\\n",
    "+\\lambda_{\\text {noobj }} \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{i j}^{\\text {noobj }}\\left(C_i-\\hat{C}_i\\right)^2 \\\\\n",
    "+\\sum_{i=0}^{S^2} \\mathbb{1}_i^{\\text {obj }} \\sum_{c \\in \\text { classes }}\\left(p_i(c)-\\hat{p}_i(c)\\right)^2\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "During training we optimize the following, multi-part where $ 1_{obj}^i $ denotes if object appears in cell **i** and $1_{obj}^{ij}$ denotes that the **j**  bounding box predictor in cell i is “responsible” for that prediction.\n",
    "\n",
    "In every image many grid cells do not contain any object. This pushes the “confidence” scores of those cells towards zero, often overpowering the gradient from cells that do contain objects. This can lead to model instability, as the model may prioritize learning to predict empty cells rather than focusing on correctly detecting objects in cells containing them, causing training to diverge early on. To remedy this, we increase the loss from bounding box coordinate predictions and decrease the loss from confidence predictions for boxes that don’t contain objects. We use two parameters, $\\lambda_{coord}$ and $\\lambda_{noobj}$  to accomplish this.\n",
    "\n",
    "Note that the loss function only penalizes classification error if an object is present in that grid cell (hence the conditional class probability discussed earlier). It also only penalizes bounding box coordinate error if that predictor is “responsible” for the ground truth box (i.e. has the highest\n",
    "IOU of any predictor in that grid cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the loss for yolo (v1) model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S=4, B=2, C=1):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        \"\"\"\n",
    "        S is split size of image (in paper 7),\n",
    "        B is number of boxes (in paper 2),\n",
    "        C is number of classes (in paper 20, in dataset 3),\n",
    "        \"\"\"\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        # These are from Yolo paper, signifying how much we should\n",
    "        # pay loss for no object (noobj) and the box coordinates (coord)\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        # predictions are shaped (BATCH_SIZE, S*S(C+B*5) when inputted\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
    "\n",
    "        # Calculate IoU for the two predicted bounding boxes with target bbox\n",
    "        iou_b1 = intersection_over_union(predictions[..., self.C + 1:self.C + 5], target[..., self.C + 1:self.C + 5])\n",
    "        iou_b2 = intersection_over_union(predictions[..., self.C + 6:self.C + 10], target[..., self.C + 1:self.C + 5])\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "\n",
    "        # Take the box with highest IoU out of the two prediction\n",
    "        # Note that bestbox will be indices of 0, 1 for which bbox was best\n",
    "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
    "        exists_box = target[..., self.C].unsqueeze(3)  # in paper this is Iobj_i\n",
    "\n",
    "        # ======================== #\n",
    "        #   FOR BOX COORDINATES    #\n",
    "        # ======================== #\n",
    "\n",
    "        # Set boxes with no object in them to 0. We only take out one of the two \n",
    "        # predictions, which is the one with highest Iou calculated previously.\n",
    "        box_predictions = exists_box * (\n",
    "            (\n",
    "                bestbox * predictions[..., self.C + 6:self.C + 10]\n",
    "                + (1 - bestbox) * predictions[..., self.C + 1:self.C + 5]\n",
    "            )\n",
    "        )\n",
    "        #print(f\"box_predictions: {box_predictions.shape}\")\n",
    "        box_targets = exists_box * target[..., self.C + 1:self.C + 5]\n",
    "\n",
    "        # Take sqrt of width, height of boxes\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
    "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
    "        )\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions, end_dim=-2),\n",
    "            torch.flatten(box_targets, end_dim=-2),\n",
    "        )\n",
    "\n",
    "        # ==================== #\n",
    "        #   FOR OBJECT LOSS    #\n",
    "        # ==================== #\n",
    "\n",
    "        # pred_box is the confidence score for the bbox with highest IoU\n",
    "        pred_box = (\n",
    "            bestbox * predictions[..., self.C + 5:self.C + 6] + (1 - bestbox) * predictions[..., self.C:self.C + 1]\n",
    "        )\n",
    "\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_box * pred_box),\n",
    "            torch.flatten(exists_box * target[..., self.C:self.C + 1]),\n",
    "        )\n",
    "\n",
    "        # ======================= #\n",
    "        #   FOR NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "\n",
    "        #max_no_obj = torch.max(predictions[..., 20:21], predictions[..., 25:26])\n",
    "        #no_object_loss = self.mse(\n",
    "        #    torch.flatten((1 - exists_box) * max_no_obj, start_dim=1),\n",
    "        #    torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
    "        #)\n",
    "\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., self.C:self.C + 1], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1),\n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., self.C + 5:self.C + 6], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1)\n",
    "        )\n",
    "\n",
    "        # ================== #\n",
    "        #   FOR CLASS LOSS   #\n",
    "        # ================== #\n",
    "\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[..., :self.C], end_dim=-2,),\n",
    "            torch.flatten(exists_box * target[..., :self.C], end_dim=-2,),\n",
    "        )\n",
    "\n",
    "        loss = (\n",
    "            self.lambda_coord * box_loss  # first two rows in paper\n",
    "            + object_loss  # third row in paper\n",
    "            + self.lambda_noobj * no_object_loss  # forth row\n",
    "            + class_loss  # fifth row\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32 # 64 in original paper but resource exhausted error otherwise.\n",
    "EPOCHS = 100\n",
    "WEIGHT_DECAY = 0\n",
    "LOAD_MODEL = False\n",
    "LOAD_MODEL_FILE = \"tinyissimoyolo.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, optimizer, loss_fn):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    mean_loss = []\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "        mean_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_postfix(loss = loss.item())\n",
    "        \n",
    "    print(f\"Mean loss was {sum(mean_loss) / len(mean_loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, bboxes):\n",
    "        for t in self.transforms:\n",
    "            img, bboxes = t(img), bboxes\n",
    "\n",
    "        return img, bboxes\n",
    "\n",
    "\n",
    "transform = Compose([transforms.Resize((88, 88)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arch/ObjDct_Repo/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "files_dir = 'one_class_data'\n",
    "model = TinyissimoYOLO().to(DEVICE)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.1, patience=3, mode='max', verbose=True)\n",
    "loss_fn = YoloLoss()\n",
    "\n",
    "\n",
    "train_dataset = DiorDataset(\n",
    "    root_dir=files_dir,\n",
    "    transform=transform,\n",
    "    train=True\n",
    ")\n",
    "\n",
    "\n",
    "# Define the length of the training set\n",
    "train_len = int(0.8 * len(train_dataset))\n",
    "\n",
    "# Define the length of the test set\n",
    "test_len = len(train_dataset) - train_len\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = random_split(train_dataset, [train_len, test_len])\n",
    "\n",
    "# Now you can create your DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:03<00:00,  9.13it/s, loss=447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 719.810179574149\n",
      "Train mAP: 0.0\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.59it/s, loss=253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 422.5536368233817\n",
      "Train mAP: 3.6685614759335294e-06\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.62it/s, loss=176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 225.34084538051061\n",
      "Train mAP: 0.003888155799359083\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.60it/s, loss=98.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 146.88130378723145\n",
      "Train mAP: 0.08662635833024979\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.59it/s, loss=70.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 125.40262058803013\n",
      "Train mAP: 0.08273293077945709\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.56it/s, loss=123] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 117.03782163347516\n",
      "Train mAP: 0.08969369530677795\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.62it/s, loss=84.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 113.05041640145438\n",
      "Train mAP: 0.08100401610136032\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.63it/s, loss=83]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 109.81447328839984\n",
      "Train mAP: 0.07886483520269394\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.62it/s, loss=83.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 108.38046210152763\n",
      "Train mAP: 0.0897611677646637\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.68it/s, loss=75.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 107.46305819920131\n",
      "Train mAP: 0.08541983366012573\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.63it/s, loss=80.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 105.51799474443708\n",
      "Train mAP: 0.09449729323387146\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.61it/s, loss=68.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 104.8215697152274\n",
      "Train mAP: 0.11029557138681412\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.57it/s, loss=107] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 104.34016418457031\n",
      "Train mAP: 0.13075706362724304\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.59it/s, loss=93.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 102.57016835893903\n",
      "Train mAP: 0.09756141901016235\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.61it/s, loss=76.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 101.65516417367118\n",
      "Train mAP: 0.12226878106594086\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.58it/s, loss=124] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 100.6690559387207\n",
      "Train mAP: 0.09728464484214783\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.57it/s, loss=55.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 99.64991542271206\n",
      "Train mAP: 0.09149190038442612\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.64it/s, loss=89.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 98.10668427603585\n",
      "Train mAP: 0.09181180596351624\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.63it/s, loss=87.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.84250940595355\n",
      "Train mAP: 0.09312420338392258\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.53it/s, loss=84]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.74336569649833\n",
      "Train mAP: 0.09250491112470627\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.61it/s, loss=88.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.5862124306815\n",
      "Train mAP: 0.09331715852022171\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.60it/s, loss=44.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.4583022253854\n",
      "Train mAP: 0.09177926927804947\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.56it/s, loss=66]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.44770649501255\n",
      "Train mAP: 0.09261822700500488\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.62it/s, loss=67.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.43336296081543\n",
      "Train mAP: 0.09261690080165863\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.61it/s, loss=74.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.42236573355538\n",
      "Train mAP: 0.09262997657060623\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.60it/s, loss=76.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40694059644427\n",
      "Train mAP: 0.09263297915458679\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.59it/s, loss=47.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40581049237933\n",
      "Train mAP: 0.09263931214809418\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.63it/s, loss=83.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40458488464355\n",
      "Train mAP: 0.0926394835114479\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.69it/s, loss=93.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40334919520787\n",
      "Train mAP: 0.09264089912176132\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.56it/s, loss=71.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40161023821149\n",
      "Train mAP: 0.09264089912176132\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.63it/s, loss=72]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40156309945243\n",
      "Train mAP: 0.09264089912176132\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.57it/s, loss=64.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40148326328823\n",
      "Train mAP: 0.09264141321182251\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.57it/s, loss=70.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40143857683454\n",
      "Train mAP: 0.09264141321182251\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.61it/s, loss=87.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40137726919991\n",
      "Train mAP: 0.09264067560434341\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.63it/s, loss=84.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.4013066973005\n",
      "Train mAP: 0.09264067560434341\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.63it/s, loss=82.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40123667035785\n",
      "Train mAP: 0.09264067560434341\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.61it/s, loss=77.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40120124816895\n",
      "Train mAP: 0.09264067560434341\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.61it/s, loss=76.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40111841474261\n",
      "Train mAP: 0.09264067560434341\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.60it/s, loss=59.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.4010512488229\n",
      "Train mAP: 0.09264067560434341\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.61it/s, loss=72.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40098054068429\n",
      "Train mAP: 0.09264067560434341\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.60it/s, loss=85.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40093667166573\n",
      "Train mAP: 0.09264067560434341\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.62it/s, loss=62.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40083694458008\n",
      "Train mAP: 0.09264067560434341\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.62it/s, loss=66.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40081351143974\n",
      "Train mAP: 0.09264067560434341\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.65it/s, loss=52.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40070847102574\n",
      "Train mAP: 0.0926416888833046\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.61it/s, loss=67.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40063149588448\n",
      "Train mAP: 0.0926416888833046\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.58it/s, loss=124] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40060097830636\n",
      "Train mAP: 0.0926416888833046\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.62it/s, loss=123] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40050806318011\n",
      "Train mAP: 0.0926416888833046\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.63it/s, loss=66.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40042304992676\n",
      "Train mAP: 0.0926416888833046\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.61it/s, loss=85.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40035002572196\n",
      "Train mAP: 0.0926416888833046\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.60it/s, loss=119] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40029416765485\n",
      "Train mAP: 0.0926416888833046\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.58it/s, loss=89]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40022577558246\n",
      "Train mAP: 0.0926416888833046\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.60it/s, loss=72.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40016991751534\n",
      "Train mAP: 0.0926416888833046\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.64it/s, loss=101] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40008408682687\n",
      "Train mAP: 0.0926416888833046\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.65it/s, loss=81.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.40005057198661\n",
      "Train mAP: 0.0926416888833046\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.60it/s, loss=101] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39996201651437\n",
      "Train mAP: 0.0926416888833046\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.64it/s, loss=72.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39988463265556\n",
      "Train mAP: 0.0926416888833046\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.57it/s, loss=85.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39980915614537\n",
      "Train mAP: 0.0926416888833046\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.62it/s, loss=53.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.399735723223\n",
      "Train mAP: 0.0926416888833046\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.58it/s, loss=89.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.3996388571603\n",
      "Train mAP: 0.0926416888833046\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.55it/s, loss=72]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39958190917969\n",
      "Train mAP: 0.0926416888833046\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.51it/s, loss=68.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39951378958565\n",
      "Train mAP: 0.0926416888833046\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.56it/s, loss=65.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39944566999164\n",
      "Train mAP: 0.0926416888833046\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.56it/s, loss=70.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39934049333844\n",
      "Train mAP: 0.0926416888833046\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.54it/s, loss=97.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39925384521484\n",
      "Train mAP: 0.0926416888833046\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.52it/s, loss=81.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.3992007119315\n",
      "Train mAP: 0.0926416888833046\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.48it/s, loss=63.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39910207475934\n",
      "Train mAP: 0.0926416888833046\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.60it/s, loss=65.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39906256539481\n",
      "Train mAP: 0.0926416888833046\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.47it/s, loss=90.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39900166647774\n",
      "Train mAP: 0.09264269471168518\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.61it/s, loss=92.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39885875156948\n",
      "Train mAP: 0.09264269471168518\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.59it/s, loss=54.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39884335654122\n",
      "Train mAP: 0.09264269471168518\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.54it/s, loss=77.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.3987146105085\n",
      "Train mAP: 0.09264269471168518\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.53it/s, loss=82.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39868273053851\n",
      "Train mAP: 0.09264269471168518\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.49it/s, loss=112] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39861352103097\n",
      "Train mAP: 0.09264269471168518\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.50it/s, loss=64.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39854431152344\n",
      "Train mAP: 0.09264269471168518\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.45it/s, loss=80.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39842496599469\n",
      "Train mAP: 0.09264269471168518\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.56it/s, loss=109] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39833722795758\n",
      "Train mAP: 0.09264560043811798\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.61it/s, loss=84.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39826338631767\n",
      "Train mAP: 0.09264560043811798\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.58it/s, loss=69.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39819744655064\n",
      "Train mAP: 0.09264560043811798\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.52it/s, loss=84]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39809308733258\n",
      "Train mAP: 0.09264560043811798\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.51it/s, loss=58.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39801216125488\n",
      "Train mAP: 0.09264560043811798\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.59it/s, loss=105] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39797074454171\n",
      "Train mAP: 0.0926450565457344\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.50it/s, loss=61.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39783028193882\n",
      "Train mAP: 0.0926450565457344\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.54it/s, loss=75.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.3977691105434\n",
      "Train mAP: 0.0926450565457344\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.56it/s, loss=105] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39768900190082\n",
      "Train mAP: 0.092645563185215\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.50it/s, loss=76.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39761352539062\n",
      "Train mAP: 0.092645563185215\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.60it/s, loss=77.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39755167279925\n",
      "Train mAP: 0.092645563185215\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.56it/s, loss=62.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39745126451764\n",
      "Train mAP: 0.092645563185215\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.57it/s, loss=76.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39734486171177\n",
      "Train mAP: 0.092645563185215\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.57it/s, loss=77.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39726230076381\n",
      "Train mAP: 0.092645563185215\n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.59it/s, loss=51.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39722796848842\n",
      "Train mAP: 0.092645563185215\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.53it/s, loss=75.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39710508074079\n",
      "Train mAP: 0.092645563185215\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.54it/s, loss=87.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39702497209821\n",
      "Train mAP: 0.092645563185215\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.61it/s, loss=74.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39692442757743\n",
      "Train mAP: 0.092645563185215\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.62it/s, loss=97.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.3968734741211\n",
      "Train mAP: 0.092645563185215\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.56it/s, loss=59.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39675154004779\n",
      "Train mAP: 0.092645563185215\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.54it/s, loss=66.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39669500078473\n",
      "Train mAP: 0.092645563185215\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.59it/s, loss=72]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39663451058524\n",
      "Train mAP: 0.092645563185215\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.56it/s, loss=61.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39651162283761\n",
      "Train mAP: 0.092645563185215\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.64it/s, loss=59.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39641789027623\n",
      "Train mAP: 0.092645563185215\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00,  9.58it/s, loss=57.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 97.39635753631592\n",
      "Train mAP: 0.092645563185215\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    train_fn(train_loader, model, optimizer, loss_fn)\n",
    "    \n",
    "    pred_boxes, target_boxes = get_bboxes(\n",
    "        train_loader, model, iou_threshold=0.5, threshold=0.4\n",
    "    )\n",
    "    #print(f\"pred_boxes: {len(pred_boxes)}, target_boxes: {len(target_boxes)}\")\n",
    "    mean_avg_prec = mean_average_precision(\n",
    "        pred_boxes, target_boxes, iou_threshold=0.5\n",
    "    )\n",
    "    print(f\"Train mAP: {mean_avg_prec}\")\n",
    "    \n",
    "    \n",
    "    scheduler.step(mean_avg_prec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"tinyissimoyolo.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyissimoYOLO(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LinearActivation()\n",
       "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LinearActivation()\n",
       "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LinearActivation()\n",
       "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LinearActivation()\n",
       "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (fclayers): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=3200, out_features=256, bias=True)\n",
       "    (2): LinearActivation()\n",
       "    (3): Linear(in_features=256, out_features=176, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "checkpoint = torch.load(\"tinyissimoyolo.pth\")\n",
    "# Load the state dictionary from the .pth file\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test mAP: 0.09889210760593414\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_boxes, target_boxes = get_bboxes(\n",
    "        test_loader, model, iou_threshold=0.5, threshold=0.4\n",
    "    )\n",
    "\n",
    "    mean_avg_prec = mean_average_precision(\n",
    "        pred_boxes, target_boxes, iou_threshold=0.5\n",
    "    )\n",
    "    print(f\"Test mAP: {mean_avg_prec}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
