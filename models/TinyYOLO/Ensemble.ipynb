{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.onnx\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torchinfo import summary\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE:  cpu\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE: \", DEVICE)\n",
    "BATCH_SIZE = 32 # 64 in original paper but resource exhausted error otherwise.\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, bboxes):\n",
    "        for t in self.transforms:\n",
    "            img, bboxes = t(img), bboxes\n",
    "\n",
    "        return img, bboxes\n",
    "\n",
    "\n",
    "transform = Compose([transforms.Resize((88, 88)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DiorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, S=4, B=2, C=1, transform=None, train=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.train = train\n",
    "\n",
    "       # Determine the directory of the images and labels\n",
    "        if self.train:\n",
    "            self.img_dir = os.path.join(self.root_dir, 'images/train')\n",
    "            self.label_dir = os.path.join(self.root_dir, 'labels/train')\n",
    "        else:\n",
    "            self.img_dir = os.path.join(self.root_dir, 'images/test')\n",
    "            self.label_dir = os.path.join(self.root_dir, 'labels/test')\n",
    "\n",
    "        self.img_ids = os.listdir(self.img_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.img_ids[index].split('.')[0]\n",
    "        boxes = []\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, img_id + '.jpg')\n",
    "        image = Image.open(img_path)\n",
    "        image = image.convert(\"L\")\n",
    "\n",
    "        # Load labels\n",
    "        label_path = os.path.join(self.label_dir, img_id + '.txt')\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                class_label, x, y, width, height = map(float, line.strip().split())\n",
    "                boxes.append([class_label, x, y, width, height])\n",
    "        \n",
    "        if len(boxes) > 3:\n",
    "            boxes = boxes[:3]\n",
    "        boxes = torch.tensor(boxes)\n",
    "        #print(f\"boxes: {boxes}\")\n",
    "        if self.transform:\n",
    "            image, boxes = self.transform(image, boxes)\n",
    "\n",
    "        # Convert To Cells\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
    "        for box in boxes:\n",
    "            class_label, x, y, width, height = box.tolist()\n",
    "            class_label = int(class_label)\n",
    "\n",
    "            i, j = int(self.S * y), int(self.S * x)\n",
    "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
    "\n",
    "            width_cell, height_cell = (\n",
    "                width * self.S,\n",
    "                height * self.S,\n",
    "            )\n",
    "\n",
    "            if label_matrix[i, j, self.C] == 0:\n",
    "                label_matrix[i, j, self.C] = 1\n",
    "\n",
    "                box_coordinates = torch.tensor(\n",
    "                    [x_cell, y_cell, width_cell, height_cell]\n",
    "                )\n",
    "\n",
    "                label_matrix[i, j, self.C+1:self.C+5] = box_coordinates\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "    \n",
    "        #print(f\"label_matrix shape: {label_matrix.shape}\")\n",
    "\n",
    "        return image, label_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "files_dir = '/home/buono/ObjDct_Repo/data/ShipDataset'\n",
    "\n",
    "train_dataset = DiorDataset(\n",
    "    root_dir=files_dir,\n",
    "    transform=transform,\n",
    "    train=True\n",
    ")\n",
    "\n",
    "\n",
    "# Define the length of the training set and validation set\n",
    "train_len = int(0.8 * len(train_dataset))\n",
    "val_len = len(train_dataset) - train_len\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, validation_dataset = random_split(train_dataset, [train_len, val_len])\n",
    "\n",
    "\n",
    "# Now you can create your DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DiorDataset(\n",
    "    root_dir=files_dir,\n",
    "    transform=transform,\n",
    "    train=False\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25426, 6357, 7946)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(validation_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(boxes_preds, boxes_labels):\n",
    "    \"\"\"\n",
    "    Calculates intersection over union\n",
    "    \n",
    "    Parameters:\n",
    "        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)    \n",
    "    Returns:\n",
    "        tensor: Intersection over union for all examples\n",
    "    \"\"\"\n",
    "    # boxes_preds shape is (N, 4) where N is the number of bboxes\n",
    "    #boxes_labels shape is (n, 4)\n",
    "    \n",
    "    box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "    box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "    box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "    box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "    box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "    box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "    box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "    box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "    \n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "    #print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
    "    \n",
    "    #.clamp(0) is for the case when they don't intersect. Since when they don't intersect, one of these will be negative so that should become 0\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "    #print(f\"intersection: {intersection}\")\n",
    "\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "    #print(f\"box1_area: {box1_area}, box2_area: {box2_area}\")\n",
    "    \n",
    "    return intersection / (box1_area + box2_area - intersection + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(bboxes, iou_threshold, threshold):\n",
    "    \"\"\"\n",
    "    Does Non Max Suppression given bboxes\n",
    "    Parameters:\n",
    "        bboxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [class_pred, prob_score, x_center, y_center, width, height]\n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        threshold (float): threshold to remove predicted bboxes (independent of IoU) \n",
    "    Returns:\n",
    "        list: bboxes after performing NMS given a specific IoU threshold\n",
    "    \"\"\"\n",
    "\n",
    "    assert type(bboxes) == list\n",
    "\n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        chosen_box = bboxes.pop(0)\n",
    "\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            if box[0] != chosen_box[0]\n",
    "            or intersection_over_union(\n",
    "                torch.tensor(chosen_box[2:]),\n",
    "                torch.tensor(box[2:]),\n",
    "            )\n",
    "            < iou_threshold\n",
    "        ]\n",
    "\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "    #print(f\"bboxes_after_nms: {bboxes_after_nms}\")\n",
    "\n",
    "    return bboxes_after_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(\n",
    "    pred_boxes, true_boxes, iou_threshold=0.5, num_classes=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates mean average precision \n",
    "    Parameters:\n",
    "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [train_idx, class_prediction, prob_score, x_center, y_center, width, height]\n",
    "        true_boxes (list): Similar as pred_boxes except all the correct ones \n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        num_classes (int): number of classes\n",
    "    Returns:\n",
    "        float: mAP value across all classes given a specific IoU threshold \n",
    "    \"\"\"\n",
    "\n",
    "    # list storing all AP for respective classes\n",
    "    average_precisions = []\n",
    "\n",
    "    # used for numerical stability later on\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # Go through all predictions and targets,\n",
    "        # and only add the ones that belong to the\n",
    "        # current class c\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "        #print(f\"{c} class has {len(detections)} detections\")\n",
    "\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "        #print(f\"{c} class has {len(ground_truths)} ground truths\")\n",
    "\n",
    "        # find the amount of bboxes for each training example\n",
    "        # Counter here finds how many ground truth bboxes we get\n",
    "        # for each training example, so let's say img 0 has 3,\n",
    "        # img 1 has 5 then we will obtain a dictionary with:\n",
    "        # amount_bboxes = {0:3, 1:5}\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "        #print(f\"{c} class has {len(amount_bboxes)} amount bboxes\")\n",
    "\n",
    "        # We then go through each key, val in this dictionary\n",
    "        # and convert to the following (w.r.t same example):\n",
    "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # sort by box probabilities which is index 2\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "        #print(f\"{c} class has {total_true_bboxes} total true bboxes\")\n",
    "        # If none exists for this class then we can safely skip\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # Only take out the ground_truths that have the same\n",
    "            # training idx as detection\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            #print(f\"{c} class has {num_gts} ground truths for detection {detection_idx}\")\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:])\n",
    "                )\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    # true positive and add this bounding box to seen\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    #These additional detections are considered false positives because they do not correspond to a new, unique object\n",
    "                    #they're essentially \"over-detecting\" an object that has already been correctly identified.\n",
    "                    FP[detection_idx] = 1\n",
    "\n",
    "            # if IOU is lower then the detection is a false positive\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        #[1, 1, 0, 1, 0] -> [1, 2, 2, 3, 3]\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        # torch.trapz for numerical integration\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes(\n",
    "    loader,\n",
    "    model,\n",
    "    iou_threshold,\n",
    "    threshold,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    # make sure model is in eval before get bboxes\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "\n",
    "    for batch_idx, (x, labels) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        true_bboxes = cellboxes_to_boxes(labels)\n",
    "        bboxes = cellboxes_to_boxes(predictions)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            nms_boxes = non_max_suppression(\n",
    "                bboxes[idx],\n",
    "                iou_threshold=iou_threshold,\n",
    "                threshold=threshold,\n",
    "            )\n",
    "\n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx] + nms_box)\n",
    "\n",
    "            for box in true_bboxes[idx]:\n",
    "                # many will get converted to 0 pred\n",
    "                if box[1] > threshold:\n",
    "                    all_true_boxes.append([train_idx] + box)\n",
    "\n",
    "            train_idx += 1\n",
    "\n",
    "    model.train()\n",
    "    return all_pred_boxes, all_true_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cellboxes(predictions, S=4, C=1):\n",
    "    \"\"\"\n",
    "    Converts bounding boxes output from Yolo with\n",
    "    an image split size of S into entire image ratios\n",
    "    rather than relative to cell ratios.\n",
    "    \"\"\"\n",
    "    predictions = predictions.to(\"cpu\")\n",
    "    batch_size = predictions.shape[0]\n",
    "    predictions = predictions.reshape(batch_size, S, S, C + 10)\n",
    "    bboxes1 = predictions[..., C + 1:C + 5]\n",
    "    bboxes2 = predictions[..., C + 6:C + 10]\n",
    "    \n",
    "    scores = torch.cat(\n",
    "        (predictions[..., C].unsqueeze(0), predictions[..., C + 5].unsqueeze(0)), dim=0\n",
    "    )\n",
    "    best_box = scores.argmax(0).unsqueeze(-1)\n",
    "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
    "    # This results in a tensor with shape (batch_size, 7, 7, 1) where each element represents the index of a grid cell.\n",
    "    cell_indices = torch.arange(S).repeat(batch_size, S, 1).unsqueeze(-1)\n",
    "\n",
    "    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n",
    "    # Permute because is used here to swap these indices to match the (x, y) convention used in the best_boxes tensor.\n",
    "    # [0,1,2]->[0,0,0]\n",
    "    # [0,1,2]->[1,1,1]\n",
    "    # [0,1,2]->[2,2,2]\n",
    "    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
    "    w_y = 1 / S * best_boxes[..., 2:4]\n",
    "\n",
    "    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n",
    "    predicted_class = predictions[..., :C].argmax(-1).unsqueeze(-1)\n",
    "    best_confidence = torch.max(predictions[..., C], predictions[..., C + 5]).unsqueeze(\n",
    "        -1\n",
    "    )\n",
    "    converted_preds = torch.cat(\n",
    "        (predicted_class, best_confidence, converted_bboxes), dim=-1\n",
    "    )\n",
    "    #print(f\"converted_preds: {converted_preds}\")\n",
    "\n",
    "    return converted_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cellboxes_to_boxes(out, S=4):\n",
    "    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n",
    "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
    "    all_bboxes = []\n",
    "\n",
    "    #iterate over each batch sample\n",
    "    for ex_idx in range(out.shape[0]):\n",
    "        bboxes = []\n",
    "        #iterate over each grid in the grid cell\n",
    "        for bbox_idx in range(S * S):\n",
    "            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n",
    "        all_bboxes.append(bboxes)\n",
    "    #print(f\"all_bboxes: {all_bboxes}\")\n",
    "    return all_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the loss for yolo (v1) model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S=4, B=2, C=1):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        \"\"\"\n",
    "        S is split size of image (in paper 7),\n",
    "        B is number of boxes (in paper 2),\n",
    "        C is number of classes (in paper 20, in dataset 3),\n",
    "        \"\"\"\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        # These are from Yolo paper, signifying how much we should\n",
    "        # pay loss for no object (noobj) and the box coordinates (coord)\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        # predictions are shaped (BATCH_SIZE, S*S(C+B*5) when inputted\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
    "\n",
    "        # Calculate IoU for the two predicted bounding boxes with target bbox\n",
    "        iou_b1 = intersection_over_union(predictions[..., self.C + 1:self.C + 5], target[..., self.C + 1:self.C + 5])\n",
    "        iou_b2 = intersection_over_union(predictions[..., self.C + 6:self.C + 10], target[..., self.C + 1:self.C + 5])\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "\n",
    "        # Take the box with highest IoU out of the two prediction\n",
    "        # Note that bestbox will be indices of 0, 1 for which bbox was best\n",
    "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
    "        exists_box = target[..., self.C].unsqueeze(3)  # in paper this is Iobj_i\n",
    "\n",
    "        # ======================== #\n",
    "        #   FOR BOX COORDINATES    #\n",
    "        # ======================== #\n",
    "\n",
    "        # Set boxes with no object in them to 0. We only take out one of the two \n",
    "        # predictions, which is the one with highest Iou calculated previously.\n",
    "        box_predictions = exists_box * (\n",
    "            (\n",
    "                bestbox * predictions[..., self.C + 6:self.C + 10]\n",
    "                + (1 - bestbox) * predictions[..., self.C + 1:self.C + 5]\n",
    "            )\n",
    "        )\n",
    "        #print(f\"box_predictions: {box_predictions.shape}\")\n",
    "        box_targets = exists_box * target[..., self.C + 1:self.C + 5]\n",
    "\n",
    "        # Take sqrt of width, height of boxes\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
    "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
    "        )\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions, end_dim=-2),\n",
    "            torch.flatten(box_targets, end_dim=-2),\n",
    "        )\n",
    "\n",
    "        # ==================== #\n",
    "        #   FOR OBJECT LOSS    #\n",
    "        # ==================== #\n",
    "\n",
    "        # pred_box is the confidence score for the bbox with highest IoU\n",
    "        pred_box = (\n",
    "            bestbox * predictions[..., self.C + 5:self.C + 6] + (1 - bestbox) * predictions[..., self.C:self.C + 1]\n",
    "        )\n",
    "\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_box * pred_box),\n",
    "            torch.flatten(exists_box * target[..., self.C:self.C + 1]),\n",
    "        )\n",
    "\n",
    "        # ======================= #\n",
    "        #   FOR NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "\n",
    "        #max_no_obj = torch.max(predictions[..., 20:21], predictions[..., 25:26])\n",
    "        #no_object_loss = self.mse(\n",
    "        #    torch.flatten((1 - exists_box) * max_no_obj, start_dim=1),\n",
    "        #    torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
    "        #)\n",
    "\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., self.C:self.C + 1], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1),\n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., self.C + 5:self.C + 6], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1)\n",
    "        )\n",
    "\n",
    "        # ================== #\n",
    "        #   FOR CLASS LOSS   #\n",
    "        # ================== #\n",
    "\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[..., :self.C], end_dim=-2,),\n",
    "            torch.flatten(exists_box * target[..., :self.C], end_dim=-2,),\n",
    "        )\n",
    "\n",
    "        loss = (\n",
    "            self.lambda_coord * box_loss  # first two rows in paper\n",
    "            + object_loss  # third row in paper\n",
    "            + self.lambda_noobj * no_object_loss  # forth row\n",
    "            + class_loss  # fifth row\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tinyissimo YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, t):\n",
    "        return torch.pow(t, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TinyissimoYOLO                           [1, 176]                  --\n",
       "├─Sequential: 1-1                        [1, 16, 44, 44]           --\n",
       "│    └─Conv2d: 2-1                       [1, 16, 88, 88]           160\n",
       "│    └─LinearActivation: 2-2             [1, 16, 88, 88]           --\n",
       "│    └─AvgPool2d: 2-3                    [1, 16, 44, 44]           --\n",
       "├─Sequential: 1-2                        [1, 32, 22, 22]           --\n",
       "│    └─Conv2d: 2-4                       [1, 32, 44, 44]           4,640\n",
       "│    └─LinearActivation: 2-5             [1, 32, 44, 44]           --\n",
       "│    └─AvgPool2d: 2-6                    [1, 32, 22, 22]           --\n",
       "├─Sequential: 1-3                        [1, 64, 11, 11]           --\n",
       "│    └─Conv2d: 2-7                       [1, 64, 22, 22]           18,496\n",
       "│    └─LinearActivation: 2-8             [1, 64, 22, 22]           --\n",
       "│    └─AvgPool2d: 2-9                    [1, 64, 11, 11]           --\n",
       "├─Sequential: 1-4                        [1, 128, 5, 5]            --\n",
       "│    └─Conv2d: 2-10                      [1, 128, 11, 11]          73,856\n",
       "│    └─LinearActivation: 2-11            [1, 128, 11, 11]          --\n",
       "│    └─AvgPool2d: 2-12                   [1, 128, 5, 5]            --\n",
       "├─Sequential: 1-5                        [1, 176]                  --\n",
       "│    └─Flatten: 2-13                     [1, 3200]                 --\n",
       "│    └─Linear: 2-14                      [1, 256]                  819,456\n",
       "│    └─LinearActivation: 2-15            [1, 256]                  --\n",
       "│    └─Linear: 2-16                      [1, 176]                  45,232\n",
       "==========================================================================================\n",
       "Total params: 961,840\n",
       "Trainable params: 961,840\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 28.98\n",
       "==========================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 1.86\n",
       "Params size (MB): 3.85\n",
       "Estimated Total Size (MB): 5.74\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TinyissimoYOLO(L.LightningModule):\n",
    "    def __init__(self, B=2, num_classes=1, S=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss_fn = YoloLoss()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            LinearActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            LinearActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            LinearActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2,stride = 2)\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            LinearActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2,stride = 2)\n",
    "        )\n",
    "\n",
    "        self.fclayers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*5*5, 256),\n",
    "            LinearActivation(),\n",
    "            nn.Linear(256, S*S*(num_classes + 5*B)),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.fclayers(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.loss_fn(out, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.loss_fn(out, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "\n",
    "\n",
    "model = TinyissimoYOLO()\n",
    "summary(model, input_size=(1, 1, 88, 88))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "loss_fn = YoloLoss()\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(save_top_k=1, \n",
    "                                      monitor=\"val_loss\", \n",
    "                                      mode=\"min\")\n",
    "lr_monitor_callback = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "trainer = L.Trainer(accelerator=DEVICE, \n",
    "                    callbacks=[checkpoint_callback, lr_monitor_callback], \n",
    "                    max_epochs=EPOCHS, \n",
    "                    enable_progress_bar=True, \n",
    "                    enable_model_summary=True)\n",
    "\n",
    "os.makedirs(\"lightning_logs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name     | Type       | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | loss_fn  | YoloLoss   | 0      | train\n",
      "1 | layer1   | Sequential | 160    | train\n",
      "2 | layer2   | Sequential | 4.6 K  | train\n",
      "3 | layer3   | Sequential | 18.5 K | train\n",
      "4 | layer4   | Sequential | 73.9 K | train\n",
      "5 | fclayers | Sequential | 864 K  | train\n",
      "------------------------------------------------\n",
      "961 K     Trainable params\n",
      "0         Non-trainable params\n",
      "961 K     Total params\n",
      "3.847     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97eb69c9e74442b59a9107362be9309e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buono/ObjDct_Repo/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fddf4dad307b437e9937a98ff20abd8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca079ad9a7d44d8288dbfef36a6bced3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394a2cef9e264d8c9f5dfa1f74b3110f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test mAP: 0.0\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"../../models/trained_models/TinyissimoYOLO.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different number of filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TinyissimoYOLOv1                         [1, 176]                  --\n",
       "├─Sequential: 1-1                        [1, 12, 44, 44]           --\n",
       "│    └─Conv2d: 2-1                       [1, 12, 88, 88]           120\n",
       "│    └─LinearActivation: 2-2             [1, 12, 88, 88]           --\n",
       "│    └─AvgPool2d: 2-3                    [1, 12, 44, 44]           --\n",
       "├─Sequential: 1-2                        [1, 24, 22, 22]           --\n",
       "│    └─Conv2d: 2-4                       [1, 24, 44, 44]           2,616\n",
       "│    └─LinearActivation: 2-5             [1, 24, 44, 44]           --\n",
       "│    └─AvgPool2d: 2-6                    [1, 24, 22, 22]           --\n",
       "├─Sequential: 1-3                        [1, 48, 11, 11]           --\n",
       "│    └─Conv2d: 2-7                       [1, 48, 22, 22]           10,416\n",
       "│    └─LinearActivation: 2-8             [1, 48, 22, 22]           --\n",
       "│    └─AvgPool2d: 2-9                    [1, 48, 11, 11]           --\n",
       "├─Sequential: 1-4                        [1, 96, 5, 5]             --\n",
       "│    └─Conv2d: 2-10                      [1, 96, 11, 11]           41,568\n",
       "│    └─LinearActivation: 2-11            [1, 96, 11, 11]           --\n",
       "│    └─AvgPool2d: 2-12                   [1, 96, 5, 5]             --\n",
       "├─Sequential: 1-5                        [1, 176]                  --\n",
       "│    └─Flatten: 2-13                     [1, 2400]                 --\n",
       "│    └─Linear: 2-14                      [1, 256]                  614,656\n",
       "│    └─LinearActivation: 2-15            [1, 256]                  --\n",
       "│    └─Linear: 2-16                      [1, 176]                  45,232\n",
       "==========================================================================================\n",
       "Total params: 714,608\n",
       "Trainable params: 714,608\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 16.72\n",
       "==========================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 1.40\n",
       "Params size (MB): 2.86\n",
       "Estimated Total Size (MB): 4.29\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TinyissimoYOLOv1(L.LightningModule):\n",
    "    def __init__(self, B=2, num_classes=1, S=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss_fn = YoloLoss()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 12, kernel_size=3, stride=1, padding=1),\n",
    "            LinearActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(12, 24, kernel_size=3, stride=1, padding=1),\n",
    "            LinearActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(24, 48, kernel_size=3, stride=1, padding=1),\n",
    "            LinearActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(48, 96, kernel_size=3, stride=1, padding=1),\n",
    "            LinearActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.fclayers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(96*5*5, 256),\n",
    "            LinearActivation(),\n",
    "            nn.Linear(256, S*S*(num_classes + 5*B)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.fclayers(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.loss_fn(out, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.loss_fn(out, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "model = TinyissimoYOLOv1()\n",
    "summary(model, input_size=(1, 1, 88, 88))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../../models/trained_models/TinyissimoYOLOv1.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a convolution level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TinyissimoYOLOv2                         [1, 176]                  --\n",
       "├─Sequential: 1-1                        [1, 16, 44, 44]           --\n",
       "│    └─Conv2d: 2-1                       [1, 16, 88, 88]           160\n",
       "│    └─LinearActivation: 2-2             [1, 16, 88, 88]           --\n",
       "│    └─AvgPool2d: 2-3                    [1, 16, 44, 44]           --\n",
       "├─Sequential: 1-2                        [1, 32, 22, 22]           --\n",
       "│    └─Conv2d: 2-4                       [1, 32, 44, 44]           4,640\n",
       "│    └─LinearActivation: 2-5             [1, 32, 44, 44]           --\n",
       "│    └─AvgPool2d: 2-6                    [1, 32, 22, 22]           --\n",
       "├─Sequential: 1-3                        [1, 64, 11, 11]           --\n",
       "│    └─Conv2d: 2-7                       [1, 64, 22, 22]           18,496\n",
       "│    └─LinearActivation: 2-8             [1, 64, 22, 22]           --\n",
       "│    └─AvgPool2d: 2-9                    [1, 64, 11, 11]           --\n",
       "├─Sequential: 1-4                        [1, 128, 5, 5]            --\n",
       "│    └─Conv2d: 2-10                      [1, 128, 11, 11]          73,856\n",
       "│    └─LinearActivation: 2-11            [1, 128, 11, 11]          --\n",
       "│    └─AvgPool2d: 2-12                   [1, 128, 5, 5]            --\n",
       "├─Sequential: 1-5                        [1, 256, 2, 2]            --\n",
       "│    └─Conv2d: 2-13                      [1, 256, 5, 5]            295,168\n",
       "│    └─LinearActivation: 2-14            [1, 256, 5, 5]            --\n",
       "│    └─AvgPool2d: 2-15                   [1, 256, 2, 2]            --\n",
       "├─Sequential: 1-6                        [1, 176]                  --\n",
       "│    └─Flatten: 2-16                     [1, 1024]                 --\n",
       "│    └─Linear: 2-17                      [1, 512]                  524,800\n",
       "│    └─LinearActivation: 2-18            [1, 512]                  --\n",
       "│    └─Linear: 2-19                      [1, 176]                  90,288\n",
       "==========================================================================================\n",
       "Total params: 1,007,408\n",
       "Trainable params: 1,007,408\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 36.11\n",
       "==========================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 1.92\n",
       "Params size (MB): 4.03\n",
       "Estimated Total Size (MB): 5.98\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TinyissimoYOLOv2(L.LightningModule):\n",
    "    def __init__(self, B=2, num_classes=1, S=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss_fn = YoloLoss()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            LinearActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            LinearActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            LinearActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            LinearActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            LinearActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.fclayers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256*2*2, 512),\n",
    "            LinearActivation(),\n",
    "            nn.Linear(512, S*S*(num_classes + 5*B)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.fclayers(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.loss_fn(out, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.loss_fn(out, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "model = TinyissimoYOLOv2()\n",
    "summary(model, input_size=(1, 1, 88, 88))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../../models/trained_models/TinyissimoYOLOv2.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different kernel size and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TinyissimoYOLOv3                         [1, 176]                  --\n",
       "├─Sequential: 1-1                        [1, 16, 44, 44]           --\n",
       "│    └─Conv2d: 2-1                       [1, 16, 88, 88]           416\n",
       "│    └─LinearActivation: 2-2             [1, 16, 88, 88]           --\n",
       "│    └─MaxPool2d: 2-3                    [1, 16, 44, 44]           --\n",
       "├─Sequential: 1-2                        [1, 32, 22, 22]           --\n",
       "│    └─Conv2d: 2-4                       [1, 32, 44, 44]           12,832\n",
       "│    └─LinearActivation: 2-5             [1, 32, 44, 44]           --\n",
       "│    └─MaxPool2d: 2-6                    [1, 32, 22, 22]           --\n",
       "├─Sequential: 1-3                        [1, 64, 11, 11]           --\n",
       "│    └─Conv2d: 2-7                       [1, 64, 22, 22]           51,264\n",
       "│    └─LinearActivation: 2-8             [1, 64, 22, 22]           --\n",
       "│    └─MaxPool2d: 2-9                    [1, 64, 11, 11]           --\n",
       "├─Sequential: 1-4                        [1, 128, 5, 5]            --\n",
       "│    └─Conv2d: 2-10                      [1, 128, 11, 11]          204,928\n",
       "│    └─LinearActivation: 2-11            [1, 128, 11, 11]          --\n",
       "│    └─MaxPool2d: 2-12                   [1, 128, 5, 5]            --\n",
       "├─Sequential: 1-5                        [1, 176]                  --\n",
       "│    └─Flatten: 2-13                     [1, 3200]                 --\n",
       "│    └─Linear: 2-14                      [1, 256]                  819,456\n",
       "│    └─LinearActivation: 2-15            [1, 256]                  --\n",
       "│    └─Linear: 2-16                      [1, 176]                  45,232\n",
       "==========================================================================================\n",
       "Total params: 1,134,128\n",
       "Trainable params: 1,134,128\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 78.54\n",
       "==========================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 1.86\n",
       "Params size (MB): 4.54\n",
       "Estimated Total Size (MB): 6.43\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TinyissimoYOLOv3(L.LightningModule):\n",
    "    def __init__(self, B=2, num_classes=1, S=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss_fn = YoloLoss()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            LinearActivation(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            LinearActivation(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            LinearActivation(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n",
    "            LinearActivation(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.fclayers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*5*5, 256),\n",
    "            LinearActivation(),\n",
    "            nn.Linear(256, S*S*(num_classes + 5*B)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.fclayers(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.loss_fn(out, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.loss_fn(out, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "model = TinyissimoYOLOv3()\n",
    "summary(model, input_size=(1, 1, 88, 88))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../../models/trained_models/TinyissimoYOLOv3.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TinyissimoYOLOv4                         [1, 176]                  --\n",
       "├─Sequential: 1-1                        [1, 16, 44, 44]           --\n",
       "│    └─Conv2d: 2-1                       [1, 16, 88, 88]           160\n",
       "│    └─LinearActivation: 2-2             [1, 16, 88, 88]           --\n",
       "│    └─AvgPool2d: 2-3                    [1, 16, 44, 44]           --\n",
       "├─Sequential: 1-2                        [1, 32, 22, 22]           --\n",
       "│    └─Conv2d: 2-4                       [1, 32, 44, 44]           4,640\n",
       "│    └─LinearActivation: 2-5             [1, 32, 44, 44]           --\n",
       "│    └─AvgPool2d: 2-6                    [1, 32, 22, 22]           --\n",
       "├─Sequential: 1-3                        [1, 64, 11, 11]           --\n",
       "│    └─Conv2d: 2-7                       [1, 64, 22, 22]           18,496\n",
       "│    └─LinearActivation: 2-8             [1, 64, 22, 22]           --\n",
       "│    └─AvgPool2d: 2-9                    [1, 64, 11, 11]           --\n",
       "├─Sequential: 1-4                        [1, 128, 5, 5]            --\n",
       "│    └─Conv2d: 2-10                      [1, 128, 11, 11]          73,856\n",
       "│    └─LinearActivation: 2-11            [1, 128, 11, 11]          --\n",
       "│    └─AvgPool2d: 2-12                   [1, 128, 5, 5]            --\n",
       "├─Sequential: 1-5                        [1, 176]                  --\n",
       "│    └─Flatten: 2-13                     [1, 3200]                 --\n",
       "│    └─Linear: 2-14                      [1, 256]                  819,456\n",
       "│    └─LinearActivation: 2-15            [1, 256]                  --\n",
       "│    └─Dropout: 2-16                     [1, 256]                  --\n",
       "│    └─Linear: 2-17                      [1, 176]                  45,232\n",
       "==========================================================================================\n",
       "Total params: 961,840\n",
       "Trainable params: 961,840\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 28.98\n",
       "==========================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 1.86\n",
       "Params size (MB): 3.85\n",
       "Estimated Total Size (MB): 5.74\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TinyissimoYOLOv4(L.LightningModule):\n",
    "    def __init__(self, B=2, num_classes=1, S=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss_fn = YoloLoss()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            LinearActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            LinearActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            LinearActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            LinearActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.fclayers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*5*5, 256),\n",
    "            LinearActivation(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256, S*S*(num_classes + 5*B)),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.fclayers(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.loss_fn(out, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.loss_fn(out, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "model = TinyissimoYOLOv4()\n",
    "summary(model, input_size=(1, 1, 88, 88))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../../models/trained_models/TinyissimoYOLOv4.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = []\n",
    "\n",
    "# model = TinyissimoYOLO()\n",
    "# model.load_state_dict(torch.load(\"../../models/trained_models/TinyissimoYOLO.pth\"))\n",
    "# models.append(model)\n",
    "\n",
    "# model = TinyissimoYOLOv1()\n",
    "# model.load_state_dict(torch.load(\"../../models/trained_models/TinyissimoYOLOv1.pth\"))\n",
    "# models.append(model)\n",
    "\n",
    "# model = TinyissimoYOLOv2()\n",
    "# model.load_state_dict(torch.load(\"../../models/trained_models/TinyissimoYOLOv2.pth\"))\n",
    "# models.append(model)\n",
    "\n",
    "# model = TinyissimoYOLOv3()\n",
    "# model.load_state_dict(torch.load(\"../../models/trained_models/TinyissimoYOLOv3.pth\"))\n",
    "# models.append(model)\n",
    "\n",
    "# model = TinyissimoYOLOv4()\n",
    "# model.load_state_dict(torch.load(\"../../models/trained_models/TinyissimoYOLOv4.pth\"))\n",
    "# models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_boxes_list = []\n",
    "# target_boxes_list = []\n",
    "# with torch.no_grad():\n",
    "#     pred_boxes, target_boxes = get_bboxes(\n",
    "#         test_loader, models[0], iou_threshold=0.5, threshold=0.4\n",
    "#     )\n",
    "#     pred_boxes_list.append(pred_boxes)\n",
    "#     target_boxes_list.append(target_boxes)\n",
    "\n",
    "#     pred_boxes, target_boxes = get_bboxes(\n",
    "#         test_loader, models[1], iou_threshold=0.5, threshold=0.4\n",
    "#     )\n",
    "#     pred_boxes_list.append(pred_boxes)\n",
    "#     target_boxes_list.append(target_boxes)\n",
    "\n",
    "#     pred_boxes, target_boxes = get_bboxes(\n",
    "#         test_loader, models[2], iou_threshold=0.5, threshold=0.4\n",
    "#     )\n",
    "#     pred_boxes_list.append(pred_boxes)\n",
    "#     target_boxes_list.append(target_boxes)\n",
    "\n",
    "#     pred_boxes, target_boxes = get_bboxes(\n",
    "#         test_loader, models[3], iou_threshold=0.5, threshold=0.4\n",
    "#     )\n",
    "#     pred_boxes_list.append(pred_boxes)\n",
    "#     target_boxes_list.append(target_boxes)\n",
    "\n",
    "#     pred_boxes, target_boxes = get_bboxes(\n",
    "#         test_loader, models[4], iou_threshold=0.5, threshold=0.4\n",
    "#     )\n",
    "#     pred_boxes_list.append(pred_boxes)\n",
    "#     target_boxes_list.append(target_boxes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_boxes_list = np.array(pred_boxes_list)\n",
    "# target_boxes_list = np.array(target_boxes_list)\n",
    "\n",
    "# pred_boxes_list.shape, target_boxes_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ensemble_center_predictions_weighted(predictions):\n",
    "#     \"\"\"\n",
    "#     Ensemble predictions from multiple models by averaging center coordinates and confidence scores\n",
    "#     with specific weights for each model.\n",
    "\n",
    "#     Parameters:\n",
    "#         predictions (numpy.ndarray): Array of shape (num_models, num_predictions, 7) containing\n",
    "#                                      the predictions from each model. Each prediction is in the\n",
    "#                                      format [predicted_class, best_confidence, x_center, y_center, width, height].\n",
    "        \n",
    "#     Returns:\n",
    "#         numpy.ndarray: Array of ensemble predictions with weights applied.\n",
    "#     \"\"\"\n",
    "#     # Define weights for each model\n",
    "#     weights = np.array([0.2,0.2,0.2,0.2,0.2])\n",
    "    \n",
    "#     def average_predictions_weighted(predictions):\n",
    "#         # Apply weights to confidence scores and center coordinates\n",
    "#         weighted_confidences = np.average(predictions[:, 1], axis=0, weights=weights)\n",
    "#         weighted_centers = np.average(predictions[:, 2:], axis=0, weights=weights)\n",
    "        \n",
    "#         # Assuming class_ids are the same for all models, take the class_id from the first model\n",
    "#         class_id = predictions[0, 0]\n",
    "        \n",
    "#         return np.array([class_id, weighted_confidences, *weighted_centers])\n",
    "    \n",
    "#     all_predictions = []\n",
    "    \n",
    "#     for i in range(predictions.shape[1]):  # Iterate over each prediction index\n",
    "#         single_prediction = predictions[:, i, :]\n",
    "#         ensembled_prediction = average_predictions_weighted(single_prediction)\n",
    "#         all_predictions.append(ensembled_prediction)\n",
    "    \n",
    "#     return np.array(all_predictions)\n",
    "\n",
    "# # Example usage\n",
    "# predictions = np.random.rand(5, 7946, 7)  # Example array with predictions\n",
    "# ensembled_predictions = ensemble_center_predictions_weighted(predictions)\n",
    "# print(ensembled_predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ensemble_center_predictions_weighted(predictions):\n",
    "#     \"\"\"\n",
    "#     Ensemble predictions from multiple models by averaging center coordinates and confidence scores\n",
    "#     with specific weights for each model.\n",
    "\n",
    "#     Parameters:\n",
    "#         predictions (numpy.ndarray): Array of shape (num_models, num_predictions, 7) containing\n",
    "#                                      the predictions from each model. Each prediction is in the\n",
    "#                                      format [predicted_class, best_confidence, x_center, y_center, width, height].\n",
    "        \n",
    "#     Returns:\n",
    "#         numpy.ndarray: Array of ensemble predictions with weights applied.\n",
    "#     \"\"\"\n",
    "#     # Define weights for each model\n",
    "#     weights = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "    \n",
    "#     def average_predictions_weighted(single_predictions):\n",
    "#         # Extract confidences and coordinates\n",
    "#         confidences = single_predictions[:, 1]\n",
    "#         centers = single_predictions[:, 2:6]\n",
    "        \n",
    "#         # Calculate weighted average of coordinates\n",
    "#         weighted_centers = np.average(centers, axis=0, weights=confidences)\n",
    "        \n",
    "#         # Calculate weighted average confidence\n",
    "#         weighted_confidence = np.average(confidences, axis=0, weights=confidences)\n",
    "        \n",
    "#         # Assuming class_ids are the same for all models, take the class_id from the first model\n",
    "#         class_id = single_predictions[0, 0]\n",
    "        \n",
    "#         return np.array([class_id, weighted_confidence, *weighted_centers])\n",
    "    \n",
    "#     all_predictions = []\n",
    "    \n",
    "#     for i in range(predictions.shape[1]):  # Iterate over each prediction index\n",
    "#         single_prediction = predictions[:, i, :]\n",
    "#         ensembled_prediction = average_predictions_weighted(single_prediction)\n",
    "#         all_predictions.append(ensembled_prediction)\n",
    "    \n",
    "#     return np.array(all_predictions)\n",
    "\n",
    "# # Example usage\n",
    "# predictions = np.random.rand(5, 7946, 7)  # Example array with predictions\n",
    "# ensembled_predictions = ensemble_center_predictions_weighted(predictions)\n",
    "# print(ensembled_predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensembled_predictions = ensemble_center_predictions_weighted(pred_boxes_list)\n",
    "# mAP = mean_average_precision(ensembled_predictions,target_boxes_list[0], iou_threshold=0.5, num_classes=1)\n",
    "# print(f\"mAP: {mAP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounding_boxes = np.array(target_boxes_list[0])[:,2:]\n",
    "# bounding_boxes\n",
    "\n",
    "# areas = np.sqrt([width * height for (x_center, y_center, width, height) in bounding_boxes])\n",
    "\n",
    "# # Raggruppamento delle bounding box per dimensione\n",
    "# bins = [0, 0.1, 0.15, 0.2, 0.25]\n",
    "# labels = ['Very Small', 'Small', 'Medium', 'Large', 'Very Large']\n",
    "# box_sizes = np.digitize(areas, bins=bins, right=True)\n",
    "\n",
    "# # Creazione dell'istogramma delle dimensioni delle bounding box\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.hist(areas, bins=bins, edgecolor='black')\n",
    "# plt.title('Histogram of Bounding Box Sizes')\n",
    "# plt.xlabel('Bounding Box Area')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.xticks(ticks=bins, labels=labels, rotation=45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_iou(box1, box2):\n",
    "#     # box1 e box2 devono essere nella forma [x_center, y_center, width, height]\n",
    "#     x1_min = box1[0] - box1[2] / 2\n",
    "#     x1_max = box1[0] + box1[2] / 2\n",
    "#     y1_min = box1[1] - box1[3] / 2\n",
    "#     y1_max = box1[1] + box1[3] / 2\n",
    "    \n",
    "#     x2_min = box2[0] - box2[2] / 2\n",
    "#     x2_max = box2[0] + box2[2] / 2\n",
    "#     y2_min = box2[1] - box2[3] / 2\n",
    "#     y2_max = box2[1] + box2[3] / 2\n",
    "    \n",
    "#     # Calcolo delle coordinate dell'intersezione\n",
    "#     x_inter_min = max(x1_min, x2_min)\n",
    "#     x_inter_max = min(x1_max, x2_max)\n",
    "#     y_inter_min = max(y1_min, y2_min)\n",
    "#     y_inter_max = min(y1_max, y2_max)\n",
    "    \n",
    "#     # Calcolo dell'area dell'intersezione\n",
    "#     inter_area = max(0, x_inter_max - x_inter_min) * max(0, y_inter_max - y_inter_min)\n",
    "    \n",
    "#     # Calcolo dell'area delle due box\n",
    "#     box1_area = (x1_max - x1_min) * (y1_max - y1_min)\n",
    "#     box2_area = (x2_max - x2_min) * (y2_max - y2_min)\n",
    "    \n",
    "#     # Calcolo dell'area dell'unione\n",
    "#     union_area = box1_area + box2_area - inter_area\n",
    "    \n",
    "#     # Calcolo dell'IoU\n",
    "#     iou = inter_area / union_area if union_area != 0 else 0\n",
    "#     return iou\n",
    "\n",
    "# def get_ious(predictions, real_boxes, bins, box_sizes):\n",
    "#     ious = []\n",
    "#     for idx_p, _, _, x_p, y_p, w_p, h_p in predictions:\n",
    "#         for idx_b, _, x_center, y_center, width, height in real_boxes:\n",
    "#             if idx_p == idx_b:\n",
    "#                 pred_box = [x_p, y_p, w_p, h_p]\n",
    "#                 real_box = [x_center, y_center, width, height]\n",
    "#                 iou = calculate_iou(pred_box, real_box)\n",
    "#                 ious.append(iou)\n",
    "\n",
    "#     # Calcolo della media dell'IoU per ciascun gruppo di dimensioni\n",
    "#     iou_means = []\n",
    "#     for i in range(len(bins) - 1):\n",
    "#         group_ious = [iou for iou, size in zip(ious, box_sizes) if size == i + 1]\n",
    "#         if group_ious:\n",
    "#             iou_means.append(np.mean(group_ious))\n",
    "#         else:\n",
    "#             iou_means.append(0)\n",
    "    \n",
    "#     return iou_means\n",
    "\n",
    "# # Calcolo dell'IoU medio per ciascun gruppo di dimensioni\n",
    "# ious = get_ious(ensembled_predictions, target_boxes_list[0], bins, box_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creiamo una lista di etichette per le categorie di dimensioni\n",
    "# labels = ['Very Small', 'Small', 'Medium', 'Large']\n",
    "\n",
    "# # Configuriamo il numero di gruppi e il numero di barre per gruppo\n",
    "# x = np.arange(len(labels))\n",
    "# width = 0.1  # Larghezza delle barre\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# # Aggiungiamo le barre per ogni set di distance_means\n",
    "# rects1 = ax.bar(x - 2 * width, get_ious(pred_boxes_list[0], target_boxes_list[0], bins, box_sizes), width, label='1st Model', color='red')\n",
    "# rects2 = ax.bar(x - 1 * width, get_ious(pred_boxes_list[1], target_boxes_list[0], bins, box_sizes), width, label='2nd Model', color='blue')\n",
    "# rects3 = ax.bar(x - 0 * width, get_ious(pred_boxes_list[2], target_boxes_list[0], bins, box_sizes), width, label='3rd Model', color='pink')\n",
    "# rects4 = ax.bar(x + 1 * width, get_ious(pred_boxes_list[3], target_boxes_list[0], bins, box_sizes), width, label='4th Model', color='green')\n",
    "# rects5 = ax.bar(x + 2 * width, get_ious(pred_boxes_list[4], target_boxes_list[0], bins, box_sizes), width, label='5th Model', color='purple')\n",
    "# rects6 = ax.bar(x + 3 * width, get_ious(ensembled_predictions, target_boxes_list[0], bins, box_sizes), width, label='Ensemble', color='black')\n",
    "\n",
    "# # Aggiungiamo le etichette, il titolo e la legenda\n",
    "# ax.set_xlabel('Bounding Box Size Category')\n",
    "# ax.set_ylabel('Average Distance')\n",
    "# ax.set_title('Average Distance by Bounding Box Size')\n",
    "# ax.set_xticks(x)\n",
    "# ax.set_xticklabels(labels)\n",
    "# ax.legend()\n",
    "\n",
    "# # Aggiungiamo una disposizione più compatta\n",
    "# fig.tight_layout()\n",
    "\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
