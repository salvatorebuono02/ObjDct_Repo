{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a tiny version of YOLO with DIOR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement YOLO architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, t):\n",
    "        return torch.pow(t, 1) + torch.pow(t,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TinyYOLOv1                               [1, 539]                  --\n",
       "├─Conv2d: 1-1                            [1, 8, 128, 128]          80\n",
       "├─SquareActivation: 1-2                  [1, 8, 128, 128]          --\n",
       "├─BatchNorm2d: 1-3                       [1, 8, 128, 128]          16\n",
       "├─AvgPool2d: 1-4                         [1, 8, 64, 64]            --\n",
       "├─Conv2d: 1-5                            [1, 16, 32, 32]           1,168\n",
       "├─SquareActivation: 1-6                  [1, 16, 32, 32]           --\n",
       "├─BatchNorm2d: 1-7                       [1, 16, 32, 32]           32\n",
       "├─AvgPool2d: 1-8                         [1, 16, 16, 16]           --\n",
       "├─Flatten: 1-9                           [1, 4096]                 --\n",
       "├─Linear: 1-10                           [1, 2048]                 8,390,656\n",
       "├─SquareActivation: 1-11                 [1, 2048]                 --\n",
       "├─Linear: 1-12                           [1, 539]                  1,104,411\n",
       "==========================================================================================\n",
       "Total params: 9,496,363\n",
       "Trainable params: 9,496,363\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 12.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.26\n",
       "Forward/backward pass size (MB): 2.38\n",
       "Params size (MB): 37.99\n",
       "Estimated Total Size (MB): 40.63\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TinyYOLOv1(nn.Module):\n",
    "    def __init__(self, B=2, num_classes=1):\n",
    "        super(TinyYOLOv1, self).__init__()\n",
    "        S = 7  # grid size\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, stride=2, padding=1)\n",
    "        self.square_activation1 = SquareActivation()\n",
    "        self.batch_norm1 = nn.BatchNorm2d(8)\n",
    "        self.avgpool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.square_activation2 = SquareActivation()\n",
    "        self.batch_norm2 = nn.BatchNorm2d(16)\n",
    "        self.avgpool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(4096, 2048)\n",
    "        self.square_activation_ft = SquareActivation()\n",
    "        self.fc2 = nn.Linear(2048, S * S * (B * 5 + num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.square_activation1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.avgpool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.square_activation2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.avgpool2(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.square_activation_ft(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Assuming the SquareActivation is defined somewhere else\n",
    "class SquareActivation(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * x\n",
    "\n",
    "model = TinyYOLOv1()\n",
    "summary(model, input_size=(1, 1, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearReluActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, t):\n",
    "        return t + torch.pow(t, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyYOLOv1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TinyYOLOv1, self).__init__()\n",
    "        S = 7 # grid size\n",
    "        B = 2 # number of bounding boxes\n",
    "        C = 1 # number of classes\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            LinearReluActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            LinearReluActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Block 3\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            LinearReluActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Block 4\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            LinearReluActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Block 5\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            LinearReluActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Block 6\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            LinearReluActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Block 7\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            LinearReluActivation(),\n",
    "            \n",
    "            # Block 8\n",
    "            nn.Conv2d(1024, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            LinearReluActivation(),\n",
    "            \n",
    "            # Flatten\n",
    "            nn.Flatten(),\n",
    "            \n",
    "            # Fully Connected Layers\n",
    "            nn.Linear(256 * 4 * 4, S * S * (B * 5 + C)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TinyYOLOv1                               [1, 539]                  --\n",
       "├─Sequential: 1-1                        [1, 539]                  --\n",
       "│    └─Conv2d: 2-1                       [1, 16, 256, 256]         160\n",
       "│    └─BatchNorm2d: 2-2                  [1, 16, 256, 256]         32\n",
       "│    └─LinearReluActivation: 2-3         [1, 16, 256, 256]         --\n",
       "│    └─AvgPool2d: 2-4                    [1, 16, 128, 128]         --\n",
       "│    └─Conv2d: 2-5                       [1, 32, 128, 128]         4,640\n",
       "│    └─BatchNorm2d: 2-6                  [1, 32, 128, 128]         64\n",
       "│    └─LinearReluActivation: 2-7         [1, 32, 128, 128]         --\n",
       "│    └─AvgPool2d: 2-8                    [1, 32, 64, 64]           --\n",
       "│    └─Conv2d: 2-9                       [1, 64, 64, 64]           18,496\n",
       "│    └─BatchNorm2d: 2-10                 [1, 64, 64, 64]           128\n",
       "│    └─LinearReluActivation: 2-11        [1, 64, 64, 64]           --\n",
       "│    └─AvgPool2d: 2-12                   [1, 64, 32, 32]           --\n",
       "│    └─Conv2d: 2-13                      [1, 128, 32, 32]          73,856\n",
       "│    └─BatchNorm2d: 2-14                 [1, 128, 32, 32]          256\n",
       "│    └─LinearReluActivation: 2-15        [1, 128, 32, 32]          --\n",
       "│    └─AvgPool2d: 2-16                   [1, 128, 16, 16]          --\n",
       "│    └─Conv2d: 2-17                      [1, 256, 16, 16]          295,168\n",
       "│    └─BatchNorm2d: 2-18                 [1, 256, 16, 16]          512\n",
       "│    └─LinearReluActivation: 2-19        [1, 256, 16, 16]          --\n",
       "│    └─AvgPool2d: 2-20                   [1, 256, 8, 8]            --\n",
       "│    └─Conv2d: 2-21                      [1, 512, 8, 8]            1,180,160\n",
       "│    └─BatchNorm2d: 2-22                 [1, 512, 8, 8]            1,024\n",
       "│    └─LinearReluActivation: 2-23        [1, 512, 8, 8]            --\n",
       "│    └─AvgPool2d: 2-24                   [1, 512, 4, 4]            --\n",
       "│    └─Conv2d: 2-25                      [1, 1024, 4, 4]           4,719,616\n",
       "│    └─BatchNorm2d: 2-26                 [1, 1024, 4, 4]           2,048\n",
       "│    └─LinearReluActivation: 2-27        [1, 1024, 4, 4]           --\n",
       "│    └─Conv2d: 2-28                      [1, 256, 4, 4]            2,359,552\n",
       "│    └─BatchNorm2d: 2-29                 [1, 256, 4, 4]            512\n",
       "│    └─LinearReluActivation: 2-30        [1, 256, 4, 4]            --\n",
       "│    └─Flatten: 2-31                     [1, 4096]                 --\n",
       "│    └─Linear: 2-32                      [1, 539]                  2,208,283\n",
       "==========================================================================================\n",
       "Total params: 10,864,507\n",
       "Trainable params: 10,864,507\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 504.47\n",
       "==========================================================================================\n",
       "Input size (MB): 0.26\n",
       "Forward/backward pass size (MB): 33.36\n",
       "Params size (MB): 43.46\n",
       "Estimated Total Size (MB): 77.08\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TinyYOLOv1()\n",
    "\n",
    "summary(model, input_size=(1, 1, 256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection over Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![IoU](https://b2633864.smushcdn.com/2633864/wp-content/uploads/2016/09/iou_equation.png?lossy=2&strip=1&webp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(boxes_preds, boxes_labels):\n",
    "    \"\"\"\n",
    "    Calculates intersection over union\n",
    "    \n",
    "    Parameters:\n",
    "        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)    \n",
    "    Returns:\n",
    "        tensor: Intersection over union for all examples\n",
    "    \"\"\"\n",
    "    # boxes_preds shape is (N, 4) where N is the number of bboxes\n",
    "    #boxes_labels shape is (n, 4)\n",
    "    \n",
    "    box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "    box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "    box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "    box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "    box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "    box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "    box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "    box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "    \n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "    #print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
    "    \n",
    "    #.clamp(0) is for the case when they don't intersect. Since when they don't intersect, one of these will be negative so that should become 0\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "    #print(f\"intersection: {intersection}\")\n",
    "\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "    #print(f\"box1_area: {box1_area}, box2_area: {box2_area}\")\n",
    "    \n",
    "    return intersection / (box1_area + box2_area - intersection + 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non Max Suppression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input**: A list of Proposal boxes B, corresponding confidence scores S and overlap threshold N.\n",
    "\n",
    "**Output**: A list of filtered proposals D.\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "1.  Select the proposal with highest confidence score, remove it from B and add it to the final proposal list D. (Initially D is empty).\n",
    "2.  Now compare this proposal with all the proposals — calculate the IOU (Intersection over Union) of this proposal with every other proposal. If the IOU is greater than the threshold N, remove that proposal from B.\n",
    "3.  Again take the proposal with the highest confidence from the remaining proposals in B and remove it from B and add it to D.\n",
    "4.  Once again calculate the IOU of this proposal with all the proposals in B and eliminate the boxes which have high IOU than threshold.\n",
    "5.  This process is repeated until there are no more proposals left in B.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(bboxes, iou_threshold, threshold):\n",
    "    \"\"\"\n",
    "    Does Non Max Suppression given bboxes\n",
    "    Parameters:\n",
    "        bboxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [class_pred, prob_score, x_center, y_center, width, height]\n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        threshold (float): threshold to remove predicted bboxes (independent of IoU) \n",
    "    Returns:\n",
    "        list: bboxes after performing NMS given a specific IoU threshold\n",
    "    \"\"\"\n",
    "\n",
    "    assert type(bboxes) == list\n",
    "\n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        chosen_box = bboxes.pop(0)\n",
    "\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            if box[0] != chosen_box[0]\n",
    "            or intersection_over_union(\n",
    "                torch.tensor(chosen_box[2:]),\n",
    "                torch.tensor(box[2:]),\n",
    "            )\n",
    "            < iou_threshold\n",
    "        ]\n",
    "\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "    #print(f\"bboxes_after_nms: {bboxes_after_nms}\")\n",
    "\n",
    "    return bboxes_after_nms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It describes a trade-off between precision and recall.\n",
    "\n",
    "**Precision**, also referred to as the positive predictive value, describes how well a model predicts the positive class. \n",
    "$$Precision=\\frac{TP}{TP+FP}$$\n",
    ">   Of all bounding box **predictions**, what fraction was actually correct?\n",
    "\n",
    "**Recall**, also called sensitivity tells you if your model made the right predictions when it should have. \n",
    "$$Recall=\\frac{TP}{TP+FN}$$\n",
    ">   Of all **target** bounding boxes, what fraction did we correctly detect?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(\n",
    "    pred_boxes, true_boxes, iou_threshold=0.5, num_classes=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates mean average precision \n",
    "    Parameters:\n",
    "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [train_idx, class_prediction, prob_score, x_center, y_center, width, height]\n",
    "        true_boxes (list): Similar as pred_boxes except all the correct ones \n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        num_classes (int): number of classes\n",
    "    Returns:\n",
    "        float: mAP value across all classes given a specific IoU threshold \n",
    "    \"\"\"\n",
    "\n",
    "    # list storing all AP for respective classes\n",
    "    average_precisions = []\n",
    "\n",
    "    # used for numerical stability later on\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # Go through all predictions and targets,\n",
    "        # and only add the ones that belong to the\n",
    "        # current class c\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "        #print(f\"{c} class has {len(detections)} detections\")\n",
    "\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "        #print(f\"{c} class has {len(ground_truths)} ground truths\")\n",
    "\n",
    "        # find the amount of bboxes for each training example\n",
    "        # Counter here finds how many ground truth bboxes we get\n",
    "        # for each training example, so let's say img 0 has 3,\n",
    "        # img 1 has 5 then we will obtain a dictionary with:\n",
    "        # amount_bboxes = {0:3, 1:5}\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "        #print(f\"{c} class has {len(amount_bboxes)} amount bboxes\")\n",
    "\n",
    "        # We then go through each key, val in this dictionary\n",
    "        # and convert to the following (w.r.t same example):\n",
    "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # sort by box probabilities which is index 2\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "        #print(f\"{c} class has {total_true_bboxes} total true bboxes\")\n",
    "        # If none exists for this class then we can safely skip\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # Only take out the ground_truths that have the same\n",
    "            # training idx as detection\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            #print(f\"{c} class has {num_gts} ground truths for detection {detection_idx}\")\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:])\n",
    "                )\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    # true positive and add this bounding box to seen\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    #These additional detections are considered false positives because they do not correspond to a new, unique object\n",
    "                    #they're essentially \"over-detecting\" an object that has already been correctly identified.\n",
    "                    FP[detection_idx] = 1\n",
    "\n",
    "            # if IOU is lower then the detection is a false positive\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        #[1, 1, 0, 1, 0] -> [1, 2, 2, 3, 3]\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        # torch.trapz for numerical integration\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image, boxes):\n",
    "    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n",
    "    im = np.array(image)\n",
    "    height, width, _ = im.shape\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(1)\n",
    "    # Display the image\n",
    "    ax.imshow(im)\n",
    "\n",
    "    # box[0] is x midpoint, box[2] is width\n",
    "    # box[1] is y midpoint, box[3] is height\n",
    "\n",
    "    # Create a Rectangle patch\n",
    "    for box in boxes:\n",
    "        class_label = int(box[0])\n",
    "        box = box[2:]\n",
    "        assert len(box) == 4, \"Got more values than in x, y, w, h, in a box!\"\n",
    "        upper_left_x = box[0] - box[2] / 2\n",
    "        upper_left_y = box[1] - box[3] / 2\n",
    "        rect = patches.Rectangle(\n",
    "            (upper_left_x * width, upper_left_y * height),\n",
    "            box[2] * width,\n",
    "            box[3] * height,\n",
    "            linewidth=1,\n",
    "            edgecolor=\"r\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add class label text\n",
    "        ax.text(upper_left_x * width, upper_left_y * height, str(class_label), color='r', fontsize=10, verticalalignment='bottom')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and convert boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes(\n",
    "    loader,\n",
    "    model,\n",
    "    iou_threshold,\n",
    "    threshold,\n",
    "    plot=False,\n",
    "):\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    # make sure model is in eval before get bboxes\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "\n",
    "    for batch_idx, (x, labels) in enumerate(loader):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        true_bboxes = cellboxes_to_boxes(labels)\n",
    "        bboxes = cellboxes_to_boxes(predictions)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            nms_boxes = non_max_suppression(\n",
    "                bboxes[idx],\n",
    "                iou_threshold=iou_threshold,\n",
    "                threshold=threshold,\n",
    "            )\n",
    "\n",
    "            # Activate only for test\n",
    "            if batch_idx == 0 and idx == 0 and plot == True:\n",
    "               plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n",
    "\n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx] + nms_box)\n",
    "\n",
    "            for box in true_bboxes[idx]:\n",
    "                # many will get converted to 0 pred\n",
    "                if box[1] > threshold:\n",
    "                    all_true_boxes.append([train_idx] + box)\n",
    "\n",
    "            train_idx += 1\n",
    "\n",
    "    model.train()\n",
    "    return all_pred_boxes, all_true_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cellboxes(predictions, S=7, C=1):\n",
    "    \"\"\"\n",
    "    Converts bounding boxes output from Yolo with\n",
    "    an image split size of S into entire image ratios\n",
    "    rather than relative to cell ratios.\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = predictions.shape[0]\n",
    "    predictions = predictions.reshape(batch_size, 7, 7, C + 10)\n",
    "    bboxes1 = predictions[..., C + 1:C + 5]\n",
    "    bboxes2 = predictions[..., C + 6:C + 10]\n",
    "    \n",
    "    scores = torch.cat(\n",
    "        (predictions[..., C].unsqueeze(0), predictions[..., C + 5].unsqueeze(0)), dim=0\n",
    "    )\n",
    "    best_box = scores.argmax(0).unsqueeze(-1)\n",
    "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
    "    # This results in a tensor with shape (batch_size, 7, 7, 1) where each element represents the index of a grid cell.\n",
    "    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n",
    "\n",
    "    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n",
    "    # Permute because is used here to swap these indices to match the (x, y) convention used in the best_boxes tensor.\n",
    "    # [0,1,2]->[0,0,0]\n",
    "    # [0,1,2]->[1,1,1]\n",
    "    # [0,1,2]->[2,2,2]\n",
    "    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
    "    w_y = 1 / S * best_boxes[..., 2:4]\n",
    "\n",
    "    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n",
    "    predicted_class = predictions[..., :C].argmax(-1).unsqueeze(-1)\n",
    "    best_confidence = torch.max(predictions[..., C], predictions[..., C + 5]).unsqueeze(\n",
    "        -1\n",
    "    )\n",
    "    converted_preds = torch.cat(\n",
    "        (predicted_class, best_confidence, converted_bboxes), dim=-1\n",
    "    )\n",
    "    #print(f\"converted_preds: {converted_preds}\")\n",
    "\n",
    "    return converted_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cellboxes_to_boxes(out, S=7):\n",
    "    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n",
    "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
    "    all_bboxes = []\n",
    "\n",
    "    for ex_idx in range(out.shape[0]):\n",
    "        bboxes = []\n",
    "\n",
    "        for bbox_idx in range(S * S):\n",
    "            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n",
    "        all_bboxes.append(bboxes)\n",
    "    #print(f\"all_bboxes: {all_bboxes}\")\n",
    "    return all_bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Loader of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DiorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, S=7, B=2, C=1, transform=None, train=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.train = train\n",
    "\n",
    "        # Determine the directory of the images and labels\n",
    "        if self.train:\n",
    "            self.img_dir = os.path.join(self.root_dir, 'images/train')\n",
    "            self.label_dir = os.path.join(self.root_dir, 'labels/train')\n",
    "        else:\n",
    "            self.img_dir = os.path.join(self.root_dir, 'images/test')\n",
    "            self.label_dir = os.path.join(self.root_dir, 'labels/test')\n",
    "\n",
    "        self.img_ids = os.listdir(self.img_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.img_ids[index].split('.')[0]\n",
    "        boxes = []\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, img_id + '.jpg')\n",
    "        image = Image.open(img_path)\n",
    "        image = image.convert(\"L\")\n",
    "\n",
    "        # Load labels\n",
    "        label_path = os.path.join(self.label_dir, img_id + '.txt')\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                class_label, x, y, width, height = map(float, line.strip().split())\n",
    "                boxes.append([class_label, x, y, width, height])\n",
    "\n",
    "        boxes = torch.tensor(boxes)\n",
    "        #print(f\"boxes: {boxes}\")\n",
    "        if self.transform:\n",
    "            image, boxes = self.transform(image, boxes)\n",
    "\n",
    "        # Convert To Cells\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
    "        for box in boxes:\n",
    "            class_label, x, y, width, height = box.tolist()\n",
    "            class_label = int(class_label)\n",
    "\n",
    "            i, j = int(self.S * y), int(self.S * x)\n",
    "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
    "\n",
    "            width_cell, height_cell = (\n",
    "                width * self.S,\n",
    "                height * self.S,\n",
    "            )\n",
    "\n",
    "            if label_matrix[i, j, self.C] == 0:\n",
    "                label_matrix[i, j, self.C] = 1\n",
    "\n",
    "                box_coordinates = torch.tensor(\n",
    "                    [x_cell, y_cell, width_cell, height_cell]\n",
    "                )\n",
    "\n",
    "                label_matrix[i, j, self.C+1:self.C+5] = box_coordinates\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "    \n",
    "        #print(f\"label_matrix shape: {label_matrix.shape}\")\n",
    "\n",
    "        return image, label_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From original paper: \n",
    ">   YOLO predicts multiple bounding boxes per grid cell. At training time we only want one bounding box predictor to be responsible for each object. We assign one predictor to be “responsible” for predicting an object based on which prediction has the highest current IOU with the ground truth. This leads to specialization between the bounding box predictors.\n",
    "Each predictor gets better at predicting certain sizes, aspect ratios, or classes of object, improving overall recall. \n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\lambda_{\\text {coord }} \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{i j}^{\\text {obj }}\\left[\\left(x_i-\\hat{x}_i\\right)^2+\\left(y_i-\\hat{y}_i\\right)^2\\right] \\\\\n",
    "+\\lambda_{\\text {coord }} \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{i j}^{\\text {obj }}\\left[\\left(\\sqrt{w_i}-\\sqrt{\\hat{w}_i}\\right)^2+\\left(\\sqrt{h_i}-\\sqrt{\\hat{h}_i}\\right)^2\\right] \\\\\n",
    "+\\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{i j}^{\\text {obj }}\\left(C_i-\\hat{C}_i\\right)^2 \\\\\n",
    "+\\lambda_{\\text {noobj }} \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{i j}^{\\text {noobj }}\\left(C_i-\\hat{C}_i\\right)^2 \\\\\n",
    "+\\sum_{i=0}^{S^2} \\mathbb{1}_i^{\\text {obj }} \\sum_{c \\in \\text { classes }}\\left(p_i(c)-\\hat{p}_i(c)\\right)^2\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "During training we optimize the following, multi-part where $ 1_{obj}^i $ denotes if object appears in cell **i** and $1_{obj}^{ij}$ denotes that the **j**  bounding box predictor in cell i is “responsible” for that prediction.\n",
    "\n",
    "In every image many grid cells do not contain any object. This pushes the “confidence” scores of those cells towards zero, often overpowering the gradient from cells that do contain objects. This can lead to model instability, as the model may prioritize learning to predict empty cells rather than focusing on correctly detecting objects in cells containing them, causing training to diverge early on. To remedy this, we increase the loss from bounding box coordinate predictions and decrease the loss from confidence predictions for boxes that don’t contain objects. We use two parameters, $\\lambda_{coord}$ and $\\lambda_{noobj}$  to accomplish this.\n",
    "\n",
    "Note that the loss function only penalizes classification error if an object is present in that grid cell (hence the conditional class probability discussed earlier). It also only penalizes bounding box coordinate error if that predictor is “responsible” for the ground truth box (i.e. has the highest\n",
    "IOU of any predictor in that grid cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the loss for yolo (v1) model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S=7, B=2, C=1):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        \"\"\"\n",
    "        S is split size of image (in paper 7),\n",
    "        B is number of boxes (in paper 2),\n",
    "        C is number of classes (in paper 20, in dataset 3),\n",
    "        \"\"\"\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        # These are from Yolo paper, signifying how much we should\n",
    "        # pay loss for no object (noobj) and the box coordinates (coord)\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        # predictions are shaped (BATCH_SIZE, S*S(C+B*5) when inputted\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
    "\n",
    "        # Calculate IoU for the two predicted bounding boxes with target bbox\n",
    "        iou_b1 = intersection_over_union(predictions[..., self.C + 1:self.C + 5], target[..., self.C + 1:self.C + 5])\n",
    "        iou_b2 = intersection_over_union(predictions[..., self.C + 6:self.C + 10], target[..., self.C + 1:self.C + 5])\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "\n",
    "        # Take the box with highest IoU out of the two prediction\n",
    "        # Note that bestbox will be indices of 0, 1 for which bbox was best\n",
    "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
    "        exists_box = target[..., self.C].unsqueeze(3)  # in paper this is Iobj_i\n",
    "\n",
    "        # ======================== #\n",
    "        #   FOR BOX COORDINATES    #\n",
    "        # ======================== #\n",
    "\n",
    "        # Set boxes with no object in them to 0. We only take out one of the two \n",
    "        # predictions, which is the one with highest Iou calculated previously.\n",
    "        box_predictions = exists_box * (\n",
    "            (\n",
    "                bestbox * predictions[..., self.C + 6:self.C + 10]\n",
    "                + (1 - bestbox) * predictions[..., self.C + 1:self.C + 5]\n",
    "            )\n",
    "        )\n",
    "        #print(f\"box_predictions: {box_predictions.shape}\")\n",
    "        box_targets = exists_box * target[..., self.C + 1:self.C + 5]\n",
    "\n",
    "        # Take sqrt of width, height of boxes\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
    "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
    "        )\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions, end_dim=-2),\n",
    "            torch.flatten(box_targets, end_dim=-2),\n",
    "        )\n",
    "\n",
    "        # ==================== #\n",
    "        #   FOR OBJECT LOSS    #\n",
    "        # ==================== #\n",
    "\n",
    "        # pred_box is the confidence score for the bbox with highest IoU\n",
    "        pred_box = (\n",
    "            bestbox * predictions[..., self.C + 5:self.C + 6] + (1 - bestbox) * predictions[..., self.C:self.C + 1]\n",
    "        )\n",
    "\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_box * pred_box),\n",
    "            torch.flatten(exists_box * target[..., self.C:self.C + 1]),\n",
    "        )\n",
    "\n",
    "        # ======================= #\n",
    "        #   FOR NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "\n",
    "        #max_no_obj = torch.max(predictions[..., 20:21], predictions[..., 25:26])\n",
    "        #no_object_loss = self.mse(\n",
    "        #    torch.flatten((1 - exists_box) * max_no_obj, start_dim=1),\n",
    "        #    torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
    "        #)\n",
    "\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., self.C:self.C + 1], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1),\n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., self.C + 5:self.C + 6], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1)\n",
    "        )\n",
    "\n",
    "        # ================== #\n",
    "        #   FOR CLASS LOSS   #\n",
    "        # ================== #\n",
    "\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[..., :self.C], end_dim=-2,),\n",
    "            torch.flatten(exists_box * target[..., :self.C], end_dim=-2,),\n",
    "        )\n",
    "\n",
    "        loss = (\n",
    "            self.lambda_coord * box_loss  # first two rows in paper\n",
    "            + object_loss  # third row in paper\n",
    "            + self.lambda_noobj * no_object_loss  # forth row\n",
    "            + class_loss  # fifth row\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = \"cpu\"\n",
    "BATCH_SIZE = 64 # 64 in original paper but resource exhausted error otherwise.\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 100\n",
    "LOAD_MODEL = False\n",
    "LOAD_MODEL_FILE = \"model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, optimizer, loss_fn):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    mean_loss = []\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "        mean_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_postfix(loss = loss.item())\n",
    "        \n",
    "    print(f\"Mean loss was {sum(mean_loss) / len(mean_loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, bboxes):\n",
    "        for t in self.transforms:\n",
    "            img, bboxes = t(img), bboxes\n",
    "\n",
    "        return img, bboxes\n",
    "\n",
    "\n",
    "transform = Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/buono/ObjDct_Repo/venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "files_dir = '/home/buono/ObjDct_Repo/data/ShipDataset'\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.1, patience=3, mode='max', verbose=True)\n",
    "loss_fn = YoloLoss()\n",
    "\n",
    "\n",
    "train_dataset = DiorDataset(\n",
    "    root_dir=files_dir,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e509407b3d64657ac45278eb979cb60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/497 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 100296.69213523327\n",
      "Train mAP: 0.0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    train_fn(train_loader, model, optimizer, loss_fn)\n",
    "    \n",
    "    pred_boxes, target_boxes = get_bboxes(\n",
    "        train_loader, model, iou_threshold=0.5, threshold=0.4\n",
    "    )\n",
    "    mean_avg_prec = mean_average_precision(\n",
    "        pred_boxes, target_boxes, iou_threshold=0.5\n",
    "    )\n",
    "    print(f\"Train mAP: {mean_avg_prec}\")    \n",
    "    \n",
    "    scheduler.step(mean_avg_prec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/home/buono/ObjDct_Repo/models/trained_models/TinyYOLO.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dir='/home/buono/ObjDct_Repo/data/ShipDataset'\n",
    "    \n",
    "test_dataset = DiorDataset( \n",
    "    root_dir=files_dir,\n",
    "    transform=transform,\n",
    "    train=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyYOLOv1(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LinearReluActivation()\n",
       "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): LinearReluActivation()\n",
       "    (7): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LinearReluActivation()\n",
       "    (11): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (12): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): LinearReluActivation()\n",
       "    (15): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (16): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): LinearReluActivation()\n",
       "    (19): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (20): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): LinearReluActivation()\n",
       "    (23): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (24): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): LinearReluActivation()\n",
       "    (27): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): LinearReluActivation()\n",
       "    (30): Flatten(start_dim=1, end_dim=-1)\n",
       "    (31): Linear(in_features=4096, out_features=539, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "checkpoint = torch.load(\"/home/buono/ObjDct_Repo/models/trained_models/TinyYOLO.pth\")\n",
    "# Load the state dictionary from the .pth file\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test mAP: 0.0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_boxes, target_boxes = get_bboxes(\n",
    "        test_loader, model, iou_threshold=0.5, threshold=0.4, plot=True\n",
    "    )\n",
    "\n",
    "    mean_avg_prec = mean_average_precision(\n",
    "        pred_boxes, target_boxes, iou_threshold=0.5\n",
    "    )\n",
    "    print(f\"Test mAP: {mean_avg_prec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = torch.rand(1, 3, 64, 64)\n",
    "# model.eval()\n",
    "# output = model(sample)\n",
    "# print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Set the path to save the ONNX model\n",
    "# onnx_model_path = \"model.onnx\"\n",
    "\n",
    "# # Export the model to ONNX format\n",
    "# torch.onnx.export(model, sample, onnx_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PYHELAYERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a subset of plain samples from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_img_list=[]\n",
    "# test_label_list=[]\n",
    "# for image,label in test_dataset:\n",
    "#     test_img_list.append(image)\n",
    "#     test_label_list.append(label)\n",
    "#     if len(test_img_list)==16:\n",
    "#         break\n",
    "\n",
    "# test_img_array = np.array(test_img_list)\n",
    "# test_label_array = np.array(test_label_list)\n",
    "# # test_img_array = test_img_array[:11728]\n",
    "# # test_label_array = test_label_array[:11728]\n",
    "# # test_img_array = test_img_array.reshape(733,16,3,64,64)\n",
    "# # test_label_array = test_label_array.reshape(733,16,7,7,30)\n",
    "# print(test_img_array.shape)\n",
    "# print(test_label_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize he scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyhelayers\n",
    "# import utils\n",
    "\n",
    "# utils.verify_memory()\n",
    "\n",
    "# print('Misc. initalizations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = pyhelayers.DefaultContext()\n",
    "# print('HE context ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# nnp = pyhelayers.NeuralNetPlain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper_params = pyhelayers.PlainModelHyperParams()\n",
    "# nnp.init_from_files(hyper_params, [\"model.onnx\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# he_run_req = pyhelayers.HeRunRequirements()\n",
    "# he_run_req.set_he_context_options([pyhelayers.DefaultContext()])\n",
    "# he_run_req.optimize_for_batch_size(16)\n",
    "\n",
    "# profile = pyhelayers.HeModel.compile(nnp, he_run_req)\n",
    "# batch_size = profile.get_optimal_batch_size()\n",
    "# print('Profile ready. Batch size=',batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile.get_he_config_requirement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = pyhelayers.HeModel.create_context(profile)\n",
    "# print('HE context initalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context.get_scheme_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context.get_default_scale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context.get_library_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context.has_secret_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context.save_secret_key_to_file('secret_key.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn = pyhelayers.NeuralNet(context)\n",
    "# nn.encode_encrypt(nnp, profile)\n",
    "# print('Encrypted network ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plain_samples, labels = utils.extract_batch(test_img_array, test_label_array, batch_size, 0)\n",
    "\n",
    "# print('Batch of size',batch_size,'loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(plain_samples.shape)\n",
    "# print(plain_samples.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iop = nn.create_io_processor()\n",
    "# samples = pyhelayers.EncryptedData(context)\n",
    "# iop.encode_encrypt_inputs_for_predict(samples, [plain_samples])\n",
    "# print('Test data encrypted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make prediction on encrypted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.start_timer()\n",
    "\n",
    "# predictions = pyhelayers.EncryptedData(context)\n",
    "# nn.predict(predictions, samples)\n",
    "\n",
    "# duration=utils.end_timer('predict')\n",
    "# utils.report_duration('predict per sample',duration/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plain_predictions_aHE = iop.decrypt_decode_output(predictions)\n",
    "# print(f\"plain prediction shape after HE: {plain_predictions_aHE.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(plain_predictions_aHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## In case I have test samples > batch size\n",
    "# plain_predictions_aHE = plain_predictions_aHE.reshape(,1470)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate prediction with HE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plain_predictions_bHE=[]\n",
    "# for sample in test_img_array:\n",
    "#     tensor_sample = torch.tensor(sample).unsqueeze(0)\n",
    "#     output = model(tensor_sample)\n",
    "#     output = output.detach().numpy()\n",
    "#     plain_predictions_bHE.append(output)\n",
    "# plain_predictions_bHE = np.array(plain_predictions_bHE)\n",
    "# plain_predictions_bHE = plain_predictions_bHE.reshape(16, 1470)\n",
    "# print(f\"plain prediction shape before HE: {plain_predictions_bHE.shape}\")\n",
    "# plain_predictions_bHE=[]\n",
    "# for sample in test_img_array:\n",
    "#     tensor_sample = torch.tensor(sample).unsqueeze(0)\n",
    "#     output = model(tensor_sample)\n",
    "#     output = output.detach().numpy()\n",
    "#     plain_predictions_bHE.append(output)\n",
    "# plain_predictions_bHE = np.array(plain_predictions_bHE)\n",
    "# plain_predictions_bHE = plain_predictions_bHE.reshape(16, 1470)\n",
    "# print(f\"plain prediction shape before HE: {plain_predictions_bHE.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Compute the absolute differences between plain prediction before and after HE\n",
    "# differences = np.abs(plain_predictions_bHE - plain_predictions_aHE)\n",
    "\n",
    "# # Compute relevant statistics\n",
    "# mean_difference = np.mean(differences)\n",
    "# max_difference = np.max(differences)\n",
    "# min_difference = np.min(differences)\n",
    "# std_difference = np.std(differences)\n",
    "\n",
    "# print(f\"Mean difference: {mean_difference}\")\n",
    "# print(f\"Max difference: {max_difference}\")\n",
    "# print(f\"Min difference: {min_difference}\")\n",
    "# print(f\"Std difference: {std_difference}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert prediction in bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_bboxes_from_prediction(\n",
    "#     predictions,\n",
    "#     test_image_array,\n",
    "#     iou_threshold,\n",
    "#     threshold,\n",
    "# ):\n",
    "#     all_pred_boxes = []\n",
    "\n",
    "    \n",
    "#     train_idx = 0\n",
    "\n",
    "#     bboxes = cellboxes_to_boxes(predictions)\n",
    "\n",
    "#     for idx in range(len(test_image_array)):\n",
    "#         image = test_image_array[idx]\n",
    "\n",
    "#         nms_boxes = non_max_suppression(\n",
    "#             bboxes[idx],\n",
    "#             iou_threshold=iou_threshold,\n",
    "#             threshold=threshold,\n",
    "#         )\n",
    "\n",
    "#         # Activate only for test\n",
    "#         if  idx == 0:\n",
    "#             plot_image(image.permute(1,2,0).to(\"cpu\"), nms_boxes)\n",
    "\n",
    "#         for nms_box in nms_boxes:\n",
    "#             all_pred_boxes.append([train_idx] + nms_box)\n",
    "\n",
    "        \n",
    "\n",
    "#         train_idx += 1\n",
    "\n",
    "#     return all_pred_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_predaHE = torch.tensor(plain_predictions_aHE)\n",
    "# tensor_predbHE = torch.tensor(plain_predictions_bHE)\n",
    "# tensor_imgs = torch.tensor(test_img_array)\n",
    "\n",
    "# print(\"Prediction before HE\")\n",
    "# bboxes_aHE = get_bboxes_from_prediction(tensor_predbHE, tensor_imgs, iou_threshold=0.5, threshold=0.4)\n",
    "\n",
    "# print(\"Prediction after HE\")\n",
    "# bboxes_bHE = get_bboxes_from_prediction(tensor_predaHE, tensor_imgs, iou_threshold=0.5, threshold=0.4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
