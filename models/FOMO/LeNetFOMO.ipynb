{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buono/ObjDct_Repo/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "# import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.onnx\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE:  cpu\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE: \", DEVICE)\n",
    "BATCH_SIZE = 32 # 64 in original paper but resource exhausted error otherwise.\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance between centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(center_preds, center_labels):\n",
    "    \"\"\"\n",
    "    Calculate euclidean distance\n",
    "    Parameters:\n",
    "        center_preds: predictions of centers (BATCH_SIZE, 2)\n",
    "        center_labels: target of centers of shape (BATCH_SIZE, 2)\n",
    "    Returns:\n",
    "        distance: euclidean distance for all examples\n",
    "    \"\"\"\n",
    "\n",
    "    x1 = center_preds[..., 0:1]\n",
    "    y1 = center_preds[..., 1:2]\n",
    "    x2 = center_labels[..., 0:1]\n",
    "    y2 = center_labels[..., 1:2]\n",
    "\n",
    "    distance = torch.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_center_inside_bbox(center, bbox):\n",
    "    \"\"\"\n",
    "    Check if a center (x, y) is inside a bounding box (x, y, w, h).\n",
    "    Parameters:\n",
    "        center (tuple): The (x, y) coordinates of the center.\n",
    "        bbox (tuple): The (x, y, w, h) coordinates of the bounding box.\n",
    "    Returns:\n",
    "        bool: True if the center is inside the bounding box, False otherwise.\n",
    "    \"\"\"\n",
    "    center_x, center_y = center\n",
    "    bbox_x, bbox_y, bbox_w, bbox_h = bbox\n",
    "\n",
    "    bbox_x_min = bbox_x - bbox_w / 2\n",
    "    bbox_x_max = bbox_x + bbox_w / 2\n",
    "    bbox_y_min = bbox_y - bbox_h / 2\n",
    "    bbox_y_max = bbox_y + bbox_h / 2\n",
    "\n",
    "    return bbox_x_min <= center_x <= bbox_x_max and bbox_y_min <= center_y <= bbox_y_max\n",
    "\n",
    "\n",
    "def mean_average_precision(\n",
    "    pred_boxes, true_boxes, num_classes=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates mean average precision\n",
    "    Parameters:\n",
    "        pred_boxes (list): list of lists containing all bboxes with each bbox\n",
    "        specified as [train_idx, class_prediction, prob_score, x_center, y_center]\n",
    "        true_boxes (list): Similar as pred_boxes except all the correct ones\n",
    "        specified as [train_idx, class_label, x, y, w, h]\n",
    "        num_classes (int): number of classes\n",
    "    Returns:\n",
    "        float: mAP value across all classes\n",
    "    \"\"\"\n",
    "    average_precisions = []\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            best_match = False\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                if is_center_inside_bbox(detection[3:5], gt[2:]):\n",
    "                    best_match = True\n",
    "                    best_gt_idx = idx\n",
    "                    break\n",
    "\n",
    "            if best_match:\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(centers):\n",
    "\n",
    "    assert type(centers) == list\n",
    "    # centers = [center for center in centers if center[1]> threshold]\n",
    "    centers = sorted(centers, key=lambda x: x[1], reverse=True)\n",
    "    centers_after_nms = []\n",
    "\n",
    "    while centers:\n",
    "        current_center = centers.pop(0)\n",
    "        centers_after_nms.append(current_center)\n",
    "        break\n",
    "\n",
    "    return centers_after_nms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and convert centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes(\n",
    "    loader,\n",
    "    model,\n",
    "    device,\n",
    "):\n",
    "    all_pred_centers = []\n",
    "    all_true_centers = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    # make sure model is in eval before get bboxes\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "\n",
    "    for batch_idx, (x, labels, boxes_list) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(x)\n",
    "\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        true_centers = cellcenters_to_centers(labels) # type: ignore\n",
    "        pred_centers = cellcenters_to_centers(predictions) # type: ignore\n",
    "\n",
    "\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "\n",
    "            boxes = boxes_list[idx].to(device)\n",
    "\n",
    "            nms_centers= non_max_suppression(\n",
    "                pred_centers[idx]\n",
    "            )\n",
    "\n",
    "            for center in nms_centers:\n",
    "                all_pred_centers.append([train_idx] + center)\n",
    "\n",
    "            # for center in pred_centers[idx]:\n",
    "            #     if center[1] > 0:\n",
    "            #         all_pred_centers.append([train_idx] + center)\n",
    "\n",
    "            for center in true_centers[idx]:\n",
    "                if center[1] > 0:\n",
    "                    all_true_centers.append([train_idx] + center)\n",
    "\n",
    "            for box in boxes:\n",
    "                all_true_boxes.append([train_idx] + box.tolist())\n",
    "\n",
    "            train_idx += 1\n",
    "\n",
    "    model.train()\n",
    "    return all_pred_centers, all_true_centers, all_true_boxes\n",
    "\n",
    "\n",
    "def convert_cellcenters(predictions, S=4, C=1):\n",
    "    \"\"\"\n",
    "    Converts predictions from the model to centers\n",
    "    \"\"\"\n",
    "    predictions = predictions.to(\"cpu\")\n",
    "    batch_size = predictions.shape[0]\n",
    "    predictions = predictions.reshape(batch_size, S, S, C + 6)\n",
    "\n",
    "    centers1 = predictions[..., C + 1:C + 3]\n",
    "    centers2 = predictions[..., C + 4:C + 6]\n",
    "\n",
    "    scores = torch.cat(\n",
    "        (predictions[..., C].unsqueeze(0), predictions[..., C + 3].unsqueeze(0)), dim=0\n",
    "    )\n",
    "    best_center = scores.argmax(0).unsqueeze(-1)\n",
    "\n",
    "    best_centers = centers1 * (1 - best_center) + best_center * centers2\n",
    "\n",
    "    # This results in a tensor with shape (batch_size, 7, 7, 1) where each element represents the index of a grid cell.\n",
    "    cell_indices = torch.arange(S).repeat(batch_size, S, 1).unsqueeze(-1)\n",
    "    x = 1 / S * (best_centers[..., :1] + cell_indices)\n",
    "    # Permute because is used here to swap these indices to match the (x, y) convention used in the best_boxes tensor.\n",
    "    # [0,1,2]->[0,0,0]\n",
    "    # [0,1,2]->[1,1,1]\n",
    "    # [0,1,2]->[2,2,2]\n",
    "    y = 1 / S * (best_centers[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
    "    converted_centers = torch.cat((x, y), dim=-1)\n",
    "    predicted_class = predictions[..., :C].argmax(-1).unsqueeze(-1)\n",
    "    best_confidence = torch.max(predictions[..., C], predictions[..., C + 3]).unsqueeze(\n",
    "        -1\n",
    "    )\n",
    "\n",
    "    converted_preds = torch.cat(\n",
    "        (predicted_class, best_confidence, converted_centers), dim=-1\n",
    "    )\n",
    "\n",
    "    return converted_preds\n",
    "\n",
    "def cellcenters_to_centers(out, S=4):\n",
    "    converted_pred = convert_cellcenters(out).reshape(out.shape[0], S * S, -1)\n",
    "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
    "    all_centers = []\n",
    "\n",
    "    for ex_idx in range(out.shape[0]):\n",
    "        centers = []\n",
    "        for center_idx in range(S * S):\n",
    "            centers.append([x.item() for x in converted_pred[ex_idx, center_idx, :]])\n",
    "        all_centers.append(centers)\n",
    "\n",
    "    return all_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the loss for yolo (v1) model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S=4, B=2, C=1):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        \"\"\"\n",
    "        S is split size of image (in paper 7),\n",
    "        B is number of boxes (in paper 2),\n",
    "        C is number of classes (in paper 20, in dataset 3),\n",
    "        \"\"\"\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        # These are from Yolo paper, signifying how much we should\n",
    "        # pay loss for no object (noobj) and the box coordinates (coord)\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        # predictions are shaped (BATCH_SIZE, S*S(C+B*3) when inputted\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 3)\n",
    "        # Calculate IoU for the two predicted bounding boxes with target bbox\n",
    "        iou_c1 = euclidean_distance(predictions[..., self.C + 1:self.C + 3], target[..., self.C + 1:self.C + 3])\n",
    "        iou_c2 = euclidean_distance(predictions[..., self.C + 4:self.C + 6], target[..., self.C + 1:self.C + 3])\n",
    "        ious = torch.cat([iou_c1.unsqueeze(0), iou_c2.unsqueeze(0)], dim=0)\n",
    "        # Take the box with highest IoU out of the two prediction\n",
    "        # Note that bestbox will be indices of 0, 1 for which bbox was best\n",
    "        iou_maxes, bestcenter = torch.max(ious, dim=0)\n",
    "        exists_center = target[..., self.C].unsqueeze(3)  # in paper this is Iobj_i\n",
    "\n",
    "        # ======================== #\n",
    "        #   FOR CENTER COORDINATES #\n",
    "        # ======================== #\n",
    "\n",
    "        # Set boxes with no object in them to 0. We only take out one of the two\n",
    "        # predictions, which is the one with highest Iou calculated previously.\n",
    "\n",
    "        center_predictions = exists_center * (\n",
    "            (\n",
    "                bestcenter * predictions[..., self.C + 4:self.C + 6]\n",
    "                + (1 - bestcenter) * predictions[..., self.C + 1:self.C + 3]\n",
    "            )\n",
    "        )\n",
    "        center_targets = exists_center * target[..., self.C + 1:self.C + 3]\n",
    "\n",
    "        center_loss = self.mse(\n",
    "            torch.flatten(center_predictions, end_dim=-2),\n",
    "            torch.flatten(center_targets, end_dim=-2),\n",
    "        )\n",
    "\n",
    "        # ==================== #\n",
    "        #   FOR OBJECT LOSS    #\n",
    "        # ==================== #\n",
    "\n",
    "        # pred_box is the confidence score for the bbox with highest IoU\n",
    "        pred_center = (\n",
    "            bestcenter * predictions[..., self.C + 3:self.C + 4] + (1 - bestcenter) * predictions[..., self.C:self.C + 1]\n",
    "        )\n",
    "\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_center * pred_center),\n",
    "            torch.flatten(exists_center * target[..., self.C:self.C + 1]),\n",
    "        )\n",
    "\n",
    "        # ======================= #\n",
    "        #   FOR NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_center) * predictions[..., self.C:self.C + 1], start_dim=1),\n",
    "            torch.flatten((1 - exists_center) * target[..., self.C:self.C + 1], start_dim=1),\n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_center) * predictions[..., self.C + 3:self.C + 4], start_dim=1),\n",
    "            torch.flatten((1 - exists_center) * target[..., self.C:self.C + 1], start_dim=1)\n",
    "        )\n",
    "\n",
    "        # ================== #\n",
    "        #   FOR CLASS LOSS   #\n",
    "        # ================== #\n",
    "\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_center * predictions[..., :self.C], end_dim=-2,),\n",
    "            torch.flatten(exists_center * target[..., :self.C], end_dim=-2,),\n",
    "        )\n",
    "\n",
    "        loss = (\n",
    "            self.lambda_coord * center_loss  # first two rows in paper\n",
    "            + object_loss  # third row in paper\n",
    "            + self.lambda_noobj * no_object_loss  # forth row\n",
    "            + class_loss  # fifth row\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TinyissimoYOLO                           [1, 112]                  --\n",
       "├─Sequential: 1-1                        [1, 4, 21, 21]            --\n",
       "│    └─Conv2d: 2-1                       [1, 4, 42, 42]            104\n",
       "│    └─LinearActivation: 2-2             [1, 4, 42, 42]            --\n",
       "│    └─AvgPool2d: 2-3                    [1, 4, 21, 21]            --\n",
       "├─Sequential: 1-2                        [1, 12, 4, 4]             --\n",
       "│    └─Conv2d: 2-4                       [1, 12, 9, 9]             1,212\n",
       "│    └─LinearActivation: 2-5             [1, 12, 9, 9]             --\n",
       "│    └─AvgPool2d: 2-6                    [1, 12, 4, 4]             --\n",
       "├─Sequential: 1-3                        [1, 112]                  --\n",
       "│    └─Flatten: 2-7                      [1, 192]                  --\n",
       "│    └─Linear: 2-8                       [1, 256]                  49,408\n",
       "│    └─LinearActivation: 2-9             [1, 256]                  --\n",
       "│    └─Linear: 2-10                      [1, 112]                  28,784\n",
       "==========================================================================================\n",
       "Total params: 79,508\n",
       "Trainable params: 79,508\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.36\n",
       "==========================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 0.07\n",
       "Params size (MB): 0.32\n",
       "Estimated Total Size (MB): 0.42\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, t):\n",
    "        return torch.pow(t, 1) + torch.pow(t,2)\n",
    "\n",
    "class TinyissimoYOLO(L.LightningModule):\n",
    "    def __init__(self, B=2, num_classes=1, S=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss_fn = YoloLoss()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=5, stride=2),\n",
    "            LinearActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(4, 12, kernel_size=5, stride=2),\n",
    "            LinearActivation(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.fclayers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(12*4*4, 256),\n",
    "            LinearActivation(),\n",
    "            nn.Linear(256, S*S*(num_classes + 3 * B)),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        # print(x.shape)\n",
    "        x = self.fclayers(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, boxes = batch\n",
    "        out = self(x)\n",
    "        loss = self.loss_fn(out, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, boxes = batch\n",
    "        out = self(x)\n",
    "        loss = self.loss_fn(out, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "\n",
    "model = TinyissimoYOLO()\n",
    "\n",
    "summary(model, input_size=(1, 1, 88, 88))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Create Loader of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, centers):\n",
    "        for t in self.transforms:\n",
    "            img, centers = t(img), centers\n",
    "\n",
    "        return img, centers\n",
    "\n",
    "\n",
    "transform = Compose([transforms.Resize((88, 88)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, S=4, B=2, C=1, transform=None, train=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.train = train\n",
    "\n",
    "        # Determine the directory of the images and labels\n",
    "        if self.train:\n",
    "            self.img_dir = os.path.join(self.root_dir, 'images/train')\n",
    "            self.label_dir = os.path.join(self.root_dir, 'labels/train')\n",
    "        else:\n",
    "            self.img_dir = os.path.join(self.root_dir, 'images/test')\n",
    "            self.label_dir = os.path.join(self.root_dir, 'labels/test')\n",
    "\n",
    "        self.img_ids = os.listdir(self.img_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.img_ids[index].split('.')[0]\n",
    "        centers = []\n",
    "        boxes = []\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, img_id + '.jpg')\n",
    "        image = Image.open(img_path)\n",
    "        image = image.convert(\"L\")\n",
    "\n",
    "        # Load labels\n",
    "        label_path = os.path.join(self.label_dir, img_id + '.txt')\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                class_label, x, y, width, height = map(float, line.strip().split())\n",
    "                centers.append([class_label, x, y])\n",
    "                boxes.append([class_label, x, y, width, height])\n",
    "\n",
    "        if len(boxes) > 3:\n",
    "            boxes = boxes[:3]\n",
    "            centers = centers[:3]\n",
    "\n",
    "        boxes = torch.tensor(boxes)\n",
    "        centers = torch.tensor(centers)\n",
    "        if self.transform:\n",
    "            image, centers = self.transform(image, centers)\n",
    "        # Convert To Cells\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.C + 3 * self.B))\n",
    "        for center in centers:\n",
    "            class_label, x, y = center\n",
    "            class_label = int(class_label)\n",
    "            i, j = int(self.S * y), int(self.S * x)\n",
    "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
    "\n",
    "            if label_matrix[i, j, self.C] == 0:\n",
    "                label_matrix[i, j, self.C] = 1\n",
    "\n",
    "                center_coordinates = torch.tensor(\n",
    "                    [x_cell, y_cell]\n",
    "                )\n",
    "\n",
    "                label_matrix[i, j, self.C + 1:self.C + 3] = center_coordinates\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "\n",
    "        #print(f\"label_matrix shape: {label_matrix.shape}\")\n",
    "\n",
    "        return image, label_matrix , boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    label_matrices = []\n",
    "    boxes_list = []\n",
    "    for item in batch:\n",
    "        images.append(item[0])\n",
    "        label_matrices.append(item[1])\n",
    "        boxes_list.append(item[2])\n",
    "    images = torch.stack(images)\n",
    "    label_matrices = torch.stack(label_matrices)\n",
    "    return images, label_matrices, boxes_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dir = '/home/buono/ObjDct_Repo/data/ShipDataset'\n",
    "loss_fn = YoloLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_dataset = DiorDataset(\n",
    "    root_dir=files_dir,\n",
    "    transform=transform,\n",
    "    train=True\n",
    ")\n",
    "\n",
    "\n",
    "# Define the length of the training set and validation set\n",
    "train_len = int(0.8 * len(train_dataset))\n",
    "val_len = len(train_dataset) - train_len\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, validation_dataset = random_split(train_dataset, [train_len, val_len])\n",
    "\n",
    "\n",
    "# Now you can create your DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DiorDataset(\n",
    "    root_dir=files_dir,\n",
    "    transform=transform,\n",
    "    train=False\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25426, 6357, 7946)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(validation_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyissimoYOLO()\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(save_top_k=1, \n",
    "                                      monitor=\"val_loss\", \n",
    "                                      mode=\"min\")\n",
    "lr_monitor_callback = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "trainer = L.Trainer(accelerator=DEVICE, \n",
    "                    callbacks=[checkpoint_callback, lr_monitor_callback], \n",
    "                    max_epochs=EPOCHS, \n",
    "                    enable_progress_bar=True, \n",
    "                    enable_model_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"lightning_logs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyissimoYOLO.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred_boxes, target_boxes, real_boxes = get_bboxes(\n",
    "        test_loader, model, DEVICE\n",
    "    )\n",
    "    mAP = mean_average_precision(pred_boxes, real_boxes)\n",
    "    print(f\"mAP: {mAP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../../models/trained_models/LeNetYOLO.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create other 2 models for ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = TinyissimoYOLO()\n",
    "trainer.fit(model=model_1, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.eval()\n",
    "with torch.no_grad():\n",
    "    pred_boxes, target_boxes, real_boxes = get_bboxes(\n",
    "        test_loader, model_1, DEVICE\n",
    "    )\n",
    "    mAP = mean_average_precision(pred_boxes, real_boxes)\n",
    "    print(f\"mAP: {mAP}\")\n",
    "torch.save(model_1.state_dict(), \"../../models/trained_models/LeNetYOLO_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = TinyissimoYOLO()\n",
    "trainer.fit(model=model_2, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.eval()\n",
    "with torch.no_grad():\n",
    "    pred_boxes, target_boxes, real_boxes = get_bboxes(\n",
    "        test_loader, model_2, DEVICE\n",
    "    )\n",
    "    mAP = mean_average_precision(pred_boxes, real_boxes)\n",
    "    print(f\"mAP: {mAP}\")\n",
    "torch.save(model_2.state_dict(), \"../../models/trained_models/LeNetYOLO_2.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "model = TinyissimoYOLO()\n",
    "checkpoint = torch.load(\"../../models/trained_models/LeNetYOLO.pth\")\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()\n",
    "models.append(model)\n",
    "\n",
    "model_1 = TinyissimoYOLO()\n",
    "checkpoint = torch.load(\"../../models/trained_models/LeNetYOLO_1.pth\")\n",
    "model_1.load_state_dict(checkpoint)\n",
    "model_1.eval()\n",
    "models.append(model_1)\n",
    "\n",
    "model_2 = TinyissimoYOLO()\n",
    "checkpoint = torch.load(\"../../models/trained_models/LeNetYOLO_2.pth\")\n",
    "model_2.load_state_dict(checkpoint)\n",
    "model_2.eval()\n",
    "models.append(model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create average predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP: 0.22950465977191925\n",
      "mAP: 0.25170090794563293\n",
      "mAP: 0.3062932789325714\n"
     ]
    }
   ],
   "source": [
    "pred_boxes_list = []\n",
    "with torch.no_grad():\n",
    "    pred_boxes_1, target_boxes, real_boxes_1 = get_bboxes(\n",
    "        test_loader, model, DEVICE\n",
    "    )\n",
    "    mAP = mean_average_precision(pred_boxes_1, real_boxes_1)\n",
    "    print(f\"mAP: {mAP}\")\n",
    "    pred_boxes_list.append(pred_boxes_1)\n",
    "\n",
    "    pred_boxes_2, target_boxes, real_boxes_2 = get_bboxes(\n",
    "        test_loader, model_1, DEVICE\n",
    "    )\n",
    "    mAP = mean_average_precision(pred_boxes_2, real_boxes_2)\n",
    "    print(f\"mAP: {mAP}\")\n",
    "    pred_boxes_list.append(pred_boxes_2)\n",
    "\n",
    "    pred_boxes_3, target_boxes, real_boxes_3 = get_bboxes(\n",
    "        test_loader, model_2, DEVICE\n",
    "    )\n",
    "    mAP = mean_average_precision(pred_boxes_3, real_boxes_3)\n",
    "    print(f\"mAP: {mAP}\")\n",
    "    pred_boxes_list.append(pred_boxes_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 7946, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_boxes_list = np.array(pred_boxes_list)\n",
    "pred_boxes_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7946, 5)\n"
     ]
    }
   ],
   "source": [
    "def ensemble_center_predictions_weighted(predictions):\n",
    "    \"\"\"\n",
    "    Ensemble predictions from multiple models by averaging center coordinates and confidence scores\n",
    "    with specific weights for each model.\n",
    "\n",
    "    Parameters:\n",
    "        predictions (numpy.ndarray): Array of shape (num_models, num_predictions, 5) containing\n",
    "                                     the predictions from each model. Each prediction is in the\n",
    "                                     format [predicted_class, best_confidence, x_center, y_center].\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Array of ensemble predictions with weights applied.\n",
    "    \"\"\"\n",
    "    # Define weights for each model\n",
    "    weights = np.array([0.325, 0.331, 0.343])\n",
    "    \n",
    "    def average_predictions_weighted(predictions):\n",
    "        # Apply weights to confidence scores and center coordinates\n",
    "        weighted_confidences = np.average(predictions[:, 1], axis=0, weights=weights)\n",
    "        weighted_centers = np.average(predictions[:, 2:], axis=0, weights=weights)\n",
    "        \n",
    "        # Assuming class_ids are the same for all models, take the class_id from the first model\n",
    "        class_id = predictions[0, 0]\n",
    "        \n",
    "        return np.array([class_id, weighted_confidences, *weighted_centers])\n",
    "    \n",
    "    all_predictions = []\n",
    "    \n",
    "    for i in range(predictions.shape[1]):  # Iterate over each prediction index\n",
    "        single_prediction = predictions[:, i, :]\n",
    "        ensembled_prediction = average_predictions_weighted(single_prediction)\n",
    "        all_predictions.append(ensembled_prediction)\n",
    "    \n",
    "    return np.array(all_predictions)\n",
    "\n",
    "# Example usage\n",
    "predictions = np.random.rand(3, 7946, 5)  # Example array with predictions\n",
    "ensembled_predictions = ensemble_center_predictions_weighted(predictions)\n",
    "print(ensembled_predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP: 0.2974923849105835\n"
     ]
    }
   ],
   "source": [
    "ensembled_predictions = ensemble_center_predictions_weighted(pred_boxes_list)\n",
    "mAP = mean_average_precision(ensembled_predictions, real_boxes_1)\n",
    "print(f\"mAP: {mAP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_boxes_1 = np.array(real_boxes_1)\n",
    "real_boxes_2 = np.array(real_boxes_2)\n",
    "real_boxes_3 = np.array(real_boxes_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  0.00000000e+00 -9.51171875e-01 -4.29687500e-02\n",
      "  -6.64062500e-02 -5.46875000e-02]\n",
      " [ 0.00000000e+00  0.00000000e+00 -5.68359375e-01 -1.99218750e-01\n",
      "  -8.98437500e-02 -5.46875000e-02]\n",
      " [ 0.00000000e+00  0.00000000e+00 -7.46093750e-01 -2.36328125e-01\n",
      "  -1.01562500e-01 -6.64062500e-02]\n",
      " ...\n",
      " [-7.94400000e+03  0.00000000e+00 -3.82812500e-01 -4.66796875e-01\n",
      "  -7.03125000e-02 -1.36718750e-01]\n",
      " [-7.94400000e+03  0.00000000e+00 -4.70703125e-01 -4.70703125e-01\n",
      "  -6.64062500e-02 -1.21093750e-01]\n",
      " [-7.94500000e+03  0.00000000e+00 -8.55468750e-01 -1.19140625e-01\n",
      "  -7.81250000e-02 -8.98437500e-02]]\n"
     ]
    }
   ],
   "source": [
    "# Check se ci sono differenze tra i valori delle bounding boxes reali (caso in cui qualche modello fa delle prediction su un modello e un altro no)\n",
    "print(real_boxes_1 - real_boxes_2- real_boxes_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot test results and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_centers(idx, pred_centers, target_centers):\n",
    "    values = test_dataset[idx]\n",
    "    image = values[0]\n",
    "    image = image.permute(1, 2, 0).to(\"cpu\")\n",
    "\n",
    "    image = np.array(image)\n",
    "    height, width, _ = image.shape\n",
    "    p_centers = []\n",
    "    t_centers = []\n",
    "    for center in pred_centers:\n",
    "        if center[0] == idx:\n",
    "            p_centers.append(center[1:])\n",
    "\n",
    "    for center in target_centers:\n",
    "        if center[0] == idx:\n",
    "            t_centers.append(center[1:])\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "    for center in p_centers:\n",
    "        rect = patches.Rectangle(\n",
    "            (center[2] * width , center[3] * height),\n",
    "            1,\n",
    "            1,\n",
    "            linewidth=5,\n",
    "            edgecolor=\"r\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "    # Create a Rectangle patch\n",
    "    for boxes in t_centers:\n",
    "        class_label = int(boxes[0])\n",
    "        box = boxes[1:]\n",
    "        assert len(box) == 4, \"Got more values than in x, y, w, h, in a box!\"\n",
    "        upper_left_x = box[0] - box[2] / 2\n",
    "        upper_left_y = box[1] - box[3] / 2\n",
    "        rect = patches.Rectangle(\n",
    "            (upper_left_x * width, upper_left_y * height),\n",
    "            box[2] * width,\n",
    "            box[3] * height,\n",
    "            linewidth=1,\n",
    "            edgecolor=\"r\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Add class label text\n",
    "        ax.text(upper_left_x * width, upper_left_y * height, str(class_label), color='r', fontsize=10, verticalalignment='bottom')\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_centers(4, ensembled_predictions, real_boxes_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_boxes = np.array(real_boxes_1)[:,2:]\n",
    "bounding_boxes\n",
    "\n",
    "areas = np.sqrt([width * height for (x_center, y_center, width, height) in bounding_boxes])\n",
    "\n",
    "# Raggruppamento delle bounding box per dimensione\n",
    "bins = [0, 0.1, 0.15, 0.2, 0.25]\n",
    "labels = ['Very Small', 'Small', 'Medium', 'Large', 'Very Large']\n",
    "box_sizes = np.digitize(areas, bins=bins, right=True)\n",
    "\n",
    "# # Creazione dell'istogramma delle dimensioni delle bounding box\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.hist(areas, bins=bins, edgecolor='black')\n",
    "# plt.title('Histogram of Bounding Box Sizes')\n",
    "# plt.xlabel('Bounding Box Area')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.xticks(ticks=bins, labels=labels, rotation=45)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo delle distanze minime\n",
    "distances = []\n",
    "\n",
    "#for (idx_p,_,_, x_p, y_p), (idx_b,_, x_center, y_center, width, height) in zip(pred_boxes, real_boxes):\n",
    "for idx_p, _ ,_ ,x_p, y_p in ensembled_predictions:\n",
    "  for idx_b,_, x_center, y_center, width, height in real_boxes_1:\n",
    "    if idx_p == idx_b:\n",
    "      x_left = x_center - width / 2\n",
    "      x_right = x_center + width / 2\n",
    "      y_top = y_center - height / 2\n",
    "      y_bottom = y_center + height / 2\n",
    "\n",
    "      dist_left = abs(x_p - x_left) if x_p < x_left else float('inf')\n",
    "      dist_right = abs(x_p - x_right) if x_p > x_right else float('inf')\n",
    "      dist_top = abs(y_p - y_top) if y_p < y_top else float('inf')\n",
    "      dist_bottom = abs(y_p - y_bottom) if y_p > y_bottom else float('inf')\n",
    "\n",
    "      # Se il punto è all'interno della bounding box, la distanza è zero\n",
    "      if x_left <= x_p <= x_right and y_top <= y_p <= y_bottom:\n",
    "          min_distance = 0\n",
    "      else:\n",
    "          min_distance = min(dist_left, dist_right, dist_top, dist_bottom)\n",
    "\n",
    "      distances.append(min_distance)\n",
    "\n",
    "# plt.hist(distances, edgecolor='black')\n",
    "# plt.xlabel('Distance')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Histogram of Distances Over Bounding Boxes')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo della distanza media per ciascun gruppo di dimensioni\n",
    "distance_means = []\n",
    "for i in range(len(bins) - 1):\n",
    "    group_distances = [dist for dist, size in zip(distances, box_sizes) if size == i + 1]\n",
    "    if group_distances:\n",
    "        distance_means.append(np.mean(group_distances))\n",
    "    else:\n",
    "        distance_means.append(0)\n",
    "\n",
    "labels = ['Very Small', 'Small', 'Medium', 'Large']\n",
    "# # Creazione del grafico delle distanze medie per categoria di dimensione\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.bar(labels, distance_means, color='orange', edgecolor='black')\n",
    "# plt.title('Average Distance by Bounding Box Size')\n",
    "# plt.xlabel('Bounding Box Size Category')\n",
    "# plt.ylabel('Average Distance')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo delle distanze minime\n",
    "distances_1 = []\n",
    "\n",
    "#for (idx_p,_,_, x_p, y_p), (idx_b,_, x_center, y_center, width, height) in zip(pred_boxes, real_boxes):\n",
    "for idx_p, _ ,_ ,x_p, y_p in pred_boxes_1:\n",
    "  for idx_b,_, x_center, y_center, width, height in real_boxes_1:\n",
    "    if idx_p == idx_b:\n",
    "      x_left = x_center - width / 2\n",
    "      x_right = x_center + width / 2\n",
    "      y_top = y_center - height / 2\n",
    "      y_bottom = y_center + height / 2\n",
    "\n",
    "      dist_left = abs(x_p - x_left) if x_p < x_left else float('inf')\n",
    "      dist_right = abs(x_p - x_right) if x_p > x_right else float('inf')\n",
    "      dist_top = abs(y_p - y_top) if y_p < y_top else float('inf')\n",
    "      dist_bottom = abs(y_p - y_bottom) if y_p > y_bottom else float('inf')\n",
    "\n",
    "      # Se il punto è all'interno della bounding box, la distanza è zero\n",
    "      if x_left <= x_p <= x_right and y_top <= y_p <= y_bottom:\n",
    "          min_distance = 0\n",
    "      else:\n",
    "          min_distance = min(dist_left, dist_right, dist_top, dist_bottom)\n",
    "\n",
    "      distances_1.append(min_distance)\n",
    "\n",
    "# plt.hist(distances_1, edgecolor='black')\n",
    "# plt.xlabel('Distance')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Histogram of Distances Over Bounding Boxes')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo delle distanze minime\n",
    "distances_2 = []\n",
    "\n",
    "#for (idx_p,_,_, x_p, y_p), (idx_b,_, x_center, y_center, width, height) in zip(pred_boxes, real_boxes):\n",
    "for idx_p, _ ,_ ,x_p, y_p in pred_boxes_2:\n",
    "  for idx_b,_, x_center, y_center, width, height in real_boxes_2:\n",
    "    if idx_p == idx_b:\n",
    "      x_left = x_center - width / 2\n",
    "      x_right = x_center + width / 2\n",
    "      y_top = y_center - height / 2\n",
    "      y_bottom = y_center + height / 2\n",
    "\n",
    "      dist_left = abs(x_p - x_left) if x_p < x_left else float('inf')\n",
    "      dist_right = abs(x_p - x_right) if x_p > x_right else float('inf')\n",
    "      dist_top = abs(y_p - y_top) if y_p < y_top else float('inf')\n",
    "      dist_bottom = abs(y_p - y_bottom) if y_p > y_bottom else float('inf')\n",
    "\n",
    "      # Se il punto è all'interno della bounding box, la distanza è zero\n",
    "      if x_left <= x_p <= x_right and y_top <= y_p <= y_bottom:\n",
    "          min_distance = 0\n",
    "      else:\n",
    "          min_distance = min(dist_left, dist_right, dist_top, dist_bottom)\n",
    "\n",
    "      distances_2.append(min_distance)\n",
    "\n",
    "# plt.hist(distances_2, edgecolor='black')\n",
    "# plt.xlabel('Distance')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Histogram of Distances Over Bounding Boxes')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo delle distanze minime\n",
    "distances_3 = []\n",
    "\n",
    "#for (idx_p,_,_, x_p, y_p), (idx_b,_, x_center, y_center, width, height) in zip(pred_boxes, real_boxes):\n",
    "for idx_p, _ ,_ ,x_p, y_p in pred_boxes_3:\n",
    "  for idx_b,_, x_center, y_center, width, height in real_boxes_3:\n",
    "    if idx_p == idx_b:\n",
    "      x_left = x_center - width / 2\n",
    "      x_right = x_center + width / 2\n",
    "      y_top = y_center - height / 2\n",
    "      y_bottom = y_center + height / 2\n",
    "\n",
    "      dist_left = abs(x_p - x_left) if x_p < x_left else float('inf')\n",
    "      dist_right = abs(x_p - x_right) if x_p > x_right else float('inf')\n",
    "      dist_top = abs(y_p - y_top) if y_p < y_top else float('inf')\n",
    "      dist_bottom = abs(y_p - y_bottom) if y_p > y_bottom else float('inf')\n",
    "\n",
    "      # Se il punto è all'interno della bounding box, la distanza è zero\n",
    "      if x_left <= x_p <= x_right and y_top <= y_p <= y_bottom:\n",
    "          min_distance = 0\n",
    "      else:\n",
    "          min_distance = min(dist_left, dist_right, dist_top, dist_bottom)\n",
    "\n",
    "      distances_3.append(min_distance)\n",
    "\n",
    "# plt.hist(distances_3, edgecolor='black')\n",
    "# plt.xlabel('Distance')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Histogram of Distances Over Bounding Boxes')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo della distanza media per ciascun gruppo di dimensioni\n",
    "distance_means = []\n",
    "for i in range(len(bins) - 1):\n",
    "    group_distances = [dist for dist, size in zip(distances, box_sizes) if size == i + 1]\n",
    "    if group_distances:\n",
    "        distance_means.append(np.mean(group_distances))\n",
    "    else:\n",
    "        distance_means.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo della distanza media per ciascun gruppo di dimensioni\n",
    "distance_means_1 = []\n",
    "for i in range(len(bins) - 1):\n",
    "    group_distances = [dist for dist, size in zip(distances_1, box_sizes) if size == i + 1]\n",
    "    if group_distances:\n",
    "        distance_means_1.append(np.mean(group_distances))\n",
    "    else:\n",
    "        distance_means_1.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo della distanza media per ciascun gruppo di dimensioni\n",
    "distance_means_2 = []\n",
    "for i in range(len(bins) - 1):\n",
    "    group_distances = [dist for dist, size in zip(distances_2, box_sizes) if size == i + 1]\n",
    "    if group_distances:\n",
    "        distance_means_2.append(np.mean(group_distances))\n",
    "    else:\n",
    "        distance_means_2.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo della distanza media per ciascun gruppo di dimensioni\n",
    "distance_means_3 = []\n",
    "for i in range(len(bins) - 1):\n",
    "    group_distances = [dist for dist, size in zip(distances_3, box_sizes) if size == i + 1]\n",
    "    if group_distances:\n",
    "        distance_means_3.append(np.mean(group_distances))\n",
    "    else:\n",
    "        distance_means_3.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creiamo una lista di etichette per le categorie di dimensioni\n",
    "labels = ['Very Small', 'Small', 'Medium', 'Large']\n",
    "\n",
    "# Configuriamo il numero di gruppi e il numero di barre per gruppo\n",
    "x = np.arange(len(labels))\n",
    "width = 0.1  # Larghezza delle barre\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Aggiungiamo le barre per ogni set di distance_means\n",
    "rects1 = ax.bar(x - 2 * width, distance_means_1, width, label='1st Model', color='red')\n",
    "rects2 = ax.bar(x - 1 * width, distance_means_2, width, label='2nd Model', color='blue')\n",
    "rects3 = ax.bar(x - 0 * width, distance_means_3, width, label='3rd Model', color='pink')\n",
    "rects4 = ax.bar(x + 1 * width, distance_means, width, label='Ensemble', color='black')\n",
    "\n",
    "# Aggiungiamo le etichette, il titolo e la legenda\n",
    "ax.set_xlabel('Bounding Box Size Category')\n",
    "ax.set_ylabel('Average Distance')\n",
    "ax.set_title('Average Distance by Bounding Box Size')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "# Aggiungiamo una disposizione più compatta\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model in ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.rand(1, 1, 88, 88).to(DEVICE)\n",
    "model.eval()\n",
    "output = model(sample)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.eval()\n",
    "output_1 = model_1(sample)\n",
    "print(output_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.eval()\n",
    "output_2 = model_2(sample)\n",
    "print(output_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to save the ONNX model\n",
    "onnx_model_path = \"../trained_models/LeNetYOLO.onnx\"\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(model, sample, onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to save the ONNX model\n",
    "onnx_model_path = \"../trained_models/LeNetYOLO_1.onnx\"\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(model_1, sample, onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"../trained_models/LeNetYOLO_2.onnx\"\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(model_2, sample, onnx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup pyhelayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhelayers\n",
    "import sys\n",
    "sys.path.append('../../modules')  # Adds the modules directory to sys.path\n",
    "\n",
    "import utilshe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7936, 1, 88, 88)\n",
      "(7936, 4, 4, 7)\n"
     ]
    }
   ],
   "source": [
    "# Preparazione dei dati\n",
    "test_img_list = []\n",
    "test_label_list = []\n",
    "for values in test_dataset:\n",
    "    image = values[0]\n",
    "    label = values[1]\n",
    "    test_img_list.append(image)\n",
    "    test_label_list.append(label)\n",
    "    if len(test_img_list) == 7936:\n",
    "        break\n",
    "\n",
    "test_img_array = np.array(test_img_list)\n",
    "test_label_array = np.array(test_label_list)\n",
    "\n",
    "print(test_img_array.shape)\n",
    "print(test_label_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misc. initializations\n"
     ]
    }
   ],
   "source": [
    "# Inizializzazioni varie\n",
    "utilshe.verify_memory()\n",
    "\n",
    "print('Misc. initializations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function which encrypt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt_model(model):\n",
    "    \"\"\"\n",
    "    Model is a string of type : ../trained_models/LeNetYOLO.onnx\n",
    "    \"\"\"\n",
    "    context = pyhelayers.DefaultContext()\n",
    "    # Modello 1\n",
    "    nnp = pyhelayers.NeuralNetPlain()\n",
    "    hyper_params = pyhelayers.PlainModelHyperParams()\n",
    "    nnp.init_from_files(hyper_params, [model])\n",
    "    he_run_req = pyhelayers.HeRunRequirements()\n",
    "    he_run_req.set_he_context_options([pyhelayers.DefaultContext()])\n",
    "    he_run_req.optimize_for_batch_size(32)\n",
    "    profile = pyhelayers.HeModel.compile(nnp, he_run_req)\n",
    "    batch_size = profile.get_optimal_batch_size()\n",
    "    profile.get_he_config_requirement()\n",
    "    context = pyhelayers.HeModel.create_context(profile)\n",
    "    profile = pyhelayers.HeModel.compile(nnp, he_run_req)\n",
    "    batch_size = profile.get_optimal_batch_size()\n",
    "    context.get_scheme_name()\n",
    "    nn = pyhelayers.NeuralNet(context)\n",
    "    nn.encode_encrypt(nnp, profile)\n",
    "    \n",
    "    tot_predictions = []\n",
    "\n",
    "    for i in range(0,int(test_img_array.shape[0]/batch_size) - 1):\n",
    "        print(f\"Batch {i}\")\n",
    "        plain_samples, labels = utilshe.extract_batch(test_img_array, test_label_array, batch_size, i)\n",
    "        iop = nn.create_io_processor()\n",
    "        samples = pyhelayers.EncryptedData(context)\n",
    "        iop.encode_encrypt_inputs_for_predict(samples, [plain_samples])\n",
    "        predictions = pyhelayers.EncryptedData(context)\n",
    "        nn.predict(predictions, samples)\n",
    "        plain_predictions = iop.decrypt_decode_output(predictions)\n",
    "        tot_predictions.append(plain_predictions)\n",
    "    \n",
    "    tot_predictions = np.array(tot_predictions)\n",
    "    tot_predictions = tot_predictions.reshape(-1, 5)\n",
    "\n",
    "    return tot_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "pred_model_1 = encrypt_model(\"../trained_models/LeNetYOLO.onnx\")\n",
    "pred_model_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model_2 = encrypt_model(\"../trained_models/LeNetYOLO_1.onnx\")\n",
    "pred_model_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model_3 = encrypt_model(\"../trained_models/LeNetYOLO_2.onnx\")\n",
    "pred_model_3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud Configuration Example with first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HE context 1 ready\n"
     ]
    }
   ],
   "source": [
    "context1 = pyhelayers.DefaultContext()\n",
    "print('HE context 1 ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile 1 ready. Batch size = 16\n"
     ]
    }
   ],
   "source": [
    "# Modello 1\n",
    "nnp1 = pyhelayers.NeuralNetPlain()\n",
    "hyper_params1 = pyhelayers.PlainModelHyperParams()\n",
    "nnp1.init_from_files(hyper_params1, [\"../trained_models/LeNetYOLO.onnx\"])\n",
    "he_run_req1 = pyhelayers.HeRunRequirements()\n",
    "he_run_req1.set_he_context_options([pyhelayers.DefaultContext()])\n",
    "he_run_req1.optimize_for_batch_size(16)\n",
    "\n",
    "profile1 = pyhelayers.HeModel.compile(nnp1, he_run_req1)\n",
    "batch_size1 = profile1.get_optimal_batch_size()\n",
    "print('Profile 1 ready. Batch size =', batch_size1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HE context 1 initialized\n"
     ]
    }
   ],
   "source": [
    "profile1.get_he_config_requirement()\n",
    "context1 = pyhelayers.HeModel.create_context(profile1)\n",
    "print('HE context 1 initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encrypted network 1 ready\n"
     ]
    }
   ],
   "source": [
    "context1.get_scheme_name()\n",
    "nn1 = pyhelayers.NeuralNet(context1)\n",
    "nn1.encode_encrypt(nnp1, profile1)\n",
    "print('Encrypted network 1 ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of size 16 loaded\n",
      "(16, 1, 88, 88)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "# Estrazione del batch\n",
    "plain_samples, labels = utilshe.extract_batch(test_img_array, test_label_array, batch_size1, 0)\n",
    "\n",
    "print('Batch of size', batch_size1, 'loaded')\n",
    "print(plain_samples.shape)\n",
    "print(plain_samples.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data encrypted for model 1\n"
     ]
    }
   ],
   "source": [
    "# Crittografia dei dati di input per il modello 1\n",
    "iop1 = nn1.create_io_processor()\n",
    "samples1 = pyhelayers.EncryptedData(context1)\n",
    "iop1.encode_encrypt_inputs_for_predict(samples1, [plain_samples])\n",
    "print('Test data encrypted for model 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples1.save_to_file('samples1_encrypted_data')\n",
    "context1.save_secret_key_to_file('secret_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_to_buffer(file_name):\n",
    "    try:\n",
    "        with open(file_name, 'rb') as file:\n",
    "            buf = file.read()\n",
    "        return buf\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Il file {file_name} non esiste.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Si è verificato un errore: {e}\")\n",
    "        return None\n",
    "\n",
    "buf = read_file_to_buffer('samples1_encrypted_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading failed due to mismatching contexts.\n",
      "The input file belongs to context with id 740737406,\n",
      "while the context in memory has id -1.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "loading failed due to mismatching contexts.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m context1 \u001b[38;5;241m=\u001b[39m pyhelayers\u001b[38;5;241m.\u001b[39mDefaultContext()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcontext1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_secret_key_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msecret_key\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: loading failed due to mismatching contexts."
     ]
    }
   ],
   "source": [
    "context1 = pyhelayers.DefaultContext()\n",
    "context1.load_secret_key_from_file('secret_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "load_samples = pyhelayers.loadEncryptedData(context1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration of predict 1: 7.930 (s)\n",
      "Duration of predict per sample model 1: 0.496 (s)\n"
     ]
    }
   ],
   "source": [
    "# Inferenza crittografata per il modello 1\n",
    "utilshe.start_timer()\n",
    "predictions1 = pyhelayers.EncryptedData(context1)\n",
    "nn1.predict(predictions1, samples1)\n",
    "duration1 = utilshe.end_timer('predict 1')\n",
    "utilshe.report_duration('predict per sample model 1', duration1 / batch_size1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes_after_HE(\n",
    "    predictions,\n",
    "    img,\n",
    "    labels,\n",
    "    device,\n",
    "):\n",
    "    all_pred_centers = []\n",
    "    all_true_centers = []\n",
    "\n",
    "    predictions = torch.tensor(predictions).to(device)\n",
    "    img = torch.tensor(img).to(device)\n",
    "    labels = torch.tensor(labels).to(device)\n",
    "    \n",
    "    batch_size = img.shape[0]\n",
    "    true_centers = cellcenters_to_centers(labels) # type: ignore\n",
    "    pred_centers = cellcenters_to_centers(predictions) # type: ignore\n",
    "\n",
    "    train_idx = 0\n",
    "\n",
    "    for idx in range(batch_size):\n",
    "\n",
    "        nms_centers= non_max_suppression(\n",
    "            pred_centers[idx]\n",
    "        )\n",
    "\n",
    "        for center in nms_centers:\n",
    "            all_pred_centers.append([train_idx] + center)\n",
    "\n",
    "        # for center in pred_centers[idx]:\n",
    "        #     if center[1] > 0:\n",
    "        #         all_pred_centers.append([train_idx] + center)\n",
    "\n",
    "        for center in true_centers[idx]:\n",
    "            if center[1] > 0:\n",
    "                all_true_centers.append([train_idx] + center)\n",
    "\n",
    "\n",
    "        train_idx += 1\n",
    "\n",
    "    return all_pred_centers, all_true_centers\n",
    "\n",
    "\n",
    "def convert_cellcenters(predictions, S=4, C=1):\n",
    "    \"\"\"\n",
    "    Converts predictions from the model to centers\n",
    "    \"\"\"\n",
    "    # predictions = predictions.to(\"cpu\")\n",
    "    batch_size = predictions.shape[0]\n",
    "    predictions = predictions.reshape(batch_size, S, S, C + 6)\n",
    "\n",
    "    centers1 = predictions[..., C + 1:C + 3]\n",
    "    centers2 = predictions[..., C + 4:C + 6]\n",
    "\n",
    "    scores = torch.cat(\n",
    "        (predictions[..., C].unsqueeze(0), predictions[..., C + 3].unsqueeze(0)), dim=0\n",
    "    )\n",
    "    best_center = scores.argmax(0).unsqueeze(-1)\n",
    "\n",
    "    best_centers = centers1 * (1 - best_center) + best_center * centers2\n",
    "\n",
    "    # This results in a tensor with shape (batch_size, 7, 7, 1) where each element represents the index of a grid cell.\n",
    "    cell_indices = torch.arange(S).repeat(batch_size, S, 1).unsqueeze(-1)\n",
    "    x = 1 / S * (best_centers[..., :1] + cell_indices)\n",
    "    # Permute because is used here to swap these indices to match the (x, y) convention used in the best_boxes tensor.\n",
    "    # [0,1,2]->[0,0,0]\n",
    "    # [0,1,2]->[1,1,1]\n",
    "    # [0,1,2]->[2,2,2]\n",
    "    y = 1 / S * (best_centers[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
    "    converted_centers = torch.cat((x, y), dim=-1)\n",
    "    predicted_class = predictions[..., :C].argmax(-1).unsqueeze(-1)\n",
    "    best_confidence = torch.max(predictions[..., C], predictions[..., C + 3]).unsqueeze(\n",
    "        -1\n",
    "    )\n",
    "\n",
    "    converted_preds = torch.cat(\n",
    "        (predicted_class, best_confidence, converted_centers), dim=-1\n",
    "    )\n",
    "\n",
    "    return converted_preds\n",
    "\n",
    "def cellcenters_to_centers(out, S=4):\n",
    "    converted_pred = convert_cellcenters(out).reshape(out.shape[0], S * S, -1)\n",
    "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
    "    all_centers = []\n",
    "\n",
    "    for ex_idx in range(out.shape[0]):\n",
    "        centers = []\n",
    "        for center_idx in range(S * S):\n",
    "            centers.append([x.item() for x in converted_pred[ex_idx, center_idx, :]])\n",
    "        all_centers.append(centers)\n",
    "\n",
    "    return all_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_centers_1, true_centers = get_bboxes_after_HE(plain_predictions_1, test_img_array, test_label_array, DEVICE)\n",
    "pred_centers_2, true_centers = get_bboxes_after_HE(plain_predictions_2, test_img_array, test_label_array, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_centers_1 = np.array(pred_centers_1)\n",
    "pred_centers_2 = np.array(pred_centers_2)\n",
    "\n",
    "pred_centers_1, pred_centers_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = np.mean([pred_centers_1, pred_centers_2], axis=0)\n",
    "ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1=[]\n",
    "for sample in test_img_array:\n",
    "    tensor_sample = torch.tensor(sample).unsqueeze(0)\n",
    "    output_1 = model(tensor_sample)\n",
    "    output_1 = output_1.detach().numpy()\n",
    "    pred_1.append(output_1)\n",
    "pred_1 = np.array(pred_1)\n",
    "pred_1 = pred_1.reshape(1, 112)\n",
    "print(f\"plain prediction shape before HE: {pred_1.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_2=[]\n",
    "for sample in test_img_array:\n",
    "    tensor_sample = torch.tensor(sample).unsqueeze(0)\n",
    "    output_2 = model(tensor_sample)\n",
    "    output_2 = output_2.detach().numpy()\n",
    "    pred_2.append(output_2)\n",
    "pred_2 = np.array(pred_2)\n",
    "pred_2 = pred_2.reshape(1, 112)\n",
    "print(f\"plain prediction shape before HE: {pred_2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1 , true_centers = get_bboxes_after_HE(pred_1, test_img_array, test_label_array, DEVICE)\n",
    "pred_2 , true_centers = get_bboxes_after_HE(pred_2, test_img_array, test_label_array, DEVICE)\n",
    "pred_1 = np.array(pred_1)\n",
    "pred_2 = np.array(pred_2)\n",
    "\n",
    "ensemble = np.mean([pred_1, pred_2], axis=0)\n",
    "ensemble"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
