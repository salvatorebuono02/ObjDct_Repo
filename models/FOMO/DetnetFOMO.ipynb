{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.onnx\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torchinfo import summary\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE:  cpu\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = \"cpu\"\n",
    "print(\"DEVICE: \", DEVICE)\n",
    "BATCH_SIZE = 32 # 64 in original paper but resource exhausted error otherwise.\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 125\n",
    "FILES_DIR = '/home/buono/ObjDct_Repo/data/ShipDataset'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Create Loader of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, centers):\n",
    "        for t in self.transforms:\n",
    "            img, centers = t(img), centers\n",
    "\n",
    "        return img, centers\n",
    "\n",
    "\n",
    "transform = Compose([transforms.Resize((88, 88)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShipDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, S=4, B=2, C=1, transform=None, train=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.train = train\n",
    "\n",
    "        # Determine the directory of the images and labels\n",
    "        if self.train:\n",
    "            self.img_dir = os.path.join(self.root_dir, 'images/train')\n",
    "            self.label_dir = os.path.join(self.root_dir, 'labels/train')\n",
    "        else:\n",
    "            self.img_dir = os.path.join(self.root_dir, 'images/test')\n",
    "            self.label_dir = os.path.join(self.root_dir, 'labels/test')\n",
    "\n",
    "        self.img_ids = os.listdir(self.img_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.img_ids[index].split('.')[0]\n",
    "        centers = []\n",
    "        boxes = []\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, img_id + '.jpg')\n",
    "        image = Image.open(img_path)\n",
    "        image = image.convert(\"L\")\n",
    "\n",
    "        # Load labels\n",
    "        label_path = os.path.join(self.label_dir, img_id + '.txt')\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                class_label, x, y, width, height = map(float, line.strip().split())\n",
    "                centers.append([class_label, x, y])\n",
    "                boxes.append([class_label, x, y, width, height])\n",
    "\n",
    "        if len(boxes) > 3:\n",
    "            boxes = boxes[:3]\n",
    "            centers = centers[:3]\n",
    "\n",
    "        boxes = torch.tensor(boxes)\n",
    "        centers = torch.tensor(centers)\n",
    "        if self.transform:\n",
    "            image, centers = self.transform(image, centers)\n",
    "        # Convert To Cells\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.C + 3 * self.B))\n",
    "        for center in centers:\n",
    "            class_label, x, y = center\n",
    "            class_label = int(class_label)\n",
    "            i, j = int(self.S * y), int(self.S * x)\n",
    "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
    "\n",
    "            if label_matrix[i, j, self.C] == 0:\n",
    "                label_matrix[i, j, self.C] = 1\n",
    "\n",
    "                center_coordinates = torch.tensor(\n",
    "                    [x_cell, y_cell]\n",
    "                )\n",
    "\n",
    "                label_matrix[i, j, self.C + 1:self.C + 3] = center_coordinates\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "\n",
    "        #print(f\"label_matrix shape: {label_matrix.shape}\")\n",
    "\n",
    "        return image, label_matrix , boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    label_matrices = []\n",
    "    boxes_list = []\n",
    "    for item in batch:\n",
    "        images.append(item[0])\n",
    "        label_matrices.append(item[1])\n",
    "        boxes_list.append(item[2])\n",
    "    images = torch.stack(images)\n",
    "    label_matrices = torch.stack(label_matrices)\n",
    "    return images, label_matrices, boxes_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "train_dataset = ShipDataset(\n",
    "    root_dir=FILES_DIR,\n",
    "    transform=transform,\n",
    "    train=True\n",
    ")\n",
    "\n",
    "\n",
    "# Define the length of the training set and validation set\n",
    "train_len = int(0.8 * len(train_dataset))\n",
    "val_len = len(train_dataset) - train_len\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, validation_dataset = random_split(train_dataset, [train_len, val_len])\n",
    "\n",
    "\n",
    "# Now you can create your DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ShipDataset(\n",
    "    root_dir=FILES_DIR,\n",
    "    transform=transform,\n",
    "    train=False\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25426, 6357, 7946)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(validation_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance between centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(center_preds, center_labels):\n",
    "    \"\"\"\n",
    "    Calculate euclidean distance\n",
    "    Parameters:\n",
    "        center_preds: predictions of centers (BATCH_SIZE, 2)\n",
    "        center_labels: target of centers of shape (BATCH_SIZE, 2)\n",
    "    Returns:\n",
    "        distance: euclidean distance for all examples\n",
    "    \"\"\"\n",
    "\n",
    "    x1 = center_preds[..., 0:1]\n",
    "    y1 = center_preds[..., 1:2]\n",
    "    x2 = center_labels[..., 0:1]\n",
    "    y2 = center_labels[..., 1:2]\n",
    "\n",
    "    distance = torch.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_center_inside_bbox(center, bbox):\n",
    "    \"\"\"\n",
    "    Check if a center (x, y) is inside a bounding box (x, y, w, h).\n",
    "    Parameters:\n",
    "        center (tuple): The (x, y) coordinates of the center.\n",
    "        bbox (tuple): The (x, y, w, h) coordinates of the bounding box.\n",
    "    Returns:\n",
    "        bool: True if the center is inside the bounding box, False otherwise.\n",
    "    \"\"\"\n",
    "    center_x, center_y = center\n",
    "    bbox_x, bbox_y, bbox_w, bbox_h = bbox\n",
    "\n",
    "    bbox_x_min = bbox_x - bbox_w / 2\n",
    "    bbox_x_max = bbox_x + bbox_w / 2\n",
    "    bbox_y_min = bbox_y - bbox_h / 2\n",
    "    bbox_y_max = bbox_y + bbox_h / 2\n",
    "\n",
    "    return bbox_x_min <= center_x <= bbox_x_max and bbox_y_min <= center_y <= bbox_y_max\n",
    "\n",
    "\n",
    "def mean_average_precision(\n",
    "    pred_boxes, true_boxes, num_classes=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates mean average precision\n",
    "    Parameters:\n",
    "        pred_boxes (list): list of lists containing all bboxes with each bbox\n",
    "        specified as [train_idx, class_prediction, prob_score, x_center, y_center]\n",
    "        true_boxes (list): Similar as pred_boxes except all the correct ones\n",
    "        specified as [train_idx, class_label, x, y, w, h]\n",
    "        num_classes (int): number of classes\n",
    "    Returns:\n",
    "        float: mAP value across all classes\n",
    "    \"\"\"\n",
    "    average_precisions = []\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            best_match = False\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                if is_center_inside_bbox(detection[3:5], gt[2:]):\n",
    "                    best_match = True\n",
    "                    best_gt_idx = idx\n",
    "                    break\n",
    "\n",
    "            if best_match:\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(centers , threshold = 0.4 , dist_threshold = 0.05):\n",
    "\n",
    "    assert type(centers) == list\n",
    "    centers = [center for center in centers if center[1]> threshold]\n",
    "    centers = sorted(centers, key=lambda x: x[1], reverse=True)\n",
    "    centers_after_nms = []\n",
    "\n",
    "    while centers:\n",
    "        current_center = centers.pop(0)\n",
    "        centers = [\n",
    "            center\n",
    "            for center in centers\n",
    "            if euclidean_distance(\n",
    "                torch.tensor(current_center[0]),\n",
    "                torch.tensor(center[0])\n",
    "            )\n",
    "            < dist_threshold\n",
    "        ]\n",
    "        centers_after_nms.append(current_center)\n",
    "        \n",
    "\n",
    "    return centers_after_nms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and convert centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cellcenters(predictions, S=4, C=1):\n",
    "    \"\"\"\n",
    "    Converts predictions from the model to centers\n",
    "    \"\"\"\n",
    "    predictions = predictions.to(\"cpu\")\n",
    "    batch_size = predictions.shape[0]\n",
    "    predictions = predictions.reshape(batch_size, S, S, C + 6)\n",
    "\n",
    "    centers1 = predictions[..., C + 1:C + 3]\n",
    "    centers2 = predictions[..., C + 4:C + 6]\n",
    "\n",
    "    scores = torch.cat(\n",
    "        (predictions[..., C].unsqueeze(0), predictions[..., C + 3].unsqueeze(0)), dim=0\n",
    "    )\n",
    "    best_center = scores.argmax(0).unsqueeze(-1)\n",
    "\n",
    "    best_centers = centers1 * (1 - best_center) + best_center * centers2\n",
    "\n",
    "    # This results in a tensor with shape (batch_size, 7, 7, 1) where each element represents the index of a grid cell.\n",
    "    cell_indices = torch.arange(S).repeat(batch_size, S, 1).unsqueeze(-1)\n",
    "    x = 1 / S * (best_centers[..., :1] + cell_indices)\n",
    "    # Permute because is used here to swap these indices to match the (x, y) convention used in the best_boxes tensor.\n",
    "    # [0,1,2]->[0,0,0]\n",
    "    # [0,1,2]->[1,1,1]\n",
    "    # [0,1,2]->[2,2,2]\n",
    "    y = 1 / S * (best_centers[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
    "    converted_centers = torch.cat((x, y), dim=-1)\n",
    "    predicted_class = predictions[..., :C].argmax(-1).unsqueeze(-1)\n",
    "    best_confidence = torch.max(predictions[..., C], predictions[..., C + 3]).unsqueeze(\n",
    "        -1\n",
    "    )\n",
    "\n",
    "    converted_preds = torch.cat(\n",
    "        (predicted_class, best_confidence, converted_centers), dim=-1\n",
    "    )\n",
    "\n",
    "    return converted_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cellcenters_to_centers(out, S=4):\n",
    "    \"\"\"\n",
    "    Converts cell centers to centers\n",
    "    \"\"\"\n",
    "    converted_pred = convert_cellcenters(out).reshape(out.shape[0], S * S, -1)\n",
    "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
    "    all_centers = []\n",
    "\n",
    "    for ex_idx in range(out.shape[0]):\n",
    "        centers = []\n",
    "        for center_idx in range(S * S):\n",
    "            centers.append([x.item() for x in converted_pred[ex_idx, center_idx, :]])\n",
    "        all_centers.append(centers)\n",
    "\n",
    "    return all_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes(\n",
    "    loader,\n",
    "    model,\n",
    "    device,\n",
    "):\n",
    "    all_pred_centers = []\n",
    "    all_true_centers = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    # make sure model is in eval before get bboxes\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "\n",
    "    for batch_idx, (x, labels, boxes_list) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(x)\n",
    "\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        true_centers = cellcenters_to_centers(labels) \n",
    "        pred_centers = cellcenters_to_centers(predictions) \n",
    "        \n",
    "        for idx in range(batch_size):\n",
    "\n",
    "            boxes = boxes_list[idx].to(device)\n",
    "\n",
    "            nms_centers= non_max_suppression(\n",
    "                pred_centers[idx]\n",
    "            )\n",
    "\n",
    "            for center in nms_centers:\n",
    "                all_pred_centers.append([train_idx] + center)\n",
    "\n",
    "            for center in true_centers[idx]:\n",
    "                if center[1] > 0:\n",
    "                    all_true_centers.append([train_idx] + center)\n",
    "\n",
    "            for box in boxes:\n",
    "                all_true_boxes.append([train_idx] + box.tolist())\n",
    "\n",
    "            train_idx += 1\n",
    "\n",
    "    return all_pred_centers, all_true_centers, all_true_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the loss for yolo (v1) model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S=4, B=2, C=1):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        \"\"\"\n",
    "        S is split size of image (in paper 7),\n",
    "        B is number of boxes (in paper 2),\n",
    "        C is number of classes (in paper 20, in dataset 3),\n",
    "        \"\"\"\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        # These are from Yolo paper, signifying how much we should\n",
    "        # pay loss for no object (noobj) and the box coordinates (coord)\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        # predictions are shaped (BATCH_SIZE, S*S(C+B*3) when inputted\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 3)\n",
    "        # Calculate distances for the two predicted bounding boxes with target bbox\n",
    "        dis_c1 = euclidean_distance(predictions[..., self.C + 1:self.C + 3], target[..., self.C + 1:self.C + 3])\n",
    "        dis_c2 = euclidean_distance(predictions[..., self.C + 4:self.C + 6], target[..., self.C + 1:self.C + 3])\n",
    "        distances = torch.cat([dis_c1.unsqueeze(0), dis_c2.unsqueeze(0)], dim=0)\n",
    "        # Take the box with highest IoU out of the two prediction\n",
    "        # Note that bestbox will be indices of 0, 1 for which bbox was best\n",
    "        dis_maxes, bestcenter = torch.max(distances, dim=0)\n",
    "        exists_center = target[..., self.C].unsqueeze(3)  # in paper this is Iobj_i\n",
    "\n",
    "        # ======================== #\n",
    "        #   FOR CENTER COORDINATES #\n",
    "        # ======================== #\n",
    "\n",
    "        # Set boxes with no object in them to 0. We only take out one of the two\n",
    "        # predictions, which is the one with highest Iou calculated previously.\n",
    "\n",
    "        center_predictions = exists_center * (\n",
    "            (\n",
    "                bestcenter * predictions[..., self.C + 4:self.C + 6]\n",
    "                + (1 - bestcenter) * predictions[..., self.C + 1:self.C + 3]\n",
    "            )\n",
    "        )\n",
    "        center_targets = exists_center * target[..., self.C + 1:self.C + 3]\n",
    "\n",
    "        center_loss = self.mse(\n",
    "            torch.flatten(center_predictions, end_dim=-2),\n",
    "            torch.flatten(center_targets, end_dim=-2),\n",
    "        )\n",
    "\n",
    "        # ==================== #\n",
    "        #   FOR OBJECT LOSS    #\n",
    "        # ==================== #\n",
    "\n",
    "        # pred_box is the confidence score for the bbox with highest IoU\n",
    "        pred_center = (\n",
    "            bestcenter * predictions[..., self.C + 3:self.C + 4] + (1 - bestcenter) * predictions[..., self.C:self.C + 1]\n",
    "        )\n",
    "\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_center * pred_center),\n",
    "            torch.flatten(exists_center * target[..., self.C:self.C + 1]),\n",
    "        )\n",
    "\n",
    "        # ======================= #\n",
    "        #   FOR NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_center) * predictions[..., self.C:self.C + 1], start_dim=1),\n",
    "            torch.flatten((1 - exists_center) * target[..., self.C:self.C + 1], start_dim=1),\n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_center) * predictions[..., self.C + 3:self.C + 4], start_dim=1),\n",
    "            torch.flatten((1 - exists_center) * target[..., self.C:self.C + 1], start_dim=1)\n",
    "        )\n",
    "\n",
    "        # ================== #\n",
    "        #   FOR CLASS LOSS   #\n",
    "        # ================== #\n",
    "\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_center * predictions[..., :self.C], end_dim=-2,),\n",
    "            torch.flatten(exists_center * target[..., :self.C], end_dim=-2,),\n",
    "        )\n",
    "\n",
    "        loss = (\n",
    "            self.lambda_coord * center_loss  # first two rows in paper\n",
    "            + object_loss  # third row in paper\n",
    "            + self.lambda_noobj * no_object_loss  # forth row\n",
    "            + class_loss  # fifth row\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, t):\n",
    "        return torch.pow(t, 1) + torch.pow(t,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DetNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer5): Sequential(\n",
      "    (0): DetNetBottleneck(\n",
      "      (conv1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): DetNetBottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): DetNetBottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer6): Sequential(\n",
      "    (0): DetNetBottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): DetNetBottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): DetNetBottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.resnet import ResNet, Bottleneck\n",
    "\n",
    "class DetNetBottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, dilation=1):\n",
    "        super(DetNetBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride,\n",
    "                               padding=dilation, dilation=dilation, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * 4)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels * 4:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * 4, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * 4),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class DetNet(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DetNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # ResNet stages\n",
    "        self.layer1 = self._make_layer(Bottleneck, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(Bottleneck, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(Bottleneck, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(Bottleneck, 512, layers[3], stride=2)\n",
    "\n",
    "        # DetNet stages\n",
    "        self.layer5 = self._make_layer(DetNetBottleneck, 256, 3, stride=1, dilation=2)\n",
    "        self.layer6 = self._make_layer(DetNetBottleneck, 256, 3, stride=1, dilation=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(256 * 4, 1000)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1, dilation=1):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, dilation))\n",
    "        self.in_channels = out_channels * 4\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels, dilation=dilation))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "model = DetNet([3, 4, 6, 3])\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Conv2d: 2, BatchNorm2d: 2, ReLU: 2, MaxPool2d: 2, Conv2d: 4, BatchNorm2d: 4, ReLU: 4, Conv2d: 4, BatchNorm2d: 4, ReLU: 4, Conv2d: 4, BatchNorm2d: 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/ObjDct_Repo/venv/lib/python3.10/site-packages/torchinfo/torchinfo.py:295\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 295\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/ObjDct_Repo/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ObjDct_Repo/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1582\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1580\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1582\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "Cell \u001b[0;32mIn[21], line 97\u001b[0m, in \u001b[0;36mDetnetFOMO.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 97\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfclayers(x)\n",
      "File \u001b[0;32m~/ObjDct_Repo/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ObjDct_Repo/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1582\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1580\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1582\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "Cell \u001b[0;32mIn[21], line 75\u001b[0m, in \u001b[0;36mDetNetBackbone.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     73\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[0;32m---> 75\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n",
      "File \u001b[0;32m~/ObjDct_Repo/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ObjDct_Repo/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1582\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1580\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1582\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "File \u001b[0;32m~/ObjDct_Repo/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/ObjDct_Repo/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ObjDct_Repo/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1582\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1580\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1582\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "File \u001b[0;32m~/ObjDct_Repo/venv/lib/python3.10/site-packages/torchvision/models/resnet.py:158\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m identity\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 119\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE, weight_decay\u001b[38;5;241m=\u001b[39mWEIGHT_DECAY)\n\u001b[1;32m    118\u001b[0m model \u001b[38;5;241m=\u001b[39m DetnetFOMO()\n\u001b[0;32m--> 119\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ObjDct_Repo/venv/lib/python3.10/site-packages/torchinfo/torchinfo.py:223\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m validate_user_params(\n\u001b[1;32m    217\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[1;32m    218\u001b[0m )\n\u001b[1;32m    220\u001b[0m x, correct_input_size \u001b[38;5;241m=\u001b[39m process_input(\n\u001b[1;32m    221\u001b[0m     input_data, input_size, batch_dim, device, dtypes\n\u001b[1;32m    222\u001b[0m )\n\u001b[0;32m--> 223\u001b[0m summary_list \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_forward_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m formatting \u001b[38;5;241m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[1;32m    227\u001b[0m results \u001b[38;5;241m=\u001b[39m ModelStatistics(\n\u001b[1;32m    228\u001b[0m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[1;32m    229\u001b[0m )\n",
      "File \u001b[0;32m~/ObjDct_Repo/venv/lib/python3.10/site-packages/torchinfo/torchinfo.py:304\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    303\u001b[0m     executed_layers \u001b[38;5;241m=\u001b[39m [layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m summary_list \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mexecuted]\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuted layers up to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecuted_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Conv2d: 2, BatchNorm2d: 2, ReLU: 2, MaxPool2d: 2, Conv2d: 4, BatchNorm2d: 4, ReLU: 4, Conv2d: 4, BatchNorm2d: 4, ReLU: 4, Conv2d: 4, BatchNorm2d: 4]"
     ]
    }
   ],
   "source": [
    "class DetNetBottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, dilation=1):\n",
    "        super(DetNetBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride,\n",
    "                               padding=dilation, dilation=dilation, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * 4)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels * 4:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * 4, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * 4),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class DetNetBackbone(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DetNetBackbone, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(1, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # ResNet stages\n",
    "        self.layer1 = self._make_layer(Bottleneck, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(Bottleneck, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(Bottleneck, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(Bottleneck, 512, layers[3], stride=2)\n",
    "\n",
    "        # DetNet stages\n",
    "        self.layer5 = self._make_layer(DetNetBottleneck, 256, 3, stride=1, dilation=2)\n",
    "        self.layer6 = self._make_layer(DetNetBottleneck, 256, 3, stride=1, dilation=2)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1, dilation=1):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, dilation))\n",
    "        self.in_channels = out_channels * 4\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels, dilation=dilation))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DetnetFOMO(L.LightningModule):\n",
    "    def __init__(self, B=2, num_classes=1, S=4):\n",
    "        super().__init__()\n",
    "        self.backbone = DetNetBackbone([3, 4, 6, 3])\n",
    "\n",
    "        self.fclayers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 4 * 3 * 3, 256),  # Adjust input size according to output from DetNet\n",
    "            LinearActivation(),\n",
    "            nn.Linear(256, S * S * (num_classes + 3 * B)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.fclayers(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, boxes = batch\n",
    "        out = self(x)\n",
    "        loss = self.loss_fn(out, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, boxes = batch\n",
    "        out = self(x)\n",
    "        loss = self.loss_fn(out, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "model = DetnetFOMO()\n",
    "summary(model, input_size=(1, 1, 256, 256))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/buono/ObjDct_Repo/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "loss_fn = YoloLoss()\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(save_top_k=1, \n",
    "                                      monitor=\"val_loss\", \n",
    "                                      mode=\"min\")\n",
    "lr_monitor_callback = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "trainer = L.Trainer(accelerator=DEVICE, \n",
    "                    callbacks=[checkpoint_callback, lr_monitor_callback], \n",
    "                    max_epochs=EPOCHS, \n",
    "                    enable_progress_bar=True, \n",
    "                    enable_model_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"lightning_logs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name     | Type       | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | loss_fn  | YoloLoss   | 0      | train\n",
      "1 | layer1   | Sequential | 104    | train\n",
      "2 | layer2   | Sequential | 1.2 K  | train\n",
      "3 | fclayers | Sequential | 78.2 K | train\n",
      "------------------------------------------------\n",
      "79.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "79.5 K    Total params\n",
      "0.318     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 1/? [00:00<00:00,  2.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buono/ObjDct_Repo/venv/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 32. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buono/ObjDct_Repo/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/795 [00:00<?, ?it/s, v_num=0]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buono/ObjDct_Repo/venv/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 21. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  54%|█████▍    | 433/795 [03:11<02:40,  2.26it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buono/ObjDct_Repo/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_centers, target_centers, real_boxes = get_bboxes(\n",
    "        test_loader, model, DEVICE\n",
    "    )\n",
    "    mAP = mean_average_precision(pred_centers, real_boxes)\n",
    "    print(f\"mAP: {mAP}\")\n",
    "torch.save(model.state_dict(), \"../../models/trained_models/LeNetFOMO.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot test results and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGgCAYAAADsNrNZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAqoElEQVR4nO3df3RU9Z3/8ddMfgxRyIREmJCaQOqiQdStBYUIu7WaLUuphRLd2kO3UTlrtfEH5Lu1phW6WjHUnhVqV3D1uKinUipnK4p7iofGmh5Pw6+4WKk1YmWbtDCDtiYDaCYh8/n+oY6dJJjcycx87kyej3Pu0ftj7rznMzN5cX98PuMxxhgBAJBmXtsFAADGJgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGBFygLogQce0LRp0zRu3DjNmTNHe/bsSdVTAQAykCcVY8H99Kc/1de+9jU9+OCDmjNnjtavX6+tW7eqvb1dkydP/tjHRqNRHT58WBMmTJDH40l2aQCAFDPG6NixYyorK5PX+zHHOSYFLr74YlNfXx+b7+/vN2VlZaapqWnYx3Z2dhpJTExMTEwZPnV2dn7s3/tcJVlvb6/a2trU2NgYW+b1elVTU6PW1tZB20ciEUUikdi8+eCAbL4+r1zljfyJnR4tjeTALxX7dCO3HGkm0n5uqR3pNZa+a05fayqew+E+T5o+vaj/0YQJEz52u6QH0Ntvv63+/n4FAoG45YFAQK+99tqg7ZuamnTnnXcOUViecj0pDCClIIBGsk83cs0fcQIIIzWWvmtpCKDhniOhUNOwl1Gs3wXX2Nio7u7u2NTZ2Wm7JABAGiT9COiMM85QTk6OQqFQ3PJQKKTS0tJB2/t8Pvl8vmSXAQBwuaQfAeXn52vWrFlqbm6OLYtGo2publZ1dXWynw4AkKGSfgQkSQ0NDaqrq9Ps2bN18cUXa/369Tpx4oSuvfbaVDwdACADpSSAvvzlL+utt97S6tWrFQwG9alPfUo7duwYdGMCAGDsSklH1NEIh8Py+/26VItTexcct2F/xC13knEbNkZqLH3XMvQ27BfMNnV3d6uwsPCU21m/Cw4AMDYRQAAAKwggAIAVKbkJwQqn5zBTce3ABedeM1qmnh8fijdnwD6izveRLVLxGXfr9yJbr005fV0j3J4jIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWuLcfkMfj7F7/TL3/HqOTjv4gbu1zkinGUvu5pX+h9Ro8I/ohV46AAABWEEAAACsIIACAFe69BoTsM5bGvRtu7De3/PZROsbSQ3ql4rfOUoQjIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACuypyOqSzpWuUI6BmZNpL15jz7ilrZIRx1jqgNyCr57Ttsng9qTIyAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKxwHEC/+tWvdMUVV6isrEwej0fbtm2LW2+M0erVqzVlyhQVFBSopqZGBw8eTFa9AIAs4TiATpw4ob/927/VAw88MOT6e++9V/fff78efPBB7d69W6effroWLFignp6eURcLAMgejjuiLly4UAsXLhxynTFG69ev1x133KHFixdLkh5//HEFAgFt27ZNV1999eiqBQBkjaReAzp06JCCwaBqampiy/x+v+bMmaPW1tYhHxOJRBQOh+MmAED2S2oABYNBSVIgEIhbHggEYusGampqkt/vj03l5eXJLAkA4FLW74JrbGxUd3d3bOrs7ExsR8Z8/JSKfSbjOVLB43E2ZbJ0vEdued/dUodT2fz5c8rpd9Mt7eP0ezbCz2NSA6i0tFSSFAqF4paHQqHYuoF8Pp8KCwvjJgBA9ktqAFVWVqq0tFTNzc2xZeFwWLt371Z1dXUynwoAkOEc3wV3/PhxvfHGG7H5Q4cOaf/+/SouLlZFRYVWrFihu+++W9OnT1dlZaVWrVqlsrIyLVmyJJl1AwAynOMA2rdvnz772c/G5hsaGiRJdXV1evTRR3XbbbfpxIkTuv7669XV1aX58+drx44dGjduXPKqBgBkPI8x7rp6GQ6H5ff7dalniXI9eSN/YCp+9Mpp06TiOdzwg2FDcdPFUScy9T1yUx04tWx9jxx+z06aPr2gp9Xd3f2x1/Wt3wUHABibCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKxw3BHVtVJxf32m3rPvVCa/TjfUnq6udE5fayrqckN7Z7J0fFZc8bfQI43gpXIEBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMCK7OkHBNji1r4xI6nLXT8Hlv3c+lmxhCMgAIAVBBAAwAoCCABghXuvARmjEQ0m9KFUnFt1w/nxRGpIx3lmt4wz5rQOzsGPjls/j8hIHAEBAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBY4d6OqKnmlo6U2Swd7ZGpbe6GTs6SOzotZ+p7OJRsei1pwBEQAMAKAggAYIWjAGpqatJFF12kCRMmaPLkyVqyZIna29vjtunp6VF9fb1KSko0fvx41dbWKhQKJbVoAEDmcxRALS0tqq+v165du7Rz50719fXpc5/7nE6cOBHbZuXKldq+fbu2bt2qlpYWHT58WEuXLk164QCAzOYxJvGroW+99ZYmT56slpYW/f3f/726u7s1adIkbd68WVdeeaUk6bXXXtOMGTPU2tqquXPnDrvPcDgsv9+vS7VYuZ48B6/E4cU/bkIYnZG0X7a0RyouonMTQnprQFqdNH16wWxTd3e3CgsLT7ndqK4BdXd3S5KKi4slSW1tberr61NNTU1sm6qqKlVUVKi1tXXIfUQiEYXD4bgJAJD9Eg6gaDSqFStWaN68eTrvvPMkScFgUPn5+SoqKorbNhAIKBgMDrmfpqYm+f3+2FReXp5oSQCADJJwANXX1+vAgQPasmXLqApobGxUd3d3bOrs7BzV/gAAmSGhjqg33XSTnn32Wf3qV7/SmWeeGVteWlqq3t5edXV1xR0FhUIhlZaWDrkvn88nn8+XSBkAgAzm6AjIGKObbrpJTz31lJ5//nlVVlbGrZ81a5by8vLU3NwcW9be3q6Ojg5VV1cnp2IAQFZwdARUX1+vzZs36+mnn9aECRNi13X8fr8KCgrk9/u1fPlyNTQ0qLi4WIWFhbr55ptVXV09ojvgAABjh6MA2rhxoyTp0ksvjVu+adMmXXPNNZKkdevWyev1qra2VpFIRAsWLNCGDRuSUuzHcnqrp1tu/XTD7eBuuSU4HRK5fXy49kyk/TL185dI3W74PLqlvTOV0/dkhNuPqh9QKiTcD2g4bv0AuuHLlqqPgBvbPBX9lwig5HLDdwLxHL4nJ02fXtDTqe0HBABAogggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwIqEfg8ISZSKccfSMb5XKsbrSsewhJk8Jpgbx0jzDPFvWBMd3T5H9LwufR+dDoqcKRzX7ZFG8HHlCAgAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAK93ZE9Xgyt9NWKo2kTTK1Q6dbOsQ6la7PaSo6LSO5+JvlCEdAAAArCCAAgBUEEADACvdeAxor3DB4YSLXCjL1XLcbB/TMZCMZeJRrUek1kvZ2yWeWIyAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKxwFEAbN27UBRdcoMLCQhUWFqq6ulo///nPY+t7enpUX1+vkpISjR8/XrW1tQqFQkkvGgCQ+RwF0Jlnnqm1a9eqra1N+/bt02WXXabFixfrt7/9rSRp5cqV2r59u7Zu3aqWlhYdPnxYS5cuTUnhrmSM8+nDMe9ONSXyHE4NV8NI6kpG+yQiFXWnoy3SwS21J/L5Gm5y+j1zi0T+Rjjllvd9BDzGjO7dKS4u1g9+8ANdeeWVmjRpkjZv3qwrr7xSkvTaa69pxowZam1t1dy5c0e0v3A4LL/fr0s9S5TryRtNaemXjg6dmdyRMlM63broC4pTcPo9cMt7OkY6fZ80fXrBbFN3d7cKCwtPuV3C14D6+/u1ZcsWnThxQtXV1Wpra1NfX59qampi21RVVamiokKtra2n3E8kElE4HI6bAADZz3EAvfLKKxo/frx8Pp9uuOEGPfXUUzr33HMVDAaVn5+voqKiuO0DgYCCweAp99fU1CS/3x+bysvLHb8IAEDmcRxA55xzjvbv36/du3frxhtvVF1dnV599dWEC2hsbFR3d3ds6uzsTHhfAIDM4Xgw0vz8fP3N3/yNJGnWrFnau3evfvjDH+rLX/6yent71dXVFXcUFAqFVFpaesr9+Xw++Xw+55UDADLaqPsBRaNRRSIRzZo1S3l5eWpubo6ta29vV0dHh6qrq0f7NACALOPoCKixsVELFy5URUWFjh07ps2bN+uFF17Qc889J7/fr+XLl6uhoUHFxcUqLCzUzTffrOrq6hHfAQcAGDscBdDRo0f1ta99TUeOHJHf79cFF1yg5557Tv/wD/8gSVq3bp28Xq9qa2sViUS0YMECbdiwISWFA6n0dPQp5Wr437o5Ka8We7+UhoqA7DPqfkDJRj+gFDxHsmtIVAb1A/qf6H8rfwQB1CuvFnlrk1AYHKEfkKulvB8QAACjQQABAKwggAAAVjjuBzSmuPE8czKeY+DrStd56eEek4463HI+3S3XAtz4GXfXZemxwdL1WY6AAABWEEAAACsIIACAFVwDykYDz+cOPH87cJ5z7gAsIICAIZwc4cmBkW4HYDACCBgCw+sAqcc/3wAAVnAElI1S0d8GAJKMIyAAgBUEEADACgIIAGAF14A+Tjp+q8fG2GT0AxobRvjbR9brGK6fWjZx62uzVBdHQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFbQERXu6RznljrSIR2vdSy1J5zxDDj2MFErZXAEBACwggACAFhBAAEArOAaEBLjloEuncrUuiXng3riI5kyUPBQnNaejLoHPmeK2oIjIACAFQQQAMCKUQXQ2rVr5fF4tGLFitiynp4e1dfXq6SkROPHj1dtba1CodBo6wQAZJmEA2jv3r36z//8T11wwQVxy1euXKnt27dr69atamlp0eHDh7V06dJRFzpqHq/zCQCykYkOmEz85PHETymS0F/Z48ePa9myZXr44Yc1ceLE2PLu7m498sgjuu+++3TZZZdp1qxZ2rRpk379619r165dSSsaAJD5Egqg+vp6LVq0SDU1NXHL29ra1NfXF7e8qqpKFRUVam1tHXJfkUhE4XA4bgIAZD/Ht2Fv2bJFL730kvbu3TtoXTAYVH5+voqKiuKWBwIBBYPBIffX1NSkO++802kZAIAM5yiAOjs7deutt2rnzp0aN25cUgpobGxUQ0NDbD4cDqu8vDwp+45jaawjK9LRbyAV54XT0VfDLX07EuGGNk/HZyUVfbUy+X1PB0vt4+gUXFtbm44ePapPf/rTys3NVW5urlpaWnT//fcrNzdXgUBAvb296urqintcKBRSaWnpkPv0+XwqLCyMmwAA2c/REdDll1+uV155JW7Ztddeq6qqKn3rW99SeXm58vLy1NzcrNraWklSe3u7Ojo6VF1dnbyqAQAZz1EATZgwQeedd17cstNPP10lJSWx5cuXL1dDQ4OKi4tVWFiom2++WdXV1Zo7d27yqgYAZLykjwW3bt06eb1e1dbWKhKJaMGCBdqwYUOynwYAkOE8xiRy5Td1wuGw/H6/LvUsUa4nz3Y5zrhlwEMbgxcmg1vabyxx42clkweMTQU3vkfDOGn69ILZpu7u7o+9rk93fwCAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWJL0jqjXD3Svvgnvj08YtfTWccut7lM2vNRUDhY72OdzSNviI0/d9hNtzBAQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVmTPYKRuwCCK8dzYHiMZJHFg3al4HekY1DNVdWDscfzZ80gj+GhxBAQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACvoBITFu7OMzEm6pmzowUln8HnEEBACwggACAFjhKID+7d/+TR6PJ26qqqqKre/p6VF9fb1KSko0fvx41dbWKhQKJb1oAEDmc3wENHPmTB05ciQ2vfjii7F1K1eu1Pbt27V161a1tLTo8OHDWrp0aVILRgoY43zCR2g/JCpTPhsp+ow7vgkhNzdXpaWlg5Z3d3frkUce0ebNm3XZZZdJkjZt2qQZM2Zo165dmjt3rtOnAgBkMcdHQAcPHlRZWZk++clPatmyZero6JAktbW1qa+vTzU1NbFtq6qqVFFRodbW1lPuLxKJKBwOx00AgOznKIDmzJmjRx99VDt27NDGjRt16NAh/d3f/Z2OHTumYDCo/Px8FRUVxT0mEAgoGAyecp9NTU3y+/2xqby8PKEXAgDILI5OwS1cuDD2/xdccIHmzJmjqVOn6sknn1RBQUFCBTQ2NqqhoSE2Hw6HCSEAGANGdRt2UVGRzj77bL3xxhsqLS1Vb2+vurq64rYJhUJDXjP6kM/nU2FhYdwEAMh+owqg48eP6/e//72mTJmiWbNmKS8vT83NzbH17e3t6ujoUHV19agLBQBkF0en4P71X/9VV1xxhaZOnarDhw/ru9/9rnJycvSVr3xFfr9fy5cvV0NDg4qLi1VYWKibb75Z1dXV3AEHABjEUQD98Y9/1Fe+8hX9+c9/1qRJkzR//nzt2rVLkyZNkiStW7dOXq9XtbW1ikQiWrBggTZs2JCSwgEAmc1jjLt6PoXDYfn9fl3qWaJcT57tcpxxS1M6HbwwVXWnow43DNSYqXVLzmt3S93pkKnfZxc4afr0gtmm7u7uj72uz1hwAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACkeDkaaVMZKSOBZTIuMpZeo4WekYwyoVr9Ut7TeWjJU2z+Tx+rIYR0AAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYIV7ByN1yunAgSMZnDAdgxEOV0ciNTCIYnqlq71T8Vlxo3QMpjsSqagjW96jJOEICABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKxwH0J/+9Cd99atfVUlJiQoKCnT++edr3759sfXGGK1evVpTpkxRQUGBampqdPDgwaQWDQDIfI4C6J133tG8efOUl5enn//853r11Vf17//+75o4cWJsm3vvvVf333+/HnzwQe3evVunn366FixYoJ6enqQXPyoez/CTG+oAPsRn5SMj+f66cUIcRx1Rv//976u8vFybNm2KLausrIz9vzFG69ev1x133KHFixdLkh5//HEFAgFt27ZNV199dZLKBgBkOkdHQM8884xmz56tq666SpMnT9aFF16ohx9+OLb+0KFDCgaDqqmpiS3z+/2aM2eOWltbh9xnJBJROByOmwAA2c9RAL355pvauHGjpk+frueee0433nijbrnlFj322GOSpGAwKEkKBAJxjwsEArF1AzU1Ncnv98em8vLyRF4HACDDODoFF41GNXv2bN1zzz2SpAsvvFAHDhzQgw8+qLq6uoQKaGxsVENDQ2w+HA4TQh/HLWPYuYXT8boSaZtsHoMtHe2H0cni98jREdCUKVN07rnnxi2bMWOGOjo6JEmlpaWSpFAoFLdNKBSKrRvI5/OpsLAwbgIAZD9HATRv3jy1t7fHLXv99dc1depUSe/fkFBaWqrm5ubY+nA4rN27d6u6ujoJ5QIAsoWjU3ArV67UJZdconvuuUf/9E//pD179uihhx7SQw89JEnyeDxasWKF7r77bk2fPl2VlZVatWqVysrKtGTJklTUDwDIUI4C6KKLLtJTTz2lxsZG3XXXXaqsrNT69eu1bNmy2Da33XabTpw4oeuvv15dXV2aP3++duzYoXHjxiW9eABA5vIY45Zff3pfOByW3+/XpVqsXE/eyB+YQRfeRoWbEOJxE8LouPECN5/xeG58j4Zx0vTpBbNN3d3dH3tdn7HgAABWEEAAACsIIACAFY5uQkgrBu8bGm2Sftnc5tn82kYrFZfHE2lvN1x3S1ENHAEBAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAAr3DsWXLp5hshiE01/HcNJxW+lJDLmlVvGEHNLHUiekbyn6fgZs1T8dhTicAQEALCCAAIAWEEAAQCsIIAAAFZwE8KH3HjDAQBkMY6AAABWEEAAACsIIACAFWPnGtDAjqYDr/kM1YGMTo4AkDIcAQEArCCAAABWEEAAACvGzjWg4fr5DHW9x60DHg7Had1c68pOqRi4Nh0yte5MZqk9OQICAFhBAAEArHAUQNOmTZPH4xk01dfXS5J6enpUX1+vkpISjR8/XrW1tQqFQikpHACQ2RwF0N69e3XkyJHYtHPnTknSVVddJUlauXKltm/frq1bt6qlpUWHDx/W0qVLk181ACDjOboJYdKkSXHza9eu1VlnnaXPfOYz6u7u1iOPPKLNmzfrsssukyRt2rRJM2bM0K5duzR37tzkVQ0AyHgJXwPq7e3Vj3/8Y1133XXyeDxqa2tTX1+fampqYttUVVWpoqJCra2tp9xPJBJROByOmwAA2S/hANq2bZu6urp0zTXXSJKCwaDy8/NVVFQUt10gEFAwGDzlfpqamuT3+2NTeXl5oiUBADJIwgH0yCOPaOHChSorKxtVAY2Njeru7o5NnZ2do9ofACAzJNQR9Q9/+IN+8Ytf6Gc/+1lsWWlpqXp7e9XV1RV3FBQKhVRaWnrKffl8Pvl8vkTKSD03dnYbSU3p6EAL93Pj53ckMrVuKbNrtyChI6BNmzZp8uTJWrRoUWzZrFmzlJeXp+bm5tiy9vZ2dXR0qLq6evSVAgCyiuMjoGg0qk2bNqmurk65uR893O/3a/ny5WpoaFBxcbEKCwt18803q7q6mjvgAACDOA6gX/ziF+ro6NB11103aN26devk9XpVW1urSCSiBQsWaMOGDUkpFACQXTzGuOuCQTgclt/v16WeJcr15NkuJ/WGa/5EzimPpcFIU/HxzeT2SLVE2tsN7ZmpdbuFw/Y7afr0gp5Wd3e3CgsLT7kdY8EBAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsCKhseCywsD72m3d85+K503Ha/EM+LeLiab+OYesw+FrHUl/hnR0jaOPSXrR3qPjuP080gi+RhwBAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVY3cw0oGD6w0cXHMotgbcTLWEBt9MQVskUofTQRLdMihlOl7rSJ4jU9vPrYZr83S0Xyre9xQZuwH0gS+aN3SVaVexevR7FekB76fV7imxXRYAZL0xfQruM6ZTXzcv68eec3Wj93N601OkpmiLikyP7dIAIOuN6QCqNa/r56rUc55KdXj8+qFntiLK1QJzyHZpAJD1xmwA5ZqoztY7eskTiC0zHo9e8gR0rnnbYmUAMDaM2QDyK6IcGb2jcXHL39E4TRSn4AAg1cZsAAEA7BqzAdQtn/rlGXS0M1E9g46KAADJN2YD6KQ3V69roi7UUcnjlScnR16vVxeakH6XM0menJxBkzxeZxMA4JTGdD+g//aco9vMbr0eLVa7d5KW9rdrnE7qOe8nbZcGAFlvTAdQi7dCRdGI6swBTezr0ZueifpO7mfV5SmwXRoAZD1H54n6+/u1atUqVVZWqqCgQGeddZa+973vyfzV0A/GGK1evVpTpkxRQUGBampqdPDgwaQXnixPe6frqzlX6Av5V+uWvAV6zXuG7ZIAYExwdAT0/e9/Xxs3btRjjz2mmTNnat++fbr22mvl9/t1yy23SJLuvfde3X///XrsscdUWVmpVatWacGCBXr11Vc1bpx7Lu57vPFjIeVMHhw8psAXP3/kaPwG/f1xs9HevoFPMviJB44nN3AbG+PNuWRcqITqcDqmWjqeI1WcjjPmlvc1FeOjpeN9T8RwzzOWxjscAUcB9Otf/1qLFy/WokWLJEnTpk3TT37yE+3Zs0fS+0c/69ev1x133KHFixdLkh5//HEFAgFt27ZNV199dZLLT8wk866KTHxYeHuH2NCTHzdr+v8Svz46IIDMyeGffNAH0HPK9d3y6S3PacPvEwAykKMAuuSSS/TQQw/p9ddf19lnn62XX35ZL774ou677z5J0qFDhxQMBlVTUxN7jN/v15w5c9Ta2jpkAEUiEUUikdh8OBxO9LWMyCTzrh4xO1RwMj48dHTo7VPuY/5B9J5ytFz/SAgByEqOAuj2229XOBxWVVWVcnJy1N/frzVr1mjZsmWSpGAwKEkKBAJxjwsEArF1AzU1NenOO+9MpPaE+BVRgfq1NmeuOjyFseXe4uLBG/sGHAG9PcwRUF8CR0ADD5c/WF+hsBrNHvkV0VsigABkH0cB9OSTT+qJJ57Q5s2bNXPmTO3fv18rVqxQWVmZ6urqEiqgsbFRDQ0NsflwOKzy8vKE9uVEh6dQb3g+Cp2c/MmDtjHjBlwDyhl4bnVAAHkGXAMa0jDXgD5c75LLDgCQKo4C6Jvf/KZuv/322Km0888/X3/4wx/U1NSkuro6lZaWSpJCoZCmTJkSe1woFNKnPvWpIffp8/nk8/mGXJdMnpyc9/9rvNJJyVNZLs9pH9X45pcGHwH5BhzwfOJnkfgFuTnx86G34mYH3ZQAAIhxdBv2u+++K683/iE5OTmKRt//V3tlZaVKS0vV3NwcWx8Oh7V7925VV1cnoVwAQLZwdAR0xRVXaM2aNaqoqNDMmTP1v//7v7rvvvt03XXXSZI8Ho9WrFihu+++W9OnT4/dhl1WVqYlS5akon4AQIZyFEA/+tGPtGrVKn3jG9/Q0aNHVVZWpq9//etavXp1bJvbbrtNJ06c0PXXX6+uri7Nnz9fO3bscFUfIACAfY4CaMKECVq/fr3Wr19/ym08Ho/uuusu3XXXXaOr7OMG9Byqs+aAbQd2NPUUvD+8jqffJ52U3plZqLfOmBhb/5Nr1g3a5eqOxXHzR9+ZFjc/8Tdd8c/xbvzI2t6ewb8rZN57L34+OvCuuA9fh4cbEUYiHZ3uMqhjX8ol0nE6Fe2Xqe9JptadIgzZDACwggACAFhBAAEArHDvzzGYqAZ12vzQSAb51IA+Oh/cKq4PrrkU/6Zbkws++tmFuzq/MGiXz0zfETd/zjk3xs0fvagobr74lYlx85Ofah9cZm/8oHOeAZ1ZB18T8sS/XhuDlQ5lJIMqcr7brlS8R275/LlVKgZezWIcAQEArCCAAABWEEAAACtcfA3I6JQdYUz/4GUDzq0OupbywU8+GPP+NRgTeksm56NtjjeeNWiX5yyMv+az5AutcfP+nPg+Pb887+y4+a7j8fOSNLHl/+Lm+weMsO3xvn+O3WM8p7wEBlgz3Gjukjt+ZBEZgSMgAIAVBBAAwAoCCABghXuvAXk8o7tnfsB5ZxN9P2vNB+ewTaRPxvvR7/vktL02aBdnvVEYN/9M3yVx819YHH9N6M8n4n+51L/8yKB9dvVPjd9mZ/x1pOiJD+bN+/V6vDnyeD/q0zTU5a9B0nHOPZH3hj4So+O0/WyNwTbw85eK930kfZxGK5G6MvUzbOm7yREQAMAKAggAYAUBBACwggACAFjh3psQUsxE+2X+6oq+6R98dd/bHY6br/yP+BsVfvFWddz8Ocvi1/9HxfZB+5x3ZXzn1tyec+LmT//1G+8/98keqUuSovEXdUfwY3wAkAn4ywUAsIIAAgBYQQABAKwYO9eAPrx28mGHK2MUN9rnENdRzMDOWT2RuNkpP42/5nPoePz1nAf+X3DQPpurN8TNf7vi83Hz//f9KknSu13jpF9KnpwcebwfvU2DBlkFgAzFERAAwAoCCABghetOwX142uuk6UvJ/nvNSYU/+O9J/fVzDM5iz4BTcJ4Bt0B7ovHjI/X39sTNR44Pfg3HcuP30XeiN27+ZN/7++g9Gfmgzj6dNB9tEzUnB+1zELf+/sqw43dl6Dha6ZKp7ZeKutMxFpxb2zMVkvweffj3e9BljIF7NcNtkWZ//OMfVV5ebrsMAMAodXZ26swzzzzletcFUDQa1eHDh2WMUUVFhTo7O1VYWDj8A/GxwuGwysvLac8koT2Ti/ZMLtvtaYzRsWPHVFZWJq/31Fd6XHcKzuv16swzz1Q4/P4oBIWFhXwgk4j2TC7aM7loz+Sy2Z5+v3/YbbgJAQBgBQEEALDCtQHk8/n03e9+Vz6fz3YpWYH2TC7aM7loz+TKlPZ03U0IAICxwbVHQACA7EYAAQCsIIAAAFYQQAAAKwggAIAVrg2gBx54QNOmTdO4ceM0Z84c7dmzx3ZJGaGpqUkXXXSRJkyYoMmTJ2vJkiVqb2+P26anp0f19fUqKSnR+PHjVVtbq1AoZKnizLF27Vp5PB6tWLEitoy2dO5Pf/qTvvrVr6qkpEQFBQU6//zztW/fvth6Y4xWr16tKVOmqKCgQDU1NTp48KDFit2rv79fq1atUmVlpQoKCnTWWWfpe9/7XtwgoK5uT+NCW7ZsMfn5+ea//uu/zG9/+1vzL//yL6aoqMiEQiHbpbneggULzKZNm8yBAwfM/v37zec//3lTUVFhjh8/HtvmhhtuMOXl5aa5udns27fPzJ0711xyySUWq3a/PXv2mGnTppkLLrjA3HrrrbHltKUzf/nLX8zUqVPNNddcY3bv3m3efPNN89xzz5k33ngjts3atWuN3+8327ZtMy+//LL54he/aCorK817771nsXJ3WrNmjSkpKTHPPvusOXTokNm6dasZP368+eEPfxjbxs3t6coAuvjii019fX1svr+/35SVlZmmpiaLVWWmo0ePGkmmpaXFGGNMV1eXycvLM1u3bo1t87vf/c5IMq2trbbKdLVjx46Z6dOnm507d5rPfOYzsQCiLZ371re+ZebPn3/K9dFo1JSWlpof/OAHsWVdXV3G5/OZn/zkJ+koMaMsWrTIXHfddXHLli5dapYtW2aMcX97uu4UXG9vr9ra2lRTUxNb5vV6VVNTo9bWVouVZabu7m5JUnFxsSSpra1NfX19ce1bVVWliooK2vcU6uvrtWjRorg2k2jLRDzzzDOaPXu2rrrqKk2ePFkXXnihHn744dj6Q4cOKRgMxrWp3+/XnDlzaNMhXHLJJWpubtbrr78uSXr55Zf14osvauHChZLc356uGw377bffVn9/vwKBQNzyQCCg1157zVJVmSkajWrFihWaN2+ezjvvPElSMBhUfn6+ioqK4rYNBAIKBoMWqnS3LVu26KWXXtLevXsHraMtnXvzzTe1ceNGNTQ06Nvf/rb27t2rW265Rfn5+aqrq4u121Dff9p0sNtvv13hcFhVVVXKyclRf3+/1qxZo2XLlkmS69vTdQGE5Kmvr9eBAwf04osv2i4lI3V2durWW2/Vzp07NW7cONvlZIVoNKrZs2frnnvukSRdeOGFOnDggB588EHV1dVZri7zPPnkk3riiSe0efNmzZw5U/v379eKFStUVlaWEe3pulNwZ5xxhnJycgbdSRQKhVRaWmqpqsxz00036dlnn9Uvf/nLuF8kLC0tVW9vr7q6uuK2p30Ha2tr09GjR/XpT39aubm5ys3NVUtLi+6//37l5uYqEAjQlg5NmTJF5557btyyGTNmqKOjQ5Ji7cb3f2S++c1v6vbbb9fVV1+t888/X//8z/+slStXqqmpSZL729N1AZSfn69Zs2apubk5tiwajaq5uVnV1dUWK8sMxhjddNNNeuqpp/T888+rsrIybv2sWbOUl5cX177t7e3q6OigfQe4/PLL9corr2j//v2xafbs2Vq2bFns/2lLZ+bNmzeoW8Drr7+uqVOnSpIqKytVWloa16bhcFi7d++mTYfw7rvvDvrF0ZycHEWjUUkZ0J6274IYypYtW4zP5zOPPvqoefXVV831119vioqKTDAYtF2a6914443G7/ebF154wRw5ciQ2vfvuu7FtbrjhBlNRUWGef/55s2/fPlNdXW2qq6stVp05/vouOGNoS6f27NljcnNzzZo1a8zBgwfNE088YU477TTz4x//OLbN2rVrTVFRkXn66afNb37zG7N48WLX3DbsNnV1deYTn/hE7Dbsn/3sZ+aMM84wt912W2wbN7enKwPIGGN+9KMfmYqKCpOfn28uvvhis2vXLtslZQRJQ06bNm2KbfPee++Zb3zjG2bixInmtNNOM1/60pfMkSNH7BWdQQYGEG3p3Pbt2815551nfD6fqaqqMg899FDc+mg0alatWmUCgYDx+Xzm8ssvN+3t7ZaqdbdwOGxuvfVWU1FRYcaNG2c++clPmu985zsmEonEtnFze/J7QAAAK1x3DQgAMDYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAV/x/qk607HjSUfAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_centers(idx, pred_centers, target_centers):\n",
    "    values = test_dataset[idx]\n",
    "    image = values[0]\n",
    "    image = image.permute(1, 2, 0).to(\"cpu\")\n",
    "\n",
    "    image = np.array(image)\n",
    "    height, width, _ = image.shape\n",
    "    p_centers = []\n",
    "    t_centers = []\n",
    "    for center in pred_centers:\n",
    "        if center[0] == idx:\n",
    "            p_centers.append(center[1:])\n",
    "\n",
    "    for center in target_centers:\n",
    "        if center[0] == idx:\n",
    "            t_centers.append(center[1:])\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "    for center in p_centers:\n",
    "        rect = patches.Rectangle(\n",
    "            (center[2] * width , center[3] * height),\n",
    "            1,\n",
    "            1,\n",
    "            linewidth=5,\n",
    "            edgecolor=\"r\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "    # Create a Rectangle patch\n",
    "    for boxes in t_centers:\n",
    "        class_label = int(boxes[0])\n",
    "        box = boxes[1:]\n",
    "        assert len(box) == 4, \"Got more values than in x, y, w, h, in a box!\"\n",
    "        upper_left_x = box[0] - box[2] / 2\n",
    "        upper_left_y = box[1] - box[3] / 2\n",
    "        rect = patches.Rectangle(\n",
    "            (upper_left_x * width, upper_left_y * height),\n",
    "            box[2] * width,\n",
    "            box[3] * height,\n",
    "            linewidth=1,\n",
    "            edgecolor=\"r\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Add class label text\n",
    "        ax.text(upper_left_x * width, upper_left_y * height, str(class_label), color='r', fontsize=10, verticalalignment='bottom')\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_centers(0, pred_centers, target_centers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
