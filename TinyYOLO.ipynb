{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a tiny version of YOLO with DIOR dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import gdown\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from xml.etree import ElementTree\n",
    "from activations import SquareActivation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = \"raw_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(raw_data_path):\n",
    "    print('[INFO] Raw data directory exists, skiping download.')\n",
    "else:\n",
    "    os.makedirs(raw_data_path)\n",
    "    print('[INFO] Raw data directory is been created.')\n",
    "    print('[INFO] Downloading data...\\n')\n",
    "    gdown.download('https://drive.google.com/uc?id=1KoQzqR20qvIXDf1qsXCHGxD003IPmXMw', output=os.path.join(raw_data_path, 'Annotations.zip'))\n",
    "    gdown.download('https://drive.google.com/file/d/1wq0FQCBsbrnf-sJfloi7IR9tBH87GGGS/view?usp=sharing', output=os.path.join(raw_data_path, 'JPEGImages-test.zip'),fuzzy=True)\n",
    "    gdown.download('https://drive.google.com/file/d/1NVRSBm3RfpGGtZvgLJG5e_XD9uP4DZmI/view?usp=sharing', output=os.path.join(raw_data_path, 'JPEGImages-trainval.zip'),fuzzy=True)\n",
    "    print('[INFO] Data is been downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dior_data_path = \"dior_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the zip data files\n",
    "if os.path.exists(dior_data_path):\n",
    "    print('[INFO] DIOR data directory exists, skiping extraction.')\n",
    "else:\n",
    "    os.makedirs(dior_data_path)\n",
    "    for i in os.listdir(raw_data_path):\n",
    "        filename = os.path.join(raw_data_path, i) \n",
    "        shutil.unpack_archive(filename=filename, extract_dir=dior_data_path)\n",
    "        print(f'[INFO] File \"{filename}\" is been extracted to \"{dior_data_path}\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for all the data\n",
    "annot_data_path = 'dior_data/Annotations/Horizontal Bounding Boxes'\n",
    "trainval_data_path = 'dior_data/JPEGImages-trainval'\n",
    "test_data_path = 'dior_data/JPEGImages-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of annotation files\n",
    "annot_file_list = sorted([os.path.join(annot_data_path, i) for i in os.listdir(annot_data_path) if '.xml' in i])\n",
    "annot_file_list[:5], annot_file_list[-5:], len(annot_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of training and validation images\n",
    "trainval_file_list = sorted([os.path.join(trainval_data_path, i) for i in os.listdir(trainval_data_path) if '.jpg' in i])\n",
    "trainval_file_list[:5], trainval_file_list[-5:], len(trainval_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of testing images\n",
    "test_file_list = sorted([os.path.join(test_data_path, i) for i in os.listdir(test_data_path) if '.jpg' in i])\n",
    "test_file_list[:5], test_file_list[-5:], len(test_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all the images path in one list\n",
    "image_data_file_list = np.concatenate((trainval_file_list, test_file_list))\n",
    "image_data_file_list[:5], image_data_file_list[-5:], len(image_data_file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data from annotation files\n",
    "meta_list = [] # To store general info for every image\n",
    "object_list_train = [] # To store object classes info of train dataset\n",
    "object_list_test = [] # To store object classes info of test dataset\n",
    "\n",
    "for file in tqdm(annot_file_list):\n",
    "    meta_dict = {}\n",
    "    root = ElementTree.parse(file).getroot()\n",
    "    \n",
    "    # Filename - extracted\n",
    "    for path in image_data_file_list:\n",
    "        if root.find('filename').text in path:\n",
    "            meta_dict['filename'] = path\n",
    "            meta_dict['split_type'] = path.split('/')[1]\n",
    "    \n",
    "    # Width - extracted\n",
    "    meta_dict['width'] = int(root.find('size').find('width').text)\n",
    "    \n",
    "    # Height - extracted\n",
    "    meta_dict['height'] = int(root.find('size').find('height').text)\n",
    "    \n",
    "    # Objects - extracted and combined into a single string\n",
    "    meta_dict['objects'] = ', '.join(np.unique([obj.find('name').text for obj in root.findall('object')]))\n",
    "    meta_list.append(meta_dict)\n",
    "    \n",
    "    # Collecting all the object classes instance and counting total appearance\n",
    "    for obj in root.findall('object'):\n",
    "        if meta_dict['split_type'] == 'JPEGImages-trainval':\n",
    "            object_list_train.append(obj.find('name').text)\n",
    "        elif meta_dict['split_type'] == 'JPEGImages-test':\n",
    "            object_list_test.append(obj.find('name').text)\n",
    "    \n",
    "# Counting the instance for every object class\n",
    "object_instance_list_train = Counter(sorted(object_list_train))\n",
    "object_instance_list_test = Counter(sorted(object_list_test))\n",
    "    \n",
    "# Collecting Class list and indexing it also in a sequence\n",
    "class_dict = {k: v for v, k in enumerate(sorted(np.unique(object_list_train)))}\n",
    "\n",
    "meta_list[:5], object_instance_list_train, object_instance_list_test, class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe from the extracted data\n",
    "meta_df = pd.DataFrame(meta_list)\n",
    "meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Size of the images width: {meta_df.width.unique()[0]} and height: {meta_df.height.unique()[0]}')\n",
    "print(f'Total number of classes with all possible combination: {len(meta_df.objects.unique())}')\n",
    "print(f'Total length of the training/validation dataset: {len(meta_df[meta_df[\"split_type\"] == \"JPEGImages-trainval\"])} and testing dataset: {len(meta_df[meta_df[\"split_type\"] == \"JPEGImages-test\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 classes in the dataset\n",
    "meta_df.objects.value_counts()[:20].plot(kind='barh').invert_yaxis()\n",
    "plt.xlabel('Images (Count)')\n",
    "plt.title('Top 20 Objects Classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "meta_df[meta_df[\"split_type\"] == \"JPEGImages-trainval\"].objects.value_counts()[:20].plot(kind='barh').invert_yaxis()\n",
    "plt.xlabel('Images (Count)')\n",
    "plt.title('Train/Val Dataset', fontsize=16)\n",
    "plt.subplot(1, 2, 2)\n",
    "meta_df[meta_df[\"split_type\"] == \"JPEGImages-test\"].objects.value_counts()[:20].plot(kind='barh').invert_yaxis()\n",
    "plt.xlabel('Images (Count)')\n",
    "plt.title('Test Dataset', fontsize=16)\n",
    "plt.suptitle('Top 20 Objects Classes in the Dataset', fontsize=20, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(28, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(list(object_instance_list_train.keys()), list(object_instance_list_train.values()))\n",
    "plt.xlabel('Objects (Count)')\n",
    "plt.title('Train/Val Dataset', fontsize=16)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(list(object_instance_list_test.keys()), list(object_instance_list_test.values()))\n",
    "plt.xlabel('Objects (Count)')\n",
    "plt.title('Test Dataset', fontsize=16)\n",
    "plt.suptitle('Total Count of Object Instances Per Class in the Dataset', fontsize=20, fontweight='bold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the classes - rerun the code for different classes.\n",
    "viz_class = random.sample(meta_df.objects.tolist(), 1)[0]\n",
    "viz_list = meta_df[meta_df['objects'] == viz_class].filename.tolist()\n",
    "plt.figure(figsize=(20, 5))\n",
    "rand = random.sample(viz_list, 4)\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.imshow(plt.imread(rand[i]))\n",
    "    plt.suptitle(f'Objects in the Image: {viz_class}', fontsize=20, fontweight='bold')\n",
    "    plt.axis(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert annotation to YOLO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for extracting data\n",
    "def extract_data_from_xml(xml_file: str):\n",
    "    \"\"\"\n",
    "    A function to extract data like filename, size, classes and bboxes from xml file.\n",
    "    \n",
    "    Parameters: xml_file: str, A string containing the path to the file.\n",
    "    \n",
    "    Returns: data_dict: dict, A dict containing all the extracted data.\n",
    "    \"\"\"\n",
    "    root = ElementTree.parse(xml_file).getroot()\n",
    "    \n",
    "    # Creating dict and list to store data\n",
    "    data_dict = {}\n",
    "    data_dict['bboxes'] = []\n",
    "    \n",
    "    # Reading the xml file\n",
    "    for element in root:\n",
    "        # Getting the filename\n",
    "        if element.tag == 'filename':\n",
    "            data_dict['filename'] = element.text\n",
    "        \n",
    "        # Getting the image size\n",
    "        elif element.tag == 'size':\n",
    "            image_size = []\n",
    "            for size_element in element:\n",
    "                image_size.append(int(size_element.text))\n",
    "            data_dict['image_size'] = image_size\n",
    "        \n",
    "        # Getting the bounding box\n",
    "        elif element.tag == 'object':\n",
    "            bbox = {}\n",
    "            for obj_element in element:\n",
    "                # Object or Class name\n",
    "                if obj_element.tag == 'name':\n",
    "                    bbox['class'] = obj_element.text\n",
    "                # Object bounding box \n",
    "                elif obj_element.tag == 'bndbox':\n",
    "                    for bbox_element in obj_element:\n",
    "                        bbox[bbox_element.tag] = int(bbox_element.text)\n",
    "            data_dict['bboxes'].append(bbox)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = extract_data_from_xml(annot_file_list[1])\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to create a YOLO format annotation\n",
    "def convert_dict_to_yolo(data_dict: dict):\n",
    "    \"\"\"\n",
    "    A function to convert the extracted data dict into a text file as per the YOLO format.\n",
    "    The final text file is saved in the directory \"dior_data/yolo_annotations/data_dict['filename'].txt\".\n",
    "    \n",
    "    Parameters: data_dict: dict, A dict containing the data.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    # Reading the bounding box data\n",
    "    for bbox in data_dict['bboxes']:\n",
    "        try:\n",
    "            class_id = class_dict[bbox['class']]\n",
    "        except KeyError:\n",
    "            print(f'Invalid Class. Object class: \"{bbox[\"class\"]}\" not present in the class list.')\n",
    "            \n",
    "        # Transforming the bbox in Yolo format [X, Y, W, H]\n",
    "        img_w, img_h, _ = data_dict['image_size'] # Normalizing the bbox using image size\n",
    "        \n",
    "        x_center = ((bbox['xmin'] + bbox['xmax']) / 2) / img_w\n",
    "        y_center = ((bbox['ymin'] + bbox['ymax']) / 2) / img_h\n",
    "        width = (bbox['xmax'] - bbox['xmin']) / img_w \n",
    "        height = (bbox['ymax'] - bbox['ymin']) / img_h\n",
    "        \n",
    "        # Writing the new data to the data list in Yolo format\n",
    "        data.append(f'{class_id} {x_center:.3f} {y_center:.3f} {width:.3f} {height:.3f}')\n",
    "        \n",
    "    # File name for saving the text file(same as xml and jpg file name)\n",
    "    yolo_annot_dir = os.path.join('dior_data', 'yolo_annotations')\n",
    "    if not os.path.exists(yolo_annot_dir):\n",
    "        os.makedirs(yolo_annot_dir)\n",
    "    save_file_name = os.path.join(yolo_annot_dir, data_dict['filename'].replace('jpg', 'txt'))\n",
    "    \n",
    "    # Saving the yolo annotation in a text file\n",
    "    f = open(save_file_name, 'w+')\n",
    "    f.write('\\n'.join(data))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all the xml files into Yolo format text files\n",
    "print('[INFO] Annotation extraction and creation into Yolo has started.')\n",
    "for annot_file in tqdm(annot_file_list):\n",
    "    data_dict = extract_data_from_xml(annot_file)\n",
    "    convert_dict_to_yolo(data_dict)\n",
    "print('[INFO] All the annotation are converted into Yolo format.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_annot_path = 'dior_data/yolo_annotations'\n",
    "yolo_annot_file_list = sorted([os.path.join(yolo_annot_path, i) for i in os.listdir(yolo_annot_path) if '.txt' in i])\n",
    "yolo_annot_file_list[:5], yolo_annot_file_list[-5:], len(yolo_annot_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict_idx = dict(zip(class_dict.values(), class_dict.keys()))\n",
    "class_dict_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bboxes(img_file: str, annot_file: str, class_dict: dict):\n",
    "    \"\"\"\n",
    "    A function to plot the bounding boxes amd their object classes onto the image.\n",
    "    \n",
    "    Parameters:\n",
    "        img_file: str, A string containing the path to the image file.\n",
    "        annot_file: str, A string containing the path to the annotation file in yolo format.\n",
    "        class_dict: dict, A dict containing the classes in the similar sequence as per the annot_file.\n",
    "    \"\"\"\n",
    "    # Reading the image and annot file\n",
    "    image = cv2.imread(img_file)\n",
    "    img_h, img_w, _ = image.shape\n",
    "    \n",
    "    with open(annot_file, 'r') as f:\n",
    "        data = f.read().split('\\n')\n",
    "        data = [i.split(' ') for i in data]\n",
    "        data = [[float(j) for j in i] for i in data]\n",
    "    \n",
    "    # Calculating the bbox in Pascal VOC format\n",
    "    for bbox in data:\n",
    "        class_idx, x_center, y_center, width, height = bbox\n",
    "        xmin = int((x_center - width / 2) * img_w)\n",
    "        ymin = int((y_center - height / 2) * img_h)\n",
    "        xmax = int((x_center + width / 2) * img_w)\n",
    "        ymax = int((y_center + height / 2) * img_h)\n",
    "        \n",
    "        # Correcting bbox if out of image size\n",
    "        if xmin < 0:\n",
    "            xmin = 0\n",
    "        if ymin < 0:\n",
    "            ymin = 0\n",
    "        if xmax > img_w - 1:\n",
    "            xmax = img_w - 1\n",
    "        if ymax > img_h - 1:\n",
    "            ymax = img_h - 1\n",
    "        \n",
    "        # Creating the box and label for the image\n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (255, 255, 0), 2)\n",
    "        cv2.putText(image, class_dict[class_idx], (xmin, 0 if ymin-10 < 0 else ymin-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 0), 2)\n",
    "    \n",
    "    # Displaying the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 8))\n",
    "rand_int = random.sample(range(len(yolo_annot_file_list)), 3)\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plot_bboxes(image_data_file_list[rand_int[i]], yolo_annot_file_list[rand_int[i]], class_dict_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement YOLO architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyYOLOv1(nn.Module):\n",
    "    def __init__(self, B=2, num_classes=20):\n",
    "        super(TinyYOLOv1, self).__init__()\n",
    "        S = 7  # grid size\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3, stride=2, padding=1),\n",
    "            SquareActivation(),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1),\n",
    "            SquareActivation(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 8, kernel_size=1, stride=1), # 1x1 convolution to reduce the depth\n",
    "            SquareActivation(),\n",
    "            nn.BatchNorm2d(8),  \n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()  # Add flatten layer\n",
    "        self.fc1 = nn.Linear(5000, 2048)  # Update input size of fc1 layer\n",
    "        self.square_activation = SquareActivation()  # Add SquareActivation()\n",
    "        self.fc2 = nn.Linear(2048, S * S * (B * 5 + num_classes))  # Reduced number of neurons\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)  # Flatten the output\n",
    "        x = self.fc1(x)\n",
    "        x = self.square_activation(x)  # Apply SquareActivation()\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = TinyYOLOv1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TinyYOLOv1                               [1, 1470]                 --\n",
       "├─Sequential: 1-1                        [1, 8, 25, 25]            --\n",
       "│    └─Conv2d: 2-1                       [1, 8, 400, 400]          224\n",
       "│    └─SquareActivation: 2-2             [1, 8, 400, 400]          --\n",
       "│    └─BatchNorm2d: 2-3                  [1, 8, 400, 400]          16\n",
       "│    └─AvgPool2d: 2-4                    [1, 8, 200, 200]          --\n",
       "│    └─Conv2d: 2-5                       [1, 16, 100, 100]         1,168\n",
       "│    └─SquareActivation: 2-6             [1, 16, 100, 100]         --\n",
       "│    └─BatchNorm2d: 2-7                  [1, 16, 100, 100]         32\n",
       "│    └─AvgPool2d: 2-8                    [1, 16, 50, 50]           --\n",
       "│    └─Conv2d: 2-9                       [1, 8, 50, 50]            136\n",
       "│    └─SquareActivation: 2-10            [1, 8, 50, 50]            --\n",
       "│    └─BatchNorm2d: 2-11                 [1, 8, 50, 50]            16\n",
       "│    └─AvgPool2d: 2-12                   [1, 8, 25, 25]            --\n",
       "├─Flatten: 1-2                           [1, 5000]                 --\n",
       "├─Linear: 1-3                            [1, 2048]                 10,242,048\n",
       "├─SquareActivation: 1-4                  [1, 2048]                 --\n",
       "├─Linear: 1-5                            [1, 1470]                 3,012,030\n",
       "==========================================================================================\n",
       "Total params: 13,255,670\n",
       "Trainable params: 13,255,670\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 61.11\n",
       "==========================================================================================\n",
       "Input size (MB): 7.68\n",
       "Forward/backward pass size (MB): 23.39\n",
       "Params size (MB): 53.02\n",
       "Estimated Total Size (MB): 84.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(1,3,800,800))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset folder in train/test/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating images and labels directory into the data directory\n",
    "root_dir = 'datasets'\n",
    "image_dir = 'datasets/images'\n",
    "label_dir = 'datasets/labels'\n",
    "img_train_dir = 'datasets/images/train'\n",
    "img_val_dir = 'datasets/images/val'\n",
    "label_train_dir = 'datasets/labels/train'\n",
    "label_val_dir = 'datasets/labels/val'\n",
    "\n",
    "# Using 20% of the test dataset for validation\n",
    "total_val_size = int(len(test_file_list) * 0.2)\n",
    "total_val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving the training images\n",
    "if not os.path.exists(img_train_dir):\n",
    "    os.makedirs(img_train_dir)\n",
    "\n",
    "for filepath in tqdm(trainval_file_list):\n",
    "    if os.path.isfile(filepath):\n",
    "        shutil.move(filepath, img_train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving the Validation images[20% of test dataset]\n",
    "if not os.path.exists(img_val_dir):\n",
    "    os.makedirs(img_val_dir)\n",
    "\n",
    "for filepath in tqdm(test_file_list[:total_val_size]):\n",
    "    if os.path.isfile(filepath):\n",
    "        shutil.move(filepath, img_val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving the training labels\n",
    "if not os.path.exists(label_train_dir):\n",
    "    os.makedirs(label_train_dir)\n",
    "\n",
    "for filepath in tqdm(trainval_file_list):\n",
    "    file_path = os.path.join('dior_data/yolo_annotations', filepath.replace('jpg', 'txt').split('/')[-1])\n",
    "    if os.path.isfile(file_path):\n",
    "        shutil.move(file_path, label_train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving the validation labels[20% of test dataset]\n",
    "if not os.path.exists(label_val_dir):\n",
    "    os.makedirs(label_val_dir)\n",
    "\n",
    "for filepath in tqdm(test_file_list[:total_val_size]):\n",
    "    file_path = os.path.join('dior_data/yolo_annotations', filepath.replace('jpg', 'txt').split('/')[-1])\n",
    "    if os.path.isfile(file_path):\n",
    "        shutil.move(file_path, label_val_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection over Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![IoU](https://b2633864.smushcdn.com/2633864/wp-content/uploads/2016/09/iou_equation.png?lossy=2&strip=1&webp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(boxes_preds, boxes_labels):\n",
    "    \"\"\"\n",
    "    Calculates intersection over union\n",
    "    \n",
    "    Parameters:\n",
    "        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)    \n",
    "    Returns:\n",
    "        tensor: Intersection over union for all examples\n",
    "    \"\"\"\n",
    "    # boxes_preds shape is (N, 4) where N is the number of bboxes\n",
    "    #boxes_labels shape is (n, 4)\n",
    "    \n",
    "    box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "    box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "    box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "    box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "    box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "    box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "    box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "    box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "    \n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "    #print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
    "    \n",
    "    #.clamp(0) is for the case when they don't intersect. Since when they don't intersect, one of these will be negative so that should become 0\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "    #print(f\"intersection: {intersection}\")\n",
    "\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "    #print(f\"box1_area: {box1_area}, box2_area: {box2_area}\")\n",
    "    \n",
    "    return intersection / (box1_area + box2_area - intersection + 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non Max Suppression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input**: A list of Proposal boxes B, corresponding confidence scores S and overlap threshold N.\n",
    "\n",
    "**Output**: A list of filtered proposals D.\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "1.  Select the proposal with highest confidence score, remove it from B and add it to the final proposal list D. (Initially D is empty).\n",
    "2.  Now compare this proposal with all the proposals — calculate the IOU (Intersection over Union) of this proposal with every other proposal. If the IOU is greater than the threshold N, remove that proposal from B.\n",
    "3.  Again take the proposal with the highest confidence from the remaining proposals in B and remove it from B and add it to D.\n",
    "4.  Once again calculate the IOU of this proposal with all the proposals in B and eliminate the boxes which have high IOU than threshold.\n",
    "5.  This process is repeated until there are no more proposals left in B.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(bboxes, iou_threshold, threshold):\n",
    "    \"\"\"\n",
    "    Does Non Max Suppression given bboxes\n",
    "    Parameters:\n",
    "        bboxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [class_pred, prob_score, x_center, y_center, width, height]\n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        threshold (float): threshold to remove predicted bboxes (independent of IoU) \n",
    "    Returns:\n",
    "        list: bboxes after performing NMS given a specific IoU threshold\n",
    "    \"\"\"\n",
    "\n",
    "    assert type(bboxes) == list\n",
    "\n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        chosen_box = bboxes.pop(0)\n",
    "\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            if box[0] != chosen_box[0]\n",
    "            or intersection_over_union(\n",
    "                torch.tensor(chosen_box[2:]),\n",
    "                torch.tensor(box[2:]),\n",
    "            )\n",
    "            < iou_threshold\n",
    "        ]\n",
    "\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "    #print(f\"bboxes_after_nms: {bboxes_after_nms}\")\n",
    "\n",
    "    return bboxes_after_nms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It describes a trade-off between precision and recall\n",
    "**Precision**, also referred to as the positive predictive value, describes how well a model predicts the positive class. \n",
    "\n",
    "**Recall**, also called sensitivity tells you if your model made the right predictions when it should have. \n",
    "\n",
    "Problems with considering only precision or recall:\n",
    "*   If a model has a high recall value but a low precision value, it means that the model is classifying as many negative samples as it is positive samples. \n",
    "*   If a model has a high precision value, but a low recall value, it means that the model has the ability to classify samples as positive, but only some. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(\n",
    "    pred_boxes, true_boxes, iou_threshold=0.5, num_classes=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates mean average precision \n",
    "    Parameters:\n",
    "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [train_idx, class_prediction, prob_score, x_center, y_center, width, height]\n",
    "        true_boxes (list): Similar as pred_boxes except all the correct ones \n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        num_classes (int): number of classes\n",
    "    Returns:\n",
    "        float: mAP value across all classes given a specific IoU threshold \n",
    "    \"\"\"\n",
    "\n",
    "    # list storing all AP for respective classes\n",
    "    average_precisions = []\n",
    "\n",
    "    # used for numerical stability later on\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # Go through all predictions and targets,\n",
    "        # and only add the ones that belong to the\n",
    "        # current class c\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "        #print(f\"{c} class has {len(detections)} detections\")\n",
    "\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "        #print(f\"{c} class has {len(ground_truths)} ground truths\")\n",
    "\n",
    "        # find the amount of bboxes for each training example\n",
    "        # Counter here finds how many ground truth bboxes we get\n",
    "        # for each training example, so let's say img 0 has 3,\n",
    "        # img 1 has 5 then we will obtain a dictionary with:\n",
    "        # amount_bboxes = {0:3, 1:5}\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "        #print(f\"{c} class has {len(amount_bboxes)} amount bboxes\")\n",
    "\n",
    "        # We then go through each key, val in this dictionary\n",
    "        # and convert to the following (w.r.t same example):\n",
    "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # sort by box probabilities which is index 2\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "        #print(f\"{c} class has {total_true_bboxes} total true bboxes\")\n",
    "        # If none exists for this class then we can safely skip\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # Only take out the ground_truths that have the same\n",
    "            # training idx as detection\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            #print(f\"{c} class has {num_gts} ground truths for detection {detection_idx}\")\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:])\n",
    "                )\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    # true positive and add this bounding box to seen\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "\n",
    "            # if IOU is lower then the detection is a false positive\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        #print(f\"{c} class has {recalls} recalls\")\n",
    "        #print(f\"{c} class has {precisions} precisions\")\n",
    "        # torch.trapz for numerical integration\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image, boxes):\n",
    "    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n",
    "    im = np.array(image)\n",
    "    height, width, _ = im.shape\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(1)\n",
    "    # Display the image\n",
    "    ax.imshow(im)\n",
    "\n",
    "    # box[0] is x midpoint, box[2] is width\n",
    "    # box[1] is y midpoint, box[3] is height\n",
    "\n",
    "    # Create a Rectangle potch\n",
    "    for box in boxes:\n",
    "        box = box[2:]\n",
    "        assert len(box) == 4, \"Got more values than in x, y, w, h, in a box!\"\n",
    "        upper_left_x = box[0] - box[2] / 2\n",
    "        upper_left_y = box[1] - box[3] / 2\n",
    "        rect = patches.Rectangle(\n",
    "            (upper_left_x * width, upper_left_y * height),\n",
    "            box[2] * width,\n",
    "            box[3] * height,\n",
    "            linewidth=1,\n",
    "            edgecolor=\"r\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and convert boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes(\n",
    "    loader,\n",
    "    model,\n",
    "    iou_threshold,\n",
    "    threshold,\n",
    "):\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    # make sure model is in eval before get bboxes\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "\n",
    "    for batch_idx, (x, labels) in enumerate(loader):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        true_bboxes = cellboxes_to_boxes(labels)\n",
    "        bboxes = cellboxes_to_boxes(predictions)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            nms_boxes = non_max_suppression(\n",
    "                bboxes[idx],\n",
    "                iou_threshold=iou_threshold,\n",
    "                threshold=threshold,\n",
    "            )\n",
    "\n",
    "\n",
    "            #if batch_idx == 0 and idx == 0:\n",
    "            #    plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n",
    "            #    print(nms_boxes)\n",
    "\n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx] + nms_box)\n",
    "\n",
    "            for box in true_bboxes[idx]:\n",
    "                # many will get converted to 0 pred\n",
    "                if box[1] > threshold:\n",
    "                    all_true_boxes.append([train_idx] + box)\n",
    "\n",
    "            train_idx += 1\n",
    "\n",
    "    model.train()\n",
    "    #print(f\"all_true_boxes: {all_true_boxes}\")\n",
    "    #print(f\"all_pred_boxes: {all_pred_boxes}\")\n",
    "    return all_pred_boxes, all_true_boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python code is part of an implementation of the YOLO (You Only Look Once) object detection algorithm, specifically the part that processes the output of the YOLO network.\n",
    "\n",
    "Here's a step-by-step explanation of what the code does:\n",
    "\n",
    "1. Reshapes the predictions tensor to have the shape (batch_size, 7, 7, C + 10), where C is the number of classes.\n",
    "\n",
    "2. Splits the reshaped predictions into two sets of bounding box predictions (`bboxes1` and `bboxes2`).\n",
    "\n",
    "3. Concatenates the confidence scores for the two sets of bounding boxes along a new dimension.\n",
    "\n",
    "4. Determines which set of bounding boxes has the higher confidence score for each cell in the grid.\n",
    "\n",
    "5. Combines the two sets of bounding boxes into one, using the set with the higher confidence score for each cell.\n",
    "\n",
    "6. Converts the x and y coordinates of the bounding boxes from offsets of the top-left corner of each cell to absolute coordinates in the image.\n",
    "\n",
    "7. Normalizes the width and height of the bounding boxes by the size of the image.\n",
    "\n",
    "8. Concatenates the x, y, width, and height of the bounding boxes along the last dimension to form `converted_bboxes`.\n",
    "\n",
    "9. Determines the class with the highest score for each cell.\n",
    "\n",
    "10. Finds the maximum confidence score for each cell.\n",
    "\n",
    "11. Concatenates the class predictions, confidence scores, and bounding box predictions along the last dimension to form `converted_preds`.\n",
    "\n",
    "This `converted_preds` tensor now contains the class predictions, confidence scores, and bounding box predictions for each cell in the grid, in a format that can be used for further processing, such as non-maximum suppression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cellboxes(predictions, S=7, C=20):\n",
    "    \"\"\"\n",
    "    Converts bounding boxes output from Yolo with\n",
    "    an image split size of S into entire image ratios\n",
    "    rather than relative to cell ratios.\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = predictions.shape[0]\n",
    "    predictions = predictions.reshape(batch_size, 7, 7, C + 10)\n",
    "    bboxes1 = predictions[..., C + 1:C + 5]\n",
    "    bboxes2 = predictions[..., C + 6:C + 10]\n",
    "    \n",
    "    scores = torch.cat(\n",
    "        (predictions[..., C].unsqueeze(0), predictions[..., C + 5].unsqueeze(0)), dim=0\n",
    "    )\n",
    "    best_box = scores.argmax(0).unsqueeze(-1)\n",
    "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
    "    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n",
    "\n",
    "    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n",
    "    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
    "    w_y = 1 / S * best_boxes[..., 2:4]\n",
    "\n",
    "    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n",
    "    predicted_class = predictions[..., :C].argmax(-1).unsqueeze(-1)\n",
    "    best_confidence = torch.max(predictions[..., C], predictions[..., C + 5]).unsqueeze(\n",
    "        -1\n",
    "    )\n",
    "    converted_preds = torch.cat(\n",
    "        (predicted_class, best_confidence, converted_bboxes), dim=-1\n",
    "    )\n",
    "    #print(f\"converted_preds: {converted_preds}\")\n",
    "\n",
    "    return converted_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cellboxes_to_boxes(out, S=7):\n",
    "    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n",
    "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
    "    all_bboxes = []\n",
    "\n",
    "    for ex_idx in range(out.shape[0]):\n",
    "        bboxes = []\n",
    "\n",
    "        for bbox_idx in range(S * S):\n",
    "            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n",
    "        all_bboxes.append(bboxes)\n",
    "    #print(f\"all_bboxes: {all_bboxes}\")\n",
    "    return all_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=\"my_checkpoint.pth\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Loader of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class DiorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, S=7, B=2, C=20, transform=None, train=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.train = train\n",
    "\n",
    "        # Determine the directory of the images and labels\n",
    "        if self.train:\n",
    "            self.img_dir = os.path.join(self.root_dir, 'images/train')\n",
    "            self.label_dir = os.path.join(self.root_dir, 'labels/train')\n",
    "        else:\n",
    "            self.img_dir = os.path.join(self.root_dir, 'images/val')\n",
    "            self.label_dir = os.path.join(self.root_dir, 'labels/val')\n",
    "\n",
    "        self.img_ids = os.listdir(self.img_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.img_ids[index].split('.')[0]\n",
    "        boxes = []\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, img_id + '.jpg')\n",
    "        image = Image.open(img_path)\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "        # Load labels\n",
    "        label_path = os.path.join(self.label_dir, img_id + '.txt')\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                class_label, x, y, width, height = map(float, line.strip().split())\n",
    "                boxes.append([class_label, x, y, width, height])\n",
    "\n",
    "        boxes = torch.tensor(boxes)\n",
    "        #print(f\"boxes: {boxes}\")\n",
    "        if self.transform:\n",
    "            image, boxes = self.transform(image, boxes)\n",
    "\n",
    "        # Convert To Cells\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
    "        for box in boxes:\n",
    "            class_label, x, y, width, height = box.tolist()\n",
    "            class_label = int(class_label)\n",
    "\n",
    "            i, j = int(self.S * y), int(self.S * x)\n",
    "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
    "\n",
    "            width_cell, height_cell = (\n",
    "                width * self.S,\n",
    "                height * self.S,\n",
    "            )\n",
    "\n",
    "            if label_matrix[i, j, self.C] == 0:\n",
    "                label_matrix[i, j, self.C] = 1\n",
    "\n",
    "                box_coordinates = torch.tensor(\n",
    "                    [x_cell, y_cell, width_cell, height_cell]\n",
    "                )\n",
    "\n",
    "                label_matrix[i, j, 21:25] = box_coordinates\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "    \n",
    "        #print(f\"label_matrix shape: {label_matrix.shape}\")\n",
    "\n",
    "        return image, label_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From original paper: \n",
    ">   YOLO predicts multiple bounding boxes per grid cell. At training time we only want one bounding box predictor to be responsible for each object. We assign one predictor to be “responsible” for predicting an object based on which prediction has the highest current IOU with the ground truth. This leads to specialization between the bounding box predictors.\n",
    "Each predictor gets better at predicting certain sizes, aspect ratios, or classes of object, improving overall recall. \n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\lambda_{\\text {coord }} \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{i j}^{\\text {obj }}\\left[\\left(x_i-\\hat{x}_i\\right)^2+\\left(y_i-\\hat{y}_i\\right)^2\\right] \\\\\n",
    "+\\lambda_{\\text {coord }} \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{i j}^{\\text {obj }}\\left[\\left(\\sqrt{w_i}-\\sqrt{\\hat{w}_i}\\right)^2+\\left(\\sqrt{h_i}-\\sqrt{\\hat{h}_i}\\right)^2\\right] \\\\\n",
    "+\\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{i j}^{\\text {obj }}\\left(C_i-\\hat{C}_i\\right)^2 \\\\\n",
    "+\\lambda_{\\text {noobj }} \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{i j}^{\\text {noobj }}\\left(C_i-\\hat{C}_i\\right)^2 \\\\\n",
    "+\\sum_{i=0}^{S^2} \\mathbb{1}_i^{\\text {obj }} \\sum_{c \\in \\text { classes }}\\left(p_i(c)-\\hat{p}_i(c)\\right)^2\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "During training we optimize the following, multi-part where $ 1_{obj}^i $ denotes if object appears in cell **i** and $1_{obj}^{ij}$ denotes that the **j**  bounding box predictor in cell i is “responsible” for that prediction.\n",
    "\n",
    "In every image many grid cells do not contain any object. This pushes the “confidence” scores of those cells towards zero, often overpowering the gradient from cells that do contain objects. This can lead to model instability, as the model may prioritize learning to predict empty cells rather than focusing on correctly detecting objects in cells containing them, causing training to diverge early on. To remedy this, we increase the loss from bounding box coordinate predictions and decrease the loss from confidence predictions for boxes that don’t contain objects. We use two parameters, $\\lambda_{coord}$ and $\\lambda_{noobj}$  to accomplish this.\n",
    "\n",
    "Note that the loss function only penalizes classification error if an object is present in that grid cell (hence the conditional class probability discussed earlier). It also only penalizes bounding box coordinate error if that predictor is “responsible” for the ground truth box (i.e. has the highest\n",
    "IOU of any predictor in that grid cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the loss for yolo (v1) model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S=7, B=2, C=20):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        \"\"\"\n",
    "        S is split size of image (in paper 7),\n",
    "        B is number of boxes (in paper 2),\n",
    "        C is number of classes (in paper 20, in dataset 3),\n",
    "        \"\"\"\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        # These are from Yolo paper, signifying how much we should\n",
    "        # pay loss for no object (noobj) and the box coordinates (coord)\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        # predictions are shaped (BATCH_SIZE, S*S(C+B*5) when inputted\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
    "\n",
    "        # Calculate IoU for the two predicted bounding boxes with target bbox\n",
    "        iou_b1 = intersection_over_union(predictions[..., self.C + 1:self.C + 5], target[..., self.C + 1:self.C + 5])\n",
    "        iou_b2 = intersection_over_union(predictions[..., self.C + 6:self.C + 10], target[..., self.C + 1:self.C + 5])\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "\n",
    "        # Take the box with highest IoU out of the two prediction\n",
    "        # Note that bestbox will be indices of 0, 1 for which bbox was best\n",
    "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
    "        exists_box = target[..., self.C].unsqueeze(3)  # in paper this is Iobj_i\n",
    "\n",
    "        # ======================== #\n",
    "        #   FOR BOX COORDINATES    #\n",
    "        # ======================== #\n",
    "\n",
    "        # Set boxes with no object in them to 0. We only take out one of the two \n",
    "        # predictions, which is the one with highest Iou calculated previously.\n",
    "        box_predictions = exists_box * (\n",
    "            (\n",
    "                bestbox * predictions[..., self.C + 6:self.C + 10]\n",
    "                + (1 - bestbox) * predictions[..., self.C + 1:self.C + 5]\n",
    "            )\n",
    "        )\n",
    "        #print(f\"box_predictions: {box_predictions.shape}\")\n",
    "        box_targets = exists_box * target[..., self.C + 1:self.C + 5]\n",
    "\n",
    "        # Take sqrt of width, height of boxes\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
    "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
    "        )\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions, end_dim=-2),\n",
    "            torch.flatten(box_targets, end_dim=-2),\n",
    "        )\n",
    "\n",
    "        # ==================== #\n",
    "        #   FOR OBJECT LOSS    #\n",
    "        # ==================== #\n",
    "\n",
    "        # pred_box is the confidence score for the bbox with highest IoU\n",
    "        pred_box = (\n",
    "            bestbox * predictions[..., self.C + 5:self.C + 6] + (1 - bestbox) * predictions[..., self.C:self.C + 1]\n",
    "        )\n",
    "\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_box * pred_box),\n",
    "            torch.flatten(exists_box * target[..., self.C:self.C + 1]),\n",
    "        )\n",
    "\n",
    "        # ======================= #\n",
    "        #   FOR NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "\n",
    "        #max_no_obj = torch.max(predictions[..., 20:21], predictions[..., 25:26])\n",
    "        #no_object_loss = self.mse(\n",
    "        #    torch.flatten((1 - exists_box) * max_no_obj, start_dim=1),\n",
    "        #    torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
    "        #)\n",
    "\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., self.C:self.C + 1], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1),\n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., self.C + 5:self.C + 6], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1)\n",
    "        )\n",
    "\n",
    "        # ================== #\n",
    "        #   FOR CLASS LOSS   #\n",
    "        # ================== #\n",
    "\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[..., :self.C], end_dim=-2,),\n",
    "            torch.flatten(exists_box * target[..., :self.C], end_dim=-2,),\n",
    "        )\n",
    "\n",
    "        loss = (\n",
    "            self.lambda_coord * box_loss  # first two rows in paper\n",
    "            + object_loss  # third row in paper\n",
    "            + self.lambda_noobj * no_object_loss  # forth row\n",
    "            + class_loss  # fifth row\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16 # 64 in original paper but resource exhausted error otherwise.\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 10\n",
    "LOAD_MODEL = False\n",
    "LOAD_MODEL_FILE = \"model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, optimizer, loss_fn):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    mean_loss = []\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "        mean_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_postfix(loss = loss.item())\n",
    "        \n",
    "    print(f\"Mean loss was {sum(mean_loss) / len(mean_loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, bboxes):\n",
    "        for t in self.transforms:\n",
    "            img, bboxes = t(img), bboxes\n",
    "\n",
    "        return img, bboxes\n",
    "\n",
    "\n",
    "transform = Compose([transforms.Resize((800, 800)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dir = 'datasets'\n",
    "\n",
    "model = TinyYOLOv1().to(DEVICE)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.1, patience=3, mode='max', verbose=True)\n",
    "loss_fn = YoloLoss()\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n",
    "\n",
    "train_dataset = DiorDataset(\n",
    "    root_dir=files_dir,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 733/733 [05:47<00:00,  2.11it/s, loss=431]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 885.9350024297488\n",
      "Train mAP: 0.00039734033634886146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 733/733 [05:17<00:00,  2.31it/s, loss=444]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 598.5482675258329\n",
      "Train mAP: 0.0005161766894161701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 733/733 [05:16<00:00,  2.31it/s, loss=438]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 494.93700936644007\n",
      "Train mAP: 0.0013833394041284919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 733/733 [05:18<00:00,  2.30it/s, loss=336]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 429.7517480368829\n",
      "Train mAP: 0.002831046236678958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 733/733 [05:20<00:00,  2.29it/s, loss=278]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 376.21429033266406\n",
      "Train mAP: 0.00471844244748354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 733/733 [05:20<00:00,  2.29it/s, loss=163]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 334.6629178826513\n",
      "Train mAP: 0.009279157035052776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 733/733 [05:20<00:00,  2.29it/s, loss=156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 308.25114488796845\n",
      "Train mAP: 0.015319416299462318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 733/733 [05:20<00:00,  2.28it/s, loss=251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 291.5718459562585\n",
      "Train mAP: 0.022296009585261345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 733/733 [05:20<00:00,  2.28it/s, loss=232]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 276.34769372042336\n",
      "Train mAP: 0.03044874593615532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 733/733 [05:23<00:00,  2.27it/s, loss=258]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 261.26982970192194\n",
      "Train mAP: 0.041019003838300705\n",
      "=> Saving checkpoint\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    train_fn(train_loader, model, optimizer, loss_fn)\n",
    "    \n",
    "    pred_boxes, target_boxes = get_bboxes(\n",
    "        train_loader, model, iou_threshold=0.5, threshold=0.4\n",
    "    )\n",
    "    #print(f\"pred_boxes: {len(pred_boxes)}, target_boxes: {len(target_boxes)}\")\n",
    "    mean_avg_prec = mean_average_precision(\n",
    "        pred_boxes, target_boxes, iou_threshold=0.5\n",
    "    )\n",
    "    print(f\"Train mAP: {mean_avg_prec}\")\n",
    "    \n",
    "    \n",
    "    scheduler.step(mean_avg_prec)\n",
    "\n",
    "checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "}\n",
    "save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DiorDataset( \n",
    "    root_dir=files_dir,\n",
    "    transform=transform,\n",
    "    train=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
