{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a tiny version of YOLO with DIOR dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arch/ObjDct_Repo/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract images only from one classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images with target classes: 3925\n"
     ]
    }
   ],
   "source": [
    "# Define the paths to your dataset folders\n",
    "dataset_folder = \"dataset\"\n",
    "images_folder = os.path.join(dataset_folder, \"images/train\")\n",
    "labels_folder = os.path.join(dataset_folder, \"labels/train\")\n",
    "\n",
    "# Define the class IDs you want to isolate\n",
    "target_class_ids = [18,19]\n",
    "\n",
    "# Create a new folder to store the isolated images and labels\n",
    "output_images_folder = \"one_class_dataset/images/train\"\n",
    "output_labels_folder = \"one_class_dataset/labels/train\"\n",
    "os.makedirs(output_images_folder, exist_ok=True)\n",
    "os.makedirs(output_labels_folder, exist_ok=True)\n",
    "count = 0\n",
    "# Iterate through each label file in the labels folder\n",
    "for label_file in os.listdir(labels_folder):\n",
    "    label_path = os.path.join(labels_folder, label_file)\n",
    "    \n",
    "    # Open the label file and read its contents\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Filter lines based on the class IDs\n",
    "    filtered_lines = [line.strip().split(' ') for line in lines if int(line.strip().split(' ')[0]) in target_class_ids]\n",
    "    \n",
    "    # If there are any lines for the target class IDs, copy the corresponding image and label file\n",
    "    if filtered_lines:\n",
    "        count += 1\n",
    "        image_file = label_file.replace('.txt', '.jpg')  # Assuming image file extensions are jpg\n",
    "        image_path = os.path.join(images_folder, image_file)\n",
    "        \n",
    "        # Copy the image file to the output folder\n",
    "        shutil.copy(image_path, output_images_folder)\n",
    "        \n",
    "        # Write the modified label lines to a new label file in the output folder\n",
    "        output_label_path = os.path.join(output_labels_folder, label_file)\n",
    "        with open(output_label_path, 'w') as f:\n",
    "            for line in filtered_lines:\n",
    "                if line[0] == '18':\n",
    "                    line[0] = '0'\n",
    "                elif line[0] == '19':\n",
    "                    line[0] = '1'\n",
    "                f.write(' '.join(line) + '\\n')\n",
    "print(f\"Total number of images with target classes: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images with target classes: 4113\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the paths to your dataset folders\n",
    "dataset_folder = \"dataset\"\n",
    "images_folder = os.path.join(dataset_folder, \"images/test\")\n",
    "labels_folder = os.path.join(dataset_folder, \"labels/test\")\n",
    "\n",
    "# Define the class IDs you want to isolate\n",
    "target_class_ids = [18,19]\n",
    "\n",
    "# Create a new folder to store the isolated images and labels\n",
    "output_images_folder = \"one_class_dataset/images/test\"\n",
    "output_labels_folder = \"one_class_dataset/labels/test\"\n",
    "os.makedirs(output_images_folder, exist_ok=True)\n",
    "os.makedirs(output_labels_folder, exist_ok=True)\n",
    "count = 0\n",
    "# Iterate through each label file in the labels folder\n",
    "for label_file in os.listdir(labels_folder):\n",
    "    label_path = os.path.join(labels_folder, label_file)\n",
    "    \n",
    "    # Open the label file and read its contents\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Filter lines based on the class IDs\n",
    "    filtered_lines = [line.strip().split(' ') for line in lines if int(line.strip().split(' ')[0]) in target_class_ids]\n",
    "    \n",
    "    # If there are any lines for the target class IDs, copy the corresponding image and label file\n",
    "    if filtered_lines:\n",
    "        count += 1\n",
    "        image_file = label_file.replace('.txt', '.jpg')  # Assuming image file extensions are jpg\n",
    "        image_path = os.path.join(images_folder, image_file)\n",
    "        \n",
    "        # Copy the image file to the output folder\n",
    "        shutil.copy(image_path, output_images_folder)\n",
    "        \n",
    "        # Write the modified label lines to a new label file in the output folder\n",
    "        output_label_path = os.path.join(output_labels_folder, label_file)\n",
    "        with open(output_label_path, 'w') as f:\n",
    "            for line in filtered_lines:\n",
    "                if line[0] == '18':\n",
    "                    line[0] = '0'\n",
    "                elif line[0] == '19':\n",
    "                    line[0] = '1'\n",
    "                f.write(' '.join(line) + '\\n')\n",
    "                \n",
    "print(f\"Total number of images with target classes: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement YOLO architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TinyissimoYOLO                           [1, 588]                  --\n",
       "├─Conv2d: 1-1                            [1, 16, 254, 254]         448\n",
       "├─SquareActivation: 1-2                  [1, 16, 254, 254]         --\n",
       "├─Conv2d: 1-3                            [1, 16, 252, 252]         2,320\n",
       "├─SquareActivation: 1-4                  [1, 16, 252, 252]         --\n",
       "├─AvgPool2d: 1-5                         [1, 16, 126, 126]         --\n",
       "├─Conv2d: 1-6                            [1, 16, 124, 124]         2,320\n",
       "├─SquareActivation: 1-7                  [1, 16, 124, 124]         --\n",
       "├─Conv2d: 1-8                            [1, 32, 122, 122]         4,640\n",
       "├─SquareActivation: 1-9                  [1, 32, 122, 122]         --\n",
       "├─AvgPool2d: 1-10                        [1, 32, 61, 61]           --\n",
       "├─Conv2d: 1-11                           [1, 32, 59, 59]           9,248\n",
       "├─SquareActivation: 1-12                 [1, 32, 59, 59]           --\n",
       "├─Conv2d: 1-13                           [1, 64, 57, 57]           18,496\n",
       "├─SquareActivation: 1-14                 [1, 64, 57, 57]           --\n",
       "├─AvgPool2d: 1-15                        [1, 64, 28, 28]           --\n",
       "├─Conv2d: 1-16                           [1, 64, 26, 26]           36,928\n",
       "├─SquareActivation: 1-17                 [1, 64, 26, 26]           --\n",
       "├─Conv2d: 1-18                           [1, 64, 24, 24]           36,928\n",
       "├─SquareActivation: 1-19                 [1, 64, 24, 24]           --\n",
       "├─AvgPool2d: 1-20                        [1, 64, 12, 12]           --\n",
       "├─Conv2d: 1-21                           [1, 128, 10, 10]          73,856\n",
       "├─SquareActivation: 1-22                 [1, 128, 10, 10]          --\n",
       "├─Conv2d: 1-23                           [1, 128, 8, 8]            147,584\n",
       "├─SquareActivation: 1-24                 [1, 128, 8, 8]            --\n",
       "├─AvgPool2d: 1-25                        [1, 128, 4, 4]            --\n",
       "├─Flatten: 1-26                          [1, 2048]                 --\n",
       "├─Linear: 1-27                           [1, 588]                  1,204,812\n",
       "==========================================================================================\n",
       "Total params: 1,537,580\n",
       "Trainable params: 1,537,580\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 437.52\n",
       "==========================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 25.53\n",
       "Params size (MB): 6.15\n",
       "Estimated Total Size (MB): 32.47\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TinyissimoYOLO(nn.Module):\n",
    "    def __init__(self, B=2, num_classes=2):\n",
    "        super(TinyissimoYOLO, self).__init__()\n",
    "        S = 7  # grid size\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3)\n",
    "        self.square_activation1 = SquareActivation()\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3)\n",
    "        self.square_activation2 = SquareActivation()\n",
    "        self.avgpool1 = nn.AvgPool2d(2, 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(16, 16, kernel_size=3)\n",
    "        self.square_activation3 = SquareActivation()\n",
    "        self.conv4 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.square_activation4 = SquareActivation()\n",
    "        self.avgpool2 = nn.AvgPool2d(2, 2)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(32, 32, kernel_size=3)\n",
    "        self.square_activation5 = SquareActivation()\n",
    "        self.conv6 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.square_activation6 = SquareActivation()\n",
    "        self.avgpool3 = nn.AvgPool2d(2, 2)\n",
    "\n",
    "        self.conv7 = nn.Conv2d(64, 64, kernel_size=3)\n",
    "        self.square_activation7 = SquareActivation()\n",
    "        self.conv8 = nn.Conv2d(64, 64, kernel_size=3)\n",
    "        self.square_activation8 = SquareActivation()\n",
    "        self.avgpool4 = nn.AvgPool2d(2, 2)\n",
    "\n",
    "        self.conv9 = nn.Conv2d(64, 128, kernel_size=3)\n",
    "        self.square_activation9 = SquareActivation()\n",
    "        self.conv10 = nn.Conv2d(128, 128, kernel_size=3)\n",
    "        self.square_activation10 = SquareActivation()\n",
    "        self.avgpool5 = nn.AvgPool2d(2, 2)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc2 = nn.Linear(2048, S * S * (5 * B + num_classes))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.square_activation1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.square_activation2(x)\n",
    "        x = self.avgpool1(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.square_activation3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.square_activation4(x)\n",
    "        x = self.avgpool2(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.square_activation5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.square_activation6(x)\n",
    "        x = self.avgpool3(x)\n",
    "\n",
    "        x = self.conv7(x)\n",
    "        x = self.square_activation7(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.square_activation8(x)\n",
    "        x = self.avgpool4(x)\n",
    "\n",
    "        x = self.conv9(x)\n",
    "        x = self.square_activation9(x)\n",
    "        x = self.conv10(x)\n",
    "        x = self.square_activation10(x)\n",
    "        x = self.avgpool5(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Assuming the SquareActivation is defined somewhere else\n",
    "class SquareActivation(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * x\n",
    "\n",
    "model = TinyissimoYOLO()\n",
    "summary(model, input_size=(1, 3, 256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection over Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(boxes_preds, boxes_labels):\n",
    "    \"\"\"\n",
    "    Calculates intersection over union\n",
    "    \n",
    "    Parameters:\n",
    "        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)    \n",
    "    Returns:\n",
    "        tensor: Intersection over union for all examples\n",
    "    \"\"\"\n",
    "    # boxes_preds shape is (N, 4) where N is the number of bboxes\n",
    "    #boxes_labels shape is (n, 4)\n",
    "    \n",
    "    box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "    box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "    box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "    box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "    box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "    box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "    box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "    box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "    \n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "    #print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
    "    \n",
    "    #.clamp(0) is for the case when they don't intersect. Since when they don't intersect, one of these will be negative so that should become 0\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "    #print(f\"intersection: {intersection}\")\n",
    "\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "    #print(f\"box1_area: {box1_area}, box2_area: {box2_area}\")\n",
    "    \n",
    "    return intersection / (box1_area + box2_area - intersection + 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non Max Suppression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input**: A list of Proposal boxes B, corresponding confidence scores S and overlap threshold N.\n",
    "\n",
    "**Output**: A list of filtered proposals D.\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "1.  Select the proposal with highest confidence score, remove it from B and add it to the final proposal list D. (Initially D is empty).\n",
    "2.  Now compare this proposal with all the proposals — calculate the IOU (Intersection over Union) of this proposal with every other proposal. If the IOU is greater than the threshold N, remove that proposal from B.\n",
    "3.  Again take the proposal with the highest confidence from the remaining proposals in B and remove it from B and add it to D.\n",
    "4.  Once again calculate the IOU of this proposal with all the proposals in B and eliminate the boxes which have high IOU than threshold.\n",
    "5.  This process is repeated until there are no more proposals left in B.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(bboxes, iou_threshold, threshold):\n",
    "    \"\"\"\n",
    "    Does Non Max Suppression given bboxes\n",
    "    Parameters:\n",
    "        bboxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [class_pred, prob_score, x_center, y_center, width, height]\n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        threshold (float): threshold to remove predicted bboxes (independent of IoU) \n",
    "    Returns:\n",
    "        list: bboxes after performing NMS given a specific IoU threshold\n",
    "    \"\"\"\n",
    "\n",
    "    assert type(bboxes) == list\n",
    "\n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        chosen_box = bboxes.pop(0)\n",
    "\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            if box[0] != chosen_box[0]\n",
    "            or intersection_over_union(\n",
    "                torch.tensor(chosen_box[2:]),\n",
    "                torch.tensor(box[2:]),\n",
    "            )\n",
    "            < iou_threshold\n",
    "        ]\n",
    "\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "    #print(f\"bboxes_after_nms: {bboxes_after_nms}\")\n",
    "\n",
    "    return bboxes_after_nms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It describes a trade-off between precision and recall.\n",
    "\n",
    "**Precision**, also referred to as the positive predictive value, describes how well a model predicts the positive class. \n",
    "$$Precision=\\frac{TP}{TP+FP}$$\n",
    ">   Of all bounding box **predictions**, what fraction was actually correct?\n",
    "\n",
    "**Recall**, also called sensitivity tells you if your model made the right predictions when it should have. \n",
    "$$Recall=\\frac{TP}{TP+FN}$$\n",
    ">   Of all **target** bounding boxes, what fraction did we correctly detect?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(\n",
    "    pred_boxes, true_boxes, iou_threshold=0.5, num_classes=2\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates mean average precision \n",
    "    Parameters:\n",
    "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [train_idx, class_prediction, prob_score, x_center, y_center, width, height]\n",
    "        true_boxes (list): Similar as pred_boxes except all the correct ones \n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        num_classes (int): number of classes\n",
    "    Returns:\n",
    "        float: mAP value across all classes given a specific IoU threshold \n",
    "    \"\"\"\n",
    "\n",
    "    # list storing all AP for respective classes\n",
    "    average_precisions = []\n",
    "\n",
    "    # used for numerical stability later on\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # Go through all predictions and targets,\n",
    "        # and only add the ones that belong to the\n",
    "        # current class c\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "        #print(f\"{c} class has {len(detections)} detections\")\n",
    "\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "        #print(f\"{c} class has {len(ground_truths)} ground truths\")\n",
    "\n",
    "        # find the amount of bboxes for each training example\n",
    "        # Counter here finds how many ground truth bboxes we get\n",
    "        # for each training example, so let's say img 0 has 3,\n",
    "        # img 1 has 5 then we will obtain a dictionary with:\n",
    "        # amount_bboxes = {0:3, 1:5}\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "        #print(f\"{c} class has {len(amount_bboxes)} amount bboxes\")\n",
    "\n",
    "        # We then go through each key, val in this dictionary\n",
    "        # and convert to the following (w.r.t same example):\n",
    "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # sort by box probabilities which is index 2\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "        #print(f\"{c} class has {total_true_bboxes} total true bboxes\")\n",
    "        # If none exists for this class then we can safely skip\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # Only take out the ground_truths that have the same\n",
    "            # training idx as detection\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            #print(f\"{c} class has {num_gts} ground truths for detection {detection_idx}\")\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:])\n",
    "                )\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    # true positive and add this bounding box to seen\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    #These additional detections are considered false positives because they do not correspond to a new, unique object\n",
    "                    #they're essentially \"over-detecting\" an object that has already been correctly identified.\n",
    "                    FP[detection_idx] = 1\n",
    "\n",
    "            # if IOU is lower then the detection is a false positive\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        #[1, 1, 0, 1, 0] -> [1, 2, 2, 3, 3]\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        # torch.trapz for numerical integration\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image, boxes):\n",
    "    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n",
    "    im = np.array(image)\n",
    "    height, width, _ = im.shape\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(1)\n",
    "    # Display the image\n",
    "    ax.imshow(im)\n",
    "\n",
    "    # box[0] is x midpoint, box[2] is width\n",
    "    # box[1] is y midpoint, box[3] is height\n",
    "\n",
    "    # Create a Rectangle patch\n",
    "    for box in boxes:\n",
    "        class_label = int(box[0])\n",
    "        box = box[2:]\n",
    "        assert len(box) == 4, \"Got more values than in x, y, w, h, in a box!\"\n",
    "        upper_left_x = box[0] - box[2] / 2\n",
    "        upper_left_y = box[1] - box[3] / 2\n",
    "        rect = patches.Rectangle(\n",
    "            (upper_left_x * width, upper_left_y * height),\n",
    "            box[2] * width,\n",
    "            box[3] * height,\n",
    "            linewidth=1,\n",
    "            edgecolor=\"r\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add class label text\n",
    "        ax.text(upper_left_x * width, upper_left_y * height, str(class_label), color='r', fontsize=10, verticalalignment='bottom')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and convert boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes(\n",
    "    loader,\n",
    "    model,\n",
    "    iou_threshold,\n",
    "    threshold,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    # make sure model is in eval before get bboxes\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "\n",
    "    for batch_idx, (x, labels) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        true_bboxes = cellboxes_to_boxes(labels)\n",
    "        bboxes = cellboxes_to_boxes(predictions)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            nms_boxes = non_max_suppression(\n",
    "                bboxes[idx],\n",
    "                iou_threshold=iou_threshold,\n",
    "                threshold=threshold,\n",
    "            )\n",
    "\n",
    "            # # Activate only for test\n",
    "            # if batch_idx == 0 and idx == 0:\n",
    "            #    plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n",
    "\n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx] + nms_box)\n",
    "\n",
    "            for box in true_bboxes[idx]:\n",
    "                # many will get converted to 0 pred\n",
    "                if box[1] > threshold:\n",
    "                    all_true_boxes.append([train_idx] + box)\n",
    "\n",
    "            train_idx += 1\n",
    "\n",
    "    model.train()\n",
    "    return all_pred_boxes, all_true_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cellboxes(predictions, S=7, C=2):\n",
    "    \"\"\"\n",
    "    Converts bounding boxes output from Yolo with\n",
    "    an image split size of S into entire image ratios\n",
    "    rather than relative to cell ratios.\n",
    "    \"\"\"\n",
    "    predictions = predictions.to(\"cpu\")\n",
    "    batch_size = predictions.shape[0]\n",
    "    predictions = predictions.reshape(batch_size, 7, 7, C + 10)\n",
    "    bboxes1 = predictions[..., C + 1:C + 5]\n",
    "    bboxes2 = predictions[..., C + 6:C + 10]\n",
    "    \n",
    "    scores = torch.cat(\n",
    "        (predictions[..., C].unsqueeze(0), predictions[..., C + 5].unsqueeze(0)), dim=0\n",
    "    )\n",
    "    best_box = scores.argmax(0).unsqueeze(-1)\n",
    "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
    "    # This results in a tensor with shape (batch_size, 7, 7, 1) where each element represents the index of a grid cell.\n",
    "    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n",
    "\n",
    "    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n",
    "    # Permute because is used here to swap these indices to match the (x, y) convention used in the best_boxes tensor.\n",
    "    # [0,1,2]->[0,0,0]\n",
    "    # [0,1,2]->[1,1,1]\n",
    "    # [0,1,2]->[2,2,2]\n",
    "    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
    "    w_y = 1 / S * best_boxes[..., 2:4]\n",
    "\n",
    "    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n",
    "    predicted_class = predictions[..., :C].argmax(-1).unsqueeze(-1)\n",
    "    best_confidence = torch.max(predictions[..., C], predictions[..., C + 5]).unsqueeze(\n",
    "        -1\n",
    "    )\n",
    "    converted_preds = torch.cat(\n",
    "        (predicted_class, best_confidence, converted_bboxes), dim=-1\n",
    "    )\n",
    "    #print(f\"converted_preds: {converted_preds}\")\n",
    "\n",
    "    return converted_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cellboxes_to_boxes(out, S=7):\n",
    "    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n",
    "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
    "    all_bboxes = []\n",
    "\n",
    "    for ex_idx in range(out.shape[0]):\n",
    "        bboxes = []\n",
    "\n",
    "        for bbox_idx in range(S * S):\n",
    "            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n",
    "        all_bboxes.append(bboxes)\n",
    "    #print(f\"all_bboxes: {all_bboxes}\")\n",
    "    return all_bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Loader of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arch/ObjDct_Repo/.venv/lib/python3.11/site-packages/pydantic/main.py:347: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected\n",
      "  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected\n",
      "  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected\n",
      "  Expected `Union[float, tuple[float, float]]` but got `list` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "# apply augmentation to the images\n",
    "IMAGE_SIZE = 256\n",
    "train_transforms = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(max_size=int(IMAGE_SIZE)),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=int(IMAGE_SIZE),\n",
    "            min_width=int(IMAGE_SIZE),\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "        ),\n",
    "        A.RandomCrop(width=IMAGE_SIZE, height=IMAGE_SIZE),\n",
    "        A.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6, hue=0.6, p=0.4),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.ShiftScaleRotate(\n",
    "                    rotate_limit=20, p=0.5, border_mode=cv2.BORDER_CONSTANT\n",
    "                ),\n",
    "                A.Affine(shear=15, p=0.5, mode=1),\n",
    "            ],\n",
    "            p=1.0,\n",
    "        ),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Blur(p=0.1),\n",
    "        A.CLAHE(p=0.1),\n",
    "        A.Posterize(p=0.1),\n",
    "        A.ToGray(p=0.1),\n",
    "        A.ChannelShuffle(p=0.05),\n",
    "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[],),\n",
    ")\n",
    "test_transforms = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(max_size=IMAGE_SIZE),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=IMAGE_SIZE, min_width=IMAGE_SIZE, border_mode=cv2.BORDER_CONSTANT\n",
    "        ),\n",
    "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, S=7, B=2, C=2, transform=None, train=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.train = train\n",
    "\n",
    "        # Determine the directory of the images and labels\n",
    "        if self.train:\n",
    "            self.img_dir = os.path.join(self.root_dir, 'images/train')\n",
    "            self.label_dir = os.path.join(self.root_dir, 'labels/train')\n",
    "        else:\n",
    "            self.img_dir = os.path.join(self.root_dir, 'images/test')\n",
    "            self.label_dir = os.path.join(self.root_dir, 'labels/test')\n",
    "\n",
    "        self.img_ids = os.listdir(self.img_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.img_ids[index].split('.')[0]\n",
    "        boxes = []\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, img_id + '.jpg')\n",
    "        image = Image.open(img_path)\n",
    "        image = image.convert(\"RGB\")\n",
    "        image = np.array(image)\n",
    "        # Load labels\n",
    "        label_path = os.path.join(self.label_dir, img_id + '.txt')\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                class_label, x, y, width, height = map(float, line.strip().split())\n",
    "                boxes.append([x, y, width, height, class_label])\n",
    "\n",
    "        if self.transform:\n",
    "            try:\n",
    "                augmentations = self.transform(image=image, bboxes=boxes)\n",
    "                image = augmentations[\"image\"]\n",
    "                bboxes = augmentations[\"bboxes\"]\n",
    "            except ValueError as e:\n",
    "                print(f\"Error in file: {index}\")\n",
    "                raise e\n",
    "\n",
    "        # Convert To Cells\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
    "        for box in bboxes:\n",
    "            x, y, width, height, class_label = box\n",
    "            class_label = int(class_label)\n",
    "\n",
    "            i, j = int(self.S * y), int(self.S * x)\n",
    "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
    "\n",
    "            width_cell, height_cell = (\n",
    "                width * self.S,\n",
    "                height * self.S,\n",
    "            )\n",
    "\n",
    "            if label_matrix[i, j, self.C] == 0:\n",
    "                label_matrix[i, j, self.C] = 1\n",
    "\n",
    "                box_coordinates = torch.tensor(\n",
    "                    [x_cell, y_cell, width_cell, height_cell]\n",
    "                )\n",
    "\n",
    "                label_matrix[i, j, self.C + 1:self.C + 5] = box_coordinates\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "    \n",
    "        #print(f\"label_matrix shape: {label_matrix.shape}\")\n",
    "\n",
    "        return image, label_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From original paper: \n",
    ">   YOLO predicts multiple bounding boxes per grid cell. At training time we only want one bounding box predictor to be responsible for each object. We assign one predictor to be “responsible” for predicting an object based on which prediction has the highest current IOU with the ground truth. This leads to specialization between the bounding box predictors.\n",
    "Each predictor gets better at predicting certain sizes, aspect ratios, or classes of object, improving overall recall. \n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\lambda_{\\text {coord }} \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{i j}^{\\text {obj }}\\left[\\left(x_i-\\hat{x}_i\\right)^2+\\left(y_i-\\hat{y}_i\\right)^2\\right] \\\\\n",
    "+\\lambda_{\\text {coord }} \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{i j}^{\\text {obj }}\\left[\\left(\\sqrt{w_i}-\\sqrt{\\hat{w}_i}\\right)^2+\\left(\\sqrt{h_i}-\\sqrt{\\hat{h}_i}\\right)^2\\right] \\\\\n",
    "+\\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{i j}^{\\text {obj }}\\left(C_i-\\hat{C}_i\\right)^2 \\\\\n",
    "+\\lambda_{\\text {noobj }} \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{i j}^{\\text {noobj }}\\left(C_i-\\hat{C}_i\\right)^2 \\\\\n",
    "+\\sum_{i=0}^{S^2} \\mathbb{1}_i^{\\text {obj }} \\sum_{c \\in \\text { classes }}\\left(p_i(c)-\\hat{p}_i(c)\\right)^2\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "During training we optimize the following, multi-part where $ 1_{obj}^i $ denotes if object appears in cell **i** and $1_{obj}^{ij}$ denotes that the **j**  bounding box predictor in cell i is “responsible” for that prediction.\n",
    "\n",
    "In every image many grid cells do not contain any object. This pushes the “confidence” scores of those cells towards zero, often overpowering the gradient from cells that do contain objects. This can lead to model instability, as the model may prioritize learning to predict empty cells rather than focusing on correctly detecting objects in cells containing them, causing training to diverge early on. To remedy this, we increase the loss from bounding box coordinate predictions and decrease the loss from confidence predictions for boxes that don’t contain objects. We use two parameters, $\\lambda_{coord}$ and $\\lambda_{noobj}$  to accomplish this.\n",
    "\n",
    "Note that the loss function only penalizes classification error if an object is present in that grid cell (hence the conditional class probability discussed earlier). It also only penalizes bounding box coordinate error if that predictor is “responsible” for the ground truth box (i.e. has the highest\n",
    "IOU of any predictor in that grid cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the loss for yolo (v1) model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S=7, B=2, C=2):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        \"\"\"\n",
    "        S is split size of image (in paper 7),\n",
    "        B is number of boxes (in paper 2),\n",
    "        C is number of classes (in paper 20, in dataset 3),\n",
    "        \"\"\"\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        # These are from Yolo paper, signifying how much we should\n",
    "        # pay loss for no object (noobj) and the box coordinates (coord)\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        # predictions are shaped (BATCH_SIZE, S*S(C+B*5) when inputted\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
    "\n",
    "        # Calculate IoU for the two predicted bounding boxes with target bbox\n",
    "        iou_b1 = intersection_over_union(predictions[..., self.C + 1:self.C + 5], target[..., self.C + 1:self.C + 5])\n",
    "        iou_b2 = intersection_over_union(predictions[..., self.C + 6:self.C + 10], target[..., self.C + 1:self.C + 5])\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "\n",
    "        # Take the box with highest IoU out of the two prediction\n",
    "        # Note that bestbox will be indices of 0, 1 for which bbox was best\n",
    "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
    "        exists_box = target[..., self.C].unsqueeze(3)  # in paper this is Iobj_i\n",
    "\n",
    "        # ======================== #\n",
    "        #   FOR BOX COORDINATES    #\n",
    "        # ======================== #\n",
    "\n",
    "        # Set boxes with no object in them to 0. We only take out one of the two \n",
    "        # predictions, which is the one with highest Iou calculated previously.\n",
    "        box_predictions = exists_box * (\n",
    "            (\n",
    "                bestbox * predictions[..., self.C + 6:self.C + 10]\n",
    "                + (1 - bestbox) * predictions[..., self.C + 1:self.C + 5]\n",
    "            )\n",
    "        )\n",
    "        #print(f\"box_predictions: {box_predictions.shape}\")\n",
    "        box_targets = exists_box * target[..., self.C + 1:self.C + 5]\n",
    "\n",
    "        # Take sqrt of width, height of boxes\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
    "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
    "        )\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions, end_dim=-2),\n",
    "            torch.flatten(box_targets, end_dim=-2),\n",
    "        )\n",
    "\n",
    "        # ==================== #\n",
    "        #   FOR OBJECT LOSS    #\n",
    "        # ==================== #\n",
    "\n",
    "        # pred_box is the confidence score for the bbox with highest IoU\n",
    "        pred_box = (\n",
    "            bestbox * predictions[..., self.C + 5:self.C + 6] + (1 - bestbox) * predictions[..., self.C:self.C + 1]\n",
    "        )\n",
    "\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_box * pred_box),\n",
    "            torch.flatten(exists_box * target[..., self.C:self.C + 1]),\n",
    "        )\n",
    "\n",
    "        # ======================= #\n",
    "        #   FOR NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "\n",
    "        #max_no_obj = torch.max(predictions[..., 20:21], predictions[..., 25:26])\n",
    "        #no_object_loss = self.mse(\n",
    "        #    torch.flatten((1 - exists_box) * max_no_obj, start_dim=1),\n",
    "        #    torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
    "        #)\n",
    "\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., self.C:self.C + 1], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1),\n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., self.C + 5:self.C + 6], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1)\n",
    "        )\n",
    "\n",
    "        # ================== #\n",
    "        #   FOR CLASS LOSS   #\n",
    "        # ================== #\n",
    "\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[..., :self.C], end_dim=-2,),\n",
    "            torch.flatten(exists_box * target[..., :self.C], end_dim=-2,),\n",
    "        )\n",
    "\n",
    "        loss = (\n",
    "            self.lambda_coord * box_loss  # first two rows in paper\n",
    "            + object_loss  # third row in paper\n",
    "            + self.lambda_noobj * no_object_loss  # forth row\n",
    "            + class_loss  # fifth row\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32 # 64 in original paper but resource exhausted error otherwise.\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 50\n",
    "LOAD_MODEL = False\n",
    "LOAD_MODEL_FILE = \"model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, optimizer, loss_fn):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    mean_loss = []\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "        mean_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_postfix(loss = loss.item())\n",
    "        \n",
    "    print(f\"Mean loss was {sum(mean_loss) / len(mean_loss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arch/ObjDct_Repo/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "files_dir = 'one_class_dataset'\n",
    "model = TinyissimoYOLO(num_classes=2).to(DEVICE)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.1, patience=3, mode='max', verbose=True)\n",
    "loss_fn = YoloLoss()\n",
    "\n",
    "\n",
    "train_dataset = DiorDataset(\n",
    "    root_dir=files_dir,\n",
    "    transform=train_transforms,\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/123 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 588])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/123 [00:01<02:57,  1.45s/it, loss=960]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DiorDataset' object has no attribute 'files'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 41\u001b[0m, in \u001b[0;36mDiorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     augmentations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     image \u001b[38;5;241m=\u001b[39m augmentations[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/ObjDct_Repo/.venv/lib/python3.11/site-packages/albumentations/core/composition.py:228\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 228\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m transforms:\n",
      "File \u001b[0;32m~/ObjDct_Repo/.venv/lib/python3.11/site-packages/albumentations/core/utils.py:88\u001b[0m, in \u001b[0;36mDataProcessor.preprocess\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_fields:\n\u001b[0;32m---> 88\u001b[0m     data[data_name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_and_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ObjDct_Repo/.venv/lib/python3.11/site-packages/albumentations/core/utils.py:102\u001b[0m, in \u001b[0;36mDataProcessor.check_and_convert\u001b[0;34m(self, data, rows, cols, direction)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m direction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_albumentations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m direction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/ObjDct_Repo/.venv/lib/python3.11/site-packages/albumentations/core/bbox_utils.py:140\u001b[0m, in \u001b[0;36mBboxProcessor.convert_to_albumentations\u001b[0;34m(self, data, rows, cols)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_albumentations\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Sequence[BoxType], rows: \u001b[38;5;28mint\u001b[39m, cols: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BoxType]:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_bboxes_to_albumentations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_validity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ObjDct_Repo/.venv/lib/python3.11/site-packages/albumentations/core/bbox_utils.py:422\u001b[0m, in \u001b[0;36mconvert_bboxes_to_albumentations\u001b[0;34m(bboxes, source_format, rows, cols, check_validity)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convert a list bounding boxes from a format specified in `source_format` to the format used by albumentations\"\"\"\u001b[39;00m\n\u001b[0;32m--> 422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mconvert_bbox_to_albumentations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_validity\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/ObjDct_Repo/.venv/lib/python3.11/site-packages/albumentations/core/bbox_utils.py:422\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convert a list bounding boxes from a format specified in `source_format` to the format used by albumentations\"\"\"\u001b[39;00m\n\u001b[0;32m--> 422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mconvert_bbox_to_albumentations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_validity\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m bbox \u001b[38;5;129;01min\u001b[39;00m bboxes]\n",
      "File \u001b[0;32m~/ObjDct_Repo/.venv/lib/python3.11/site-packages/albumentations/core/bbox_utils.py:358\u001b[0m, in \u001b[0;36mconvert_bbox_to_albumentations\u001b[0;34m(bbox, source_format, rows, cols, check_validity)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_validity:\n\u001b[0;32m--> 358\u001b[0m     \u001b[43mcheck_bbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bbox\n",
      "File \u001b[0;32m~/ObjDct_Repo/.venv/lib/python3.11/site-packages/albumentations/core/bbox_utils.py:453\u001b[0m, in \u001b[0;36mcheck_bbox\u001b[0;34m(bbox)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m value \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misclose(value, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misclose(value, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 453\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for bbox \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbbox\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to be in the range [0.0, 1.0], got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    454\u001b[0m x_min, y_min, x_max, y_max \u001b[38;5;241m=\u001b[39m bbox[:\u001b[38;5;241m4\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Expected x_max for bbox (0.9875, 0.0455, 1.0005, 0.0665, 0.0) to be in the range [0.0, 1.0], got 1.0005.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     pred_boxes, target_boxes \u001b[38;5;241m=\u001b[39m get_bboxes(\n\u001b[1;32m      5\u001b[0m         train_loader, model, iou_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m\n\u001b[1;32m      6\u001b[0m     )\n\u001b[1;32m      7\u001b[0m     mean_avg_prec \u001b[38;5;241m=\u001b[39m mean_average_precision(\n\u001b[1;32m      8\u001b[0m         pred_boxes, target_boxes, iou_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m      9\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m, in \u001b[0;36mtrain_fn\u001b[0;34m(train_loader, model, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m      2\u001b[0m loop \u001b[38;5;241m=\u001b[39m tqdm(train_loader, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m mean_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ObjDct_Repo/.venv/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/ObjDct_Repo/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/ObjDct_Repo/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/ObjDct_Repo/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/ObjDct_Repo/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[11], line 45\u001b[0m, in \u001b[0;36mDiorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     43\u001b[0m         bboxes \u001b[38;5;241m=\u001b[39m augmentations[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 45\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfiles\u001b[49m[index]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Convert To Cells\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DiorDataset' object has no attribute 'files'"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    train_fn(train_loader, model, optimizer, loss_fn)\n",
    "    pred_boxes, target_boxes = get_bboxes(\n",
    "        train_loader, model, iou_threshold=0.5, threshold=0.4\n",
    "    )\n",
    "    mean_avg_prec = mean_average_precision(\n",
    "        pred_boxes, target_boxes, iou_threshold=0.5\n",
    "    )\n",
    "    print(f\"Train mAP: {mean_avg_prec}\")\n",
    "    scheduler.step(mean_avg_prec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dir='one_class_dataset'\n",
    "    \n",
    "test_dataset = DiorDataset( \n",
    "    root_dir=files_dir,\n",
    "    transform=transform,\n",
    "    train=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyYOLOv1(\n",
       "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (square_activation1): SquareActivation()\n",
       "  (batch_norm1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (avgpool1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (square_activation2): SquareActivation()\n",
       "  (batch_norm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (avgpool2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "  (square_activation_ft): SquareActivation()\n",
       "  (fc2): Linear(in_features=2048, out_features=539, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "checkpoint = torch.load(\"model.pth\")\n",
    "# Load the state dictionary from the .pth file\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test mAP: 0.004085736349225044\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_boxes, target_boxes = get_bboxes(\n",
    "        test_loader, model, iou_threshold=0.5, threshold=0.4\n",
    "    )\n",
    "\n",
    "    mean_avg_prec = mean_average_precision(\n",
    "        pred_boxes, target_boxes, iou_threshold=0.5\n",
    "    )\n",
    "    print(f\"Test mAP: {mean_avg_prec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 588])\n"
     ]
    }
   ],
   "source": [
    "sample = torch.rand(1, 3, 256, 256)\n",
    "model.eval()\n",
    "output = model(sample)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the path to save the ONNX model\n",
    "onnx_model_path = \"model.onnx\"\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(model, sample, onnx_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PYHELAYERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a subset of plain samples from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 256, 256)\n",
      "(1, 7, 7, 12)\n"
     ]
    }
   ],
   "source": [
    "test_img_list=[]\n",
    "test_label_list=[]\n",
    "for image,label in test_dataset:\n",
    "    test_img_list.append(image)\n",
    "    test_label_list.append(label)\n",
    "    if len(test_img_list)==1:\n",
    "        break\n",
    "\n",
    "test_img_array = np.array(test_img_list)\n",
    "test_label_array = np.array(test_label_list)\n",
    "# test_img_array = test_img_array[:11728]\n",
    "# test_label_array = test_label_array[:11728]\n",
    "# test_img_array = test_img_array.reshape(733,16,3,64,64)\n",
    "# test_label_array = test_label_array.reshape(733,16,7,7,30)\n",
    "print(test_img_array.shape)\n",
    "print(test_label_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize he scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misc. initalizations\n"
     ]
    }
   ],
   "source": [
    "import pyhelayers\n",
    "import utils\n",
    "\n",
    "utils.verify_memory()\n",
    "\n",
    "print('Misc. initalizations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HE context ready\n"
     ]
    }
   ],
   "source": [
    "context = pyhelayers.DefaultContext()\n",
    "print('HE context ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nnp = pyhelayers.NeuralNetPlain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = pyhelayers.PlainModelHyperParams()\n",
    "nnp.init_from_files(hyper_params, [\"model.onnx\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile ready. Batch size= 1\n"
     ]
    }
   ],
   "source": [
    "he_run_req = pyhelayers.HeRunRequirements()\n",
    "he_run_req.set_he_context_options([pyhelayers.DefaultContext()])\n",
    "he_run_req.optimize_for_batch_size(1)\n",
    "\n",
    "profile = pyhelayers.HeModel.compile(nnp, he_run_req)\n",
    "batch_size = profile.get_optimal_batch_size()\n",
    "print('Profile ready. Batch size=',batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "He configuration requirement:\n",
       "Security level: 128\n",
       "Integer part precision: 10\n",
       "Fractional part precision: 50\n",
       "Number of slots: 16384\n",
       "Multiplication depth: 11\n",
       "Bootstrappable: False\n",
       "Automatic bootstrapping: False\n",
       "Rotation keys policy: custom, 20 keys required:\n",
       "[-4096, -2048, -1024, -512, -256, -128, -64, -32, -16, 16, 32, 64, 128, 256, 512, 1024, 2048, 3072, 4096, 8192]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile.get_he_config_requirement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HE context initalized\n"
     ]
    }
   ],
   "source": [
    "context = pyhelayers.HeModel.create_context(profile)\n",
    "print('HE context initalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CKKS'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.get_scheme_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1125899903956542.8"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.get_default_scale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SEAL'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.get_library_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.has_secret_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.save_secret_key_to_file('secret_key.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encrypted network ready\n"
     ]
    }
   ],
   "source": [
    "nn = pyhelayers.NeuralNet(context)\n",
    "nn.encode_encrypt(nnp, profile)\n",
    "print('Encrypted network ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of size 1 loaded\n"
     ]
    }
   ],
   "source": [
    "plain_samples, labels = utils.extract_batch(test_img_array, test_label_array, batch_size, 0)\n",
    "\n",
    "print('Batch of size',batch_size,'loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 3, 64, 64)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "print(plain_samples.shape)\n",
    "print(plain_samples.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data encrypted\n"
     ]
    }
   ],
   "source": [
    "iop = nn.create_io_processor()\n",
    "samples = pyhelayers.EncryptedData(context)\n",
    "iop.encode_encrypt_inputs_for_predict(samples, [plain_samples])\n",
    "print('Test data encrypted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make prediction on encrypted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration of predict: 66.590 (s)\n",
      "Duration of predict per sample: 66.590 (s)\n"
     ]
    }
   ],
   "source": [
    "utils.start_timer()\n",
    "\n",
    "predictions = pyhelayers.EncryptedData(context)\n",
    "nn.predict(predictions, samples)\n",
    "\n",
    "duration=utils.end_timer('predict')\n",
    "utils.report_duration('predict per sample',duration/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plain prediction shape after HE: (16, 1470)\n"
     ]
    }
   ],
   "source": [
    "plain_predictions_aHE = iop.decrypt_decode_output(predictions)\n",
    "print(f\"plain prediction shape after HE: {plain_predictions_aHE.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(plain_predictions_aHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In case I have test samples > batch size\n",
    "plain_predictions_aHE = plain_predictions_aHE.reshape(,1470)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate prediction with HE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plain prediction shape before HE: (16, 1470)\n"
     ]
    }
   ],
   "source": [
    "plain_predictions_bHE=[]\n",
    "for sample in test_img_array:\n",
    "    tensor_sample = torch.tensor(sample).unsqueeze(0)\n",
    "    output = model(tensor_sample)\n",
    "    output = output.detach().numpy()\n",
    "    plain_predictions_bHE.append(output)\n",
    "plain_predictions_bHE = np.array(plain_predictions_bHE)\n",
    "plain_predictions_bHE = plain_predictions_bHE.reshape(16, 1470)\n",
    "print(f\"plain prediction shape before HE: {plain_predictions_bHE.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean difference: 8.096096892812246e-08\n",
      "Max difference: 1.0847093330212942e-06\n",
      "Min difference: 4.0473735474222394e-12\n",
      "Std difference: 7.963937368555468e-08\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compute the absolute differences between plain prediction before and after HE\n",
    "differences = np.abs(plain_predictions_bHE - plain_predictions_aHE)\n",
    "\n",
    "# Compute relevant statistics\n",
    "mean_difference = np.mean(differences)\n",
    "max_difference = np.max(differences)\n",
    "min_difference = np.min(differences)\n",
    "std_difference = np.std(differences)\n",
    "\n",
    "print(f\"Mean difference: {mean_difference}\")\n",
    "print(f\"Max difference: {max_difference}\")\n",
    "print(f\"Min difference: {min_difference}\")\n",
    "print(f\"Std difference: {std_difference}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert prediction in bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes_from_prediction(\n",
    "    predictions,\n",
    "    test_image_array,\n",
    "    iou_threshold,\n",
    "    threshold,\n",
    "):\n",
    "    all_pred_boxes = []\n",
    "\n",
    "    \n",
    "    train_idx = 0\n",
    "\n",
    "    bboxes = cellboxes_to_boxes(predictions)\n",
    "\n",
    "    for idx in range(len(test_image_array)):\n",
    "        image = test_image_array[idx]\n",
    "\n",
    "        nms_boxes = non_max_suppression(\n",
    "            bboxes[idx],\n",
    "            iou_threshold=iou_threshold,\n",
    "            threshold=threshold,\n",
    "        )\n",
    "\n",
    "        # Activate only for test\n",
    "        if  idx == 0:\n",
    "            plot_image(image.permute(1,2,0).to(\"cpu\"), nms_boxes)\n",
    "\n",
    "        for nms_box in nms_boxes:\n",
    "            all_pred_boxes.append([train_idx] + nms_box)\n",
    "\n",
    "        \n",
    "\n",
    "        train_idx += 1\n",
    "\n",
    "    return all_pred_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before HE\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction after HE\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensor_predaHE = torch.tensor(plain_predictions_aHE)\n",
    "tensor_predbHE = torch.tensor(plain_predictions_bHE)\n",
    "tensor_imgs = torch.tensor(test_img_array)\n",
    "\n",
    "print(\"Prediction before HE\")\n",
    "bboxes_aHE = get_bboxes_from_prediction(tensor_predbHE, tensor_imgs, iou_threshold=0.5, threshold=0.4)\n",
    "\n",
    "print(\"Prediction after HE\")\n",
    "bboxes_bHE = get_bboxes_from_prediction(tensor_predaHE, tensor_imgs, iou_threshold=0.5, threshold=0.4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
